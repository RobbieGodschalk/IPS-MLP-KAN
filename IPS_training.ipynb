{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (7200, 623)\n",
      "df_train_x: (6480, 620)\n",
      "df_train_y: (6480, 2)\n",
      "df_val_x: (720, 620)\n",
      "df_val_y: (720, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test01-02-03_full: (21600, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01', '02', '03'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test01-02-03_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=620):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_256 = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "\n",
    "TRIALS_MLP = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-12 22:04:50,430] A new study created in memory with name: no-name-bf837885-0f5a-413a-9cc0-2dabd8d5202c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-12 22:04:51,708] Trial 0 finished with value: 8.23174786567688 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'dropout_rate': 0.3938193813296327, 'lr': 0.006561113899932776, 'batch_size': 208, 'epochs': 123}. Best is trial 0 with value: 8.23174786567688.\n",
      "[I 2024-06-12 22:04:54,260] Trial 1 finished with value: 5.1046747207641605 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'dropout_rate': 0.538091733959047, 'lr': 0.009356227644938722, 'batch_size': 176, 'epochs': 91}. Best is trial 1 with value: 5.1046747207641605.\n",
      "[I 2024-06-12 22:05:02,501] A new study created in memory with name: no-name-66df529e-1345-48f3-a2d2-ce7c49bdccab\n",
      "[I 2024-06-12 22:05:06,766] Trial 0 finished with value: 22.108896255493164 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 64, 'dropout_rate': 0.4917770497411652, 'lr': 0.009903961698618118, 'batch_size': 336, 'epochs': 71}. Best is trial 0 with value: 22.108896255493164.\n",
      "[I 2024-06-12 22:05:09,445] Trial 1 finished with value: 10.939193373141082 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'dropout_rate': 0.30902776457290404, 'lr': 0.001304059226995654, 'batch_size': 32, 'epochs': 128}. Best is trial 1 with value: 10.939193373141082.\n",
      "[I 2024-06-12 22:05:20,345] A new study created in memory with name: no-name-0628177e-564c-4d4d-82ca-e1a800515614\n",
      "[I 2024-06-12 22:05:22,458] Trial 0 finished with value: 23.838818550109863 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'dropout_rate': 0.5763132670072848, 'lr': 0.009584221274875671, 'batch_size': 496, 'epochs': 65}. Best is trial 0 with value: 23.838818550109863.\n",
      "[I 2024-06-12 22:05:23,317] Trial 1 finished with value: 23.776403427124023 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 128, 'dropout_rate': 0.510021986710484, 'lr': 0.008855861014486668, 'batch_size': 480, 'epochs': 60}. Best is trial 1 with value: 23.776403427124023.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=620):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_256:\n",
    "    print('Starting MLP reduced grid search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/256_MLP.pth', encoders, 256)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [620] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_256 = True\n",
    "SEARCH_KAN_REDUCED_128 = True \n",
    "\n",
    "TRIALS_KAN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-12 22:05:25,494] A new study created in memory with name: no-name-2edb1257-3da0-4b8f-bd4b-295fab8843ae\n",
      "[I 2024-06-12 22:05:33,111] Trial 0 finished with value: 1.1233530640602112 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 16, 'lr': 0.001536693497500527, 'batch_size': 224, 'epochs': 71}. Best is trial 0 with value: 1.1233530640602112.\n",
      "[I 2024-06-12 22:05:45,593] Trial 1 finished with value: 0.886724129319191 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 64, 'lr': 0.001774863386753514, 'batch_size': 96, 'epochs': 81}. Best is trial 1 with value: 0.886724129319191.\n",
      "[I 2024-06-12 22:06:06,813] A new study created in memory with name: no-name-3aaba5c7-9af8-4a3a-8fc3-47cf3dec0890\n",
      "[I 2024-06-12 22:06:12,121] Trial 0 finished with value: 1.6600315570831299 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'lr': 0.004723190777446269, 'batch_size': 288, 'epochs': 70}. Best is trial 0 with value: 1.6600315570831299.\n",
      "[I 2024-06-12 22:06:18,086] Trial 1 finished with value: 1.9225968917210896 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'lr': 0.005199552437083982, 'batch_size': 272, 'epochs': 78}. Best is trial 0 with value: 1.6600315570831299.\n",
      "[I 2024-06-12 22:06:31,138] A new study created in memory with name: no-name-931b9e7e-a786-4c0b-90a2-5ad73165aea7\n",
      "[I 2024-06-12 22:06:35,901] Trial 0 finished with value: 1.9112414717674255 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 700, 'lr': 0.006144191239845539, 'batch_size': 480, 'epochs': 136}. Best is trial 0 with value: 1.9112414717674255.\n",
      "[I 2024-06-12 22:06:42,078] Trial 1 finished with value: 1.4424817323684693 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 128, 'lr': 0.004193362088373625, 'batch_size': 144, 'epochs': 60}. Best is trial 1 with value: 1.4424817323684693.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 620, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_256:\n",
    "    print('Starting KAN reduced search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 256, f'./models/KAN/256_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "256\n",
      "512\n",
      "256\n",
      "128\n",
      "512\n",
      "256\n",
      "512\n",
      "256\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '256': './models/MLP/256_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '256': './models/KAN/256_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '256': ['./models/MLP/256_encoder_512.pth', './models/MLP/256_encoder_256.pth'],\n",
    "        '128': ['./models/MLP/128_encoder_512.pth', './models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '256': ['./models/KAN/256_encoder_512.pth', './models/KAN/256_encoder_256.pth'],\n",
    "        '128': ['./models/KAN/128_encoder_512.pth', './models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 128, the SAE will have 512 -> 256 -> 128\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 620\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 512 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [64, 16, 256, 256], 0.538091733959047, 620)\n",
    "    mlp_256 = load_MLP_model(model_paths['MLP']['256'], [32, 64], 0.30902776457290404, 256)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [128, 700, 128], 0.510021986710484, 128)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [256, 620, 128, 64], 620)\n",
    "    kan_256 = load_KAN_model(model_paths['KAN']['256'], [64, 256, 128], 256)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [512, 700, 128], 128)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_256 = load_SAE(SAE_paths['MLP']['256'], 256)\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    kan_SAE_256 = load_SAE(SAE_paths['KAN']['256'], 256)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy()\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1))\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_256 = stacked_encode_data(X_test_tensor, mlp_SAE_256)\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    kan_test_data_encoded_256 = stacked_encode_data(X_test_tensor, kan_SAE_256)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_256_distances = evaluate_model(mlp_256, mlp_test_data_encoded_256, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_256_distances = evaluate_model(kan_256, kan_test_data_encoded_256, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_256_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_256_distances.shape)\n",
    "    print(kan_128_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_256': mlp_256_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_256': kan_256_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_256'], data['KAN_256']], labels=['MLP_256', 'KAN_256'])\n",
    "    axes[1].set_title('Reduced models (256)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[2].set_title('Reduced models (128)')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
