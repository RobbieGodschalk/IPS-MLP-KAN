{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (4320, 451)\n",
      "df_train_x: (3888, 448)\n",
      "df_train_y: (3888, 2)\n",
      "df_val_x: (432, 448)\n",
      "df_val_y: (432, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/V1.0/'\n",
    "training_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 448\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test01-02-03_full: (12960, 451)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "sets = ['01', '02', '03'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test01-02-03_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=448):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Update input_dim for next layer\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "SEARCH_MLP_REDUCED_64 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 23:07:54,408] A new study created in memory with name: no-name-2e28c9ca-c358-4dda-9ce8-5c65177f30ff\n",
      "[I 2024-06-14 23:07:56,289] Trial 0 finished with value: 11.952187538146973 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 64, 'dropout_rate': 0.5353825776770658, 'lr': 0.0013154801555835735, 'batch_size': 176, 'epochs': 102}. Best is trial 0 with value: 11.952187538146973.\n",
      "[I 2024-06-14 23:08:00,302] Trial 1 finished with value: 2.788967490196228 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 128, 'dropout_rate': 0.5951778252952069, 'lr': 0.009934192959429131, 'batch_size': 336, 'epochs': 65}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:01,207] Trial 2 finished with value: 6.585254192352295 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'dropout_rate': 0.35744606895372355, 'lr': 0.007856451665488692, 'batch_size': 400, 'epochs': 77}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:03,375] Trial 3 finished with value: 6.597606499989827 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 512, 'dropout_rate': 0.35959610804846276, 'lr': 0.0012985306127669097, 'batch_size': 80, 'epochs': 92}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:04,784] Trial 4 finished with value: 5.3363964557647705 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 576, 'dropout_rate': 0.3618629909270287, 'lr': 0.006240350888499283, 'batch_size': 256, 'epochs': 84}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:05,762] Trial 5 finished with value: 19.1051607131958 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'dropout_rate': 0.3545380545124798, 'lr': 0.007481302303851236, 'batch_size': 128, 'epochs': 105}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:07,006] Trial 6 finished with value: 18.838086128234863 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 64, 'dropout_rate': 0.33125833407503075, 'lr': 0.0075289577585839805, 'batch_size': 384, 'epochs': 92}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:08,785] Trial 7 finished with value: 4.2438836097717285 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.5533575807754154, 'lr': 0.0053799504228149485, 'batch_size': 256, 'epochs': 53}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:10,537] Trial 8 finished with value: 30.94202423095703 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 64, 'dropout_rate': 0.36094369550698013, 'lr': 0.008668048373807769, 'batch_size': 400, 'epochs': 83}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:14,581] Trial 9 finished with value: 10.22684383392334 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 640, 'dropout_rate': 0.29106012217013977, 'lr': 0.006966953660856778, 'batch_size': 448, 'epochs': 86}. Best is trial 1 with value: 2.788967490196228.\n",
      "[I 2024-06-14 23:08:18,390] Trial 10 finished with value: 2.425966501235962 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 640, 'dropout_rate': 0.4669215439246963, 'lr': 0.00989312574850715, 'batch_size': 512, 'epochs': 138}. Best is trial 10 with value: 2.425966501235962.\n",
      "[I 2024-06-14 23:08:24,107] Trial 11 finished with value: 2.4868288040161133 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 640, 'dropout_rate': 0.4693871140765374, 'lr': 0.0099428429651704, 'batch_size': 496, 'epochs': 149}. Best is trial 10 with value: 2.425966501235962.\n",
      "[I 2024-06-14 23:08:26,096] Trial 12 finished with value: 4.948612213134766 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 640, 'dropout_rate': 0.46131922469385894, 'lr': 0.009947610444529461, 'batch_size': 512, 'epochs': 150}. Best is trial 10 with value: 2.425966501235962.\n",
      "[I 2024-06-14 23:08:33,865] Trial 13 finished with value: 1.6223100423812866 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 640, 'dropout_rate': 0.4492689954451808, 'lr': 0.004556369013124802, 'batch_size': 512, 'epochs': 148}. Best is trial 13 with value: 1.6223100423812866.\n",
      "[I 2024-06-14 23:09:03,433] Trial 14 finished with value: 1.5702060328589544 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 640, 'dropout_rate': 0.20361991807188823, 'lr': 0.004209352558748687, 'batch_size': 16, 'epochs': 131}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:09:20,269] Trial 15 finished with value: 1.8193172083960638 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2013775404152479, 'lr': 0.00415688216628954, 'batch_size': 16, 'epochs': 124}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:09:20,969] Trial 16 finished with value: 8.556913693745932 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'dropout_rate': 0.22081983749655704, 'lr': 0.0032748060487571007, 'batch_size': 192, 'epochs': 128}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:09:27,304] Trial 17 finished with value: 1.63395494222641 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'dropout_rate': 0.25776127058345966, 'lr': 0.003271811582143641, 'batch_size': 320, 'epochs': 120}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:09:41,363] Trial 18 finished with value: 2.28212187466798 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'dropout_rate': 0.4197311833223666, 'lr': 0.004805745413437441, 'batch_size': 16, 'epochs': 114}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:09:42,672] Trial 19 finished with value: 33.05487823486328 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 448, 'dropout_rate': 0.4115849414317103, 'lr': 0.002838330719240797, 'batch_size': 192, 'epochs': 133}. Best is trial 14 with value: 1.5702060328589544.\n",
      "[I 2024-06-14 23:10:21,986] A new study created in memory with name: no-name-6d08228d-cd4e-4380-aacc-0762702b23f1\n",
      "[I 2024-06-14 23:10:22,668] Trial 0 finished with value: 37.64819145202637 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 640, 'dropout_rate': 0.3771145356806376, 'lr': 0.005271605749496, 'batch_size': 336, 'epochs': 79}. Best is trial 0 with value: 37.64819145202637.\n",
      "[I 2024-06-14 23:10:23,886] Trial 1 finished with value: 24.02988052368164 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 512, 'dropout_rate': 0.47577009206668314, 'lr': 0.005060817893248232, 'batch_size': 368, 'epochs': 103}. Best is trial 1 with value: 24.02988052368164.\n",
      "[I 2024-06-14 23:10:28,829] Trial 2 finished with value: 2.6955153942108154 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 448, 'dropout_rate': 0.3526923300265439, 'lr': 0.0013073940047279107, 'batch_size': 480, 'epochs': 57}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:30,621] Trial 3 finished with value: 76.89065361022949 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 128, 'dropout_rate': 0.4008360156552528, 'lr': 0.007368578040897622, 'batch_size': 112, 'epochs': 91}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:33,656] Trial 4 finished with value: 11.811984539031982 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'dropout_rate': 0.3108173840677423, 'lr': 0.0038014180451142634, 'batch_size': 384, 'epochs': 134}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:34,192] Trial 5 finished with value: 19.921046257019043 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 64, 'dropout_rate': 0.22320434666034977, 'lr': 0.004534531837496148, 'batch_size': 368, 'epochs': 53}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:35,960] Trial 6 finished with value: 2.824387788772583 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2233819579379918, 'lr': 0.005468224035027197, 'batch_size': 368, 'epochs': 105}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:36,614] Trial 7 finished with value: 25.469412803649902 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 448, 'dropout_rate': 0.5633703166250542, 'lr': 0.0013920071006074163, 'batch_size': 320, 'epochs': 122}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:37,629] Trial 8 finished with value: 11.07714056968689 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 64, 'dropout_rate': 0.4600102755644951, 'lr': 0.0047871822084290284, 'batch_size': 112, 'epochs': 73}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:38,121] Trial 9 finished with value: 14.000553131103516 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'dropout_rate': 0.4487489075733696, 'lr': 0.009125446256824678, 'batch_size': 368, 'epochs': 120}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:40,897] Trial 10 finished with value: 2.8872532844543457 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'dropout_rate': 0.304758896003523, 'lr': 0.001257990027850531, 'batch_size': 496, 'epochs': 51}. Best is trial 2 with value: 2.6955153942108154.\n",
      "[I 2024-06-14 23:10:46,159] Trial 11 finished with value: 2.247746229171753 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2031954431943273, 'lr': 0.0024577745469438394, 'batch_size': 512, 'epochs': 148}. Best is trial 11 with value: 2.247746229171753.\n",
      "[I 2024-06-14 23:10:55,736] Trial 12 finished with value: 1.9067243337631226 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 256, 'dropout_rate': 0.29845599919705146, 'lr': 0.002787417362892719, 'batch_size': 512, 'epochs': 148}. Best is trial 12 with value: 1.9067243337631226.\n",
      "[I 2024-06-14 23:10:57,722] Trial 13 finished with value: 2.5284894704818726 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.20696346391525988, 'lr': 0.003131248752710121, 'batch_size': 208, 'epochs': 150}. Best is trial 12 with value: 1.9067243337631226.\n",
      "[I 2024-06-14 23:11:07,328] Trial 14 finished with value: 1.815034031867981 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 576, 'dropout_rate': 0.2849251153555756, 'lr': 0.0031322341822292896, 'batch_size': 512, 'epochs': 150}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:14,582] Trial 15 finished with value: 2.276991128921509 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 576, 'dropout_rate': 0.3152560072659352, 'lr': 0.006961401878186915, 'batch_size': 448, 'epochs': 137}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:32,044] Trial 16 finished with value: 2.4833236270480685 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 576, 'dropout_rate': 0.2677918582962313, 'lr': 0.002866893492147236, 'batch_size': 16, 'epochs': 132}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:35,165] Trial 17 finished with value: 2.818789482116699 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 576, 'dropout_rate': 0.26469219188305354, 'lr': 0.006931242957160059, 'batch_size': 432, 'epochs': 117}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:37,097] Trial 18 finished with value: 4.6990807056427 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 16, 'dropout_rate': 0.3466068197902975, 'lr': 0.003692778762504388, 'batch_size': 256, 'epochs': 142}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:43,947] Trial 19 finished with value: 2.3485240936279297 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.26922157330292346, 'lr': 0.0022631522877537193, 'batch_size': 432, 'epochs': 123}. Best is trial 14 with value: 1.815034031867981.\n",
      "[I 2024-06-14 23:11:58,646] A new study created in memory with name: no-name-3caac49c-8164-4996-87f6-529f8fcd6fbd\n",
      "[I 2024-06-14 23:12:00,398] Trial 0 finished with value: 12.556305408477783 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'dropout_rate': 0.23017159221215713, 'lr': 0.006016657017242215, 'batch_size': 304, 'epochs': 71}. Best is trial 0 with value: 12.556305408477783.\n",
      "[I 2024-06-14 23:12:01,799] Trial 1 finished with value: 15.263229846954346 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 128, 'dropout_rate': 0.4827500036805001, 'lr': 0.009066297667733713, 'batch_size': 400, 'epochs': 73}. Best is trial 0 with value: 12.556305408477783.\n",
      "[I 2024-06-14 23:12:03,437] Trial 2 finished with value: 4.705169439315796 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 640, 'dropout_rate': 0.31083016085022896, 'lr': 0.009975741302699261, 'batch_size': 304, 'epochs': 87}. Best is trial 2 with value: 4.705169439315796.\n",
      "[I 2024-06-14 23:12:09,264] Trial 3 finished with value: 3.047252575556437 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 640, 'dropout_rate': 0.33657593090626214, 'lr': 0.005394120188661234, 'batch_size': 192, 'epochs': 54}. Best is trial 3 with value: 3.047252575556437.\n",
      "[I 2024-06-14 23:12:18,820] Trial 4 finished with value: 2.7877219915390015 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'dropout_rate': 0.2906123224859166, 'lr': 0.00567973730223239, 'batch_size': 336, 'epochs': 88}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:19,641] Trial 5 finished with value: 12.568652153015137 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 16, 'dropout_rate': 0.21564108702038365, 'lr': 0.0018445454369439814, 'batch_size': 336, 'epochs': 147}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:21,201] Trial 6 finished with value: 3.8549150228500366 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'dropout_rate': 0.36652428675905835, 'lr': 0.005372351892352433, 'batch_size': 368, 'epochs': 98}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:22,422] Trial 7 finished with value: 14.056122779846191 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 64, 'dropout_rate': 0.5893039129074515, 'lr': 0.005757049002763743, 'batch_size': 480, 'epochs': 69}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:24,308] Trial 8 finished with value: 10.448031902313232 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'dropout_rate': 0.4170377357844979, 'lr': 0.0014308981800313534, 'batch_size': 32, 'epochs': 107}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:24,951] Trial 9 finished with value: 67.97673034667969 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 256, 'dropout_rate': 0.5016341257834469, 'lr': 0.003202798278913267, 'batch_size': 448, 'epochs': 97}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:27,086] Trial 10 finished with value: 5.699885845184326 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'dropout_rate': 0.2959341504769269, 'lr': 0.007646707499520438, 'batch_size': 192, 'epochs': 127}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:30,021] Trial 11 finished with value: 3.3963801860809326 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 256, 'dropout_rate': 0.3162372023795204, 'lr': 0.004002229538934429, 'batch_size': 192, 'epochs': 57}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:38,538] Trial 12 finished with value: 2.8765529791514077 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 640, 'dropout_rate': 0.39378589072885717, 'lr': 0.007257537149718441, 'batch_size': 192, 'epochs': 53}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:43,686] Trial 13 finished with value: 2.8257242242495217 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'dropout_rate': 0.41962661447836946, 'lr': 0.007619640413274499, 'batch_size': 80, 'epochs': 118}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:48,594] Trial 14 finished with value: 5.906012508604261 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'dropout_rate': 0.45959951908536567, 'lr': 0.007700740953925694, 'batch_size': 16, 'epochs': 118}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:50,136] Trial 15 finished with value: 4.7605355978012085 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2532274672700424, 'lr': 0.006828685181969347, 'batch_size': 112, 'epochs': 130}. Best is trial 4 with value: 2.7877219915390015.\n",
      "[I 2024-06-14 23:12:55,840] Trial 16 finished with value: 2.7015674114227295 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 448, 'dropout_rate': 0.43358500150109136, 'lr': 0.0039140535507299476, 'batch_size': 112, 'epochs': 85}. Best is trial 16 with value: 2.7015674114227295.\n",
      "[I 2024-06-14 23:12:59,895] Trial 17 finished with value: 3.1954997777938843 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 448, 'dropout_rate': 0.5234458116686094, 'lr': 0.003950622628900121, 'batch_size': 256, 'epochs': 84}. Best is trial 16 with value: 2.7015674114227295.\n",
      "[I 2024-06-14 23:13:14,399] Trial 18 finished with value: 1.6801150143146515 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 576, 'dropout_rate': 0.27340071951882833, 'lr': 0.0029328629129947038, 'batch_size': 128, 'epochs': 85}. Best is trial 18 with value: 1.6801150143146515.\n",
      "[I 2024-06-14 23:13:23,409] Trial 19 finished with value: 2.5161556601524353 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 448, 'dropout_rate': 0.5547972533764771, 'lr': 0.002552943900604007, 'batch_size': 128, 'epochs': 78}. Best is trial 18 with value: 1.6801150143146515.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=448):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_64:\n",
    "    print('Starting MLP reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/64_MLP.pth', encoders, 64)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization 448AP - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [448] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_128 = True\n",
    "SEARCH_KAN_REDUCED_64 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 23:13:31,667] A new study created in memory with name: no-name-431858d0-b518-4986-b5f1-3bae229d6798\n",
      "[I 2024-06-14 23:14:14,544] Trial 0 finished with value: 1.3107733130455017 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 448, 'lr': 0.0038989057924775712, 'batch_size': 304, 'epochs': 77}. Best is trial 0 with value: 1.3107733130455017.\n",
      "[I 2024-06-14 23:15:43,557] Trial 1 finished with value: 1.1337553262710571 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 128, 'lr': 0.009711447963711255, 'batch_size': 448, 'epochs': 99}. Best is trial 1 with value: 1.1337553262710571.\n",
      "[I 2024-06-14 23:18:49,375] Trial 2 finished with value: 0.7131530344486237 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 448, 'lr': 0.0019200058728260664, 'batch_size': 32, 'epochs': 119}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:19:29,685] Trial 3 finished with value: 1.3212135434150696 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 448, 'lr': 0.0018380368180574021, 'batch_size': 288, 'epochs': 58}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:20:25,472] Trial 4 finished with value: 0.8126148581504822 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'lr': 0.003402355383287359, 'batch_size': 160, 'epochs': 104}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:21:14,310] Trial 5 finished with value: 1.024381297606009 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 448, 'lr': 0.008084059340786208, 'batch_size': 16, 'epochs': 73}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:22:00,590] Trial 6 finished with value: 0.914521187543869 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'lr': 0.0013037144420059645, 'batch_size': 416, 'epochs': 143}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:22:49,158] Trial 7 finished with value: 1.5397637287775676 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 512, 'lr': 0.009868801812137264, 'batch_size': 160, 'epochs': 70}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:23:23,898] Trial 8 finished with value: 0.7416584050213849 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'lr': 0.0019563553511312195, 'batch_size': 16, 'epochs': 58}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:24:10,896] Trial 9 finished with value: 1.8924020528793335 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 512, 'lr': 0.008998237063988772, 'batch_size': 368, 'epochs': 109}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:24:30,431] Trial 10 finished with value: 0.9483998815218607 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 576, 'lr': 0.006614510638021585, 'batch_size': 144, 'epochs': 145}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:24:54,419] Trial 11 finished with value: 0.998200257619222 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 576, 'lr': 0.0034089311339165207, 'batch_size': 16, 'epochs': 126}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:25:02,959] Trial 12 finished with value: 1.226789903640747 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'lr': 0.0054072743103211985, 'batch_size': 96, 'epochs': 51}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:25:32,060] Trial 13 finished with value: 1.0636843613215856 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 640, 'lr': 0.0022262016048301527, 'batch_size': 64, 'epochs': 122}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:25:51,232] Trial 14 finished with value: 0.892967700958252 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'lr': 0.00493513692489963, 'batch_size': 224, 'epochs': 90}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:26:55,149] Trial 15 finished with value: 1.120801329612732 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'lr': 0.0010026051664293986, 'batch_size': 512, 'epochs': 128}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:27:41,127] Trial 16 finished with value: 0.780275916059812 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'lr': 0.0025946626330333638, 'batch_size': 80, 'epochs': 91}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:28:06,011] Trial 17 finished with value: 0.7853127717971802 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 512, 'lr': 0.006593228105238222, 'batch_size': 208, 'epochs': 120}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:29:26,873] Trial 18 finished with value: 0.9337102058860991 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 576, 'lr': 0.004506899897135638, 'batch_size': 16, 'epochs': 113}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:29:47,402] Trial 19 finished with value: 0.8565597385168076 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 576, 'lr': 0.0027809425289940645, 'batch_size': 112, 'epochs': 134}. Best is trial 2 with value: 0.7131530344486237.\n",
      "[I 2024-06-14 23:31:26,694] A new study created in memory with name: no-name-9a2e422d-61a2-4674-b427-f78b6bfc930f\n",
      "[I 2024-06-14 23:31:53,524] Trial 0 finished with value: 1.670828938484192 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 640, 'lr': 0.003244704221391328, 'batch_size': 256, 'epochs': 125}. Best is trial 0 with value: 1.670828938484192.\n",
      "[I 2024-06-14 23:32:22,258] Trial 1 finished with value: 1.469997763633728 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.009619035804565277, 'batch_size': 384, 'epochs': 124}. Best is trial 1 with value: 1.469997763633728.\n",
      "[I 2024-06-14 23:32:53,725] Trial 2 finished with value: 1.31405508518219 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'lr': 0.004177382579843583, 'batch_size': 304, 'epochs': 50}. Best is trial 2 with value: 1.31405508518219.\n",
      "[I 2024-06-14 23:33:26,460] Trial 3 finished with value: 1.7522794008255005 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 448, 'lr': 0.004327333013057768, 'batch_size': 176, 'epochs': 126}. Best is trial 2 with value: 1.31405508518219.\n",
      "[I 2024-06-14 23:33:35,344] Trial 4 finished with value: 1.5590659379959106 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 448, 'lr': 0.0032500413924522412, 'batch_size': 480, 'epochs': 121}. Best is trial 2 with value: 1.31405508518219.\n",
      "[I 2024-06-14 23:33:46,015] Trial 5 finished with value: 1.4555052121480305 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 576, 'lr': 0.007302161002179743, 'batch_size': 208, 'epochs': 128}. Best is trial 2 with value: 1.31405508518219.\n",
      "[I 2024-06-14 23:34:29,425] Trial 6 finished with value: 1.2654467821121216 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 640, 'lr': 0.004016207539584748, 'batch_size': 160, 'epochs': 150}. Best is trial 6 with value: 1.2654467821121216.\n",
      "[I 2024-06-14 23:34:39,058] Trial 7 finished with value: 1.2578001618385315 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'lr': 0.006352132176304154, 'batch_size': 80, 'epochs': 105}. Best is trial 7 with value: 1.2578001618385315.\n",
      "[I 2024-06-14 23:35:06,405] Trial 8 finished with value: 1.1196311712265015 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 512, 'lr': 0.007261514734478699, 'batch_size': 64, 'epochs': 98}. Best is trial 8 with value: 1.1196311712265015.\n",
      "[I 2024-06-14 23:35:36,574] Trial 9 finished with value: 1.5328465700149536 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 512, 'lr': 0.008283059337195087, 'batch_size': 304, 'epochs': 111}. Best is trial 8 with value: 1.1196311712265015.\n",
      "[I 2024-06-14 23:36:08,102] Trial 10 finished with value: 0.9803368371945841 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0012272330928405385, 'batch_size': 16, 'epochs': 76}. Best is trial 10 with value: 0.9803368371945841.\n",
      "[I 2024-06-14 23:36:47,150] Trial 11 finished with value: 0.9664667226650097 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0014395146043522456, 'batch_size': 16, 'epochs': 77}. Best is trial 11 with value: 0.9664667226650097.\n",
      "[I 2024-06-14 23:37:03,721] Trial 12 finished with value: 1.2914451978824757 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0010247521346909748, 'batch_size': 16, 'epochs': 73}. Best is trial 11 with value: 0.9664667226650097.\n",
      "[I 2024-06-14 23:37:52,513] Trial 13 finished with value: 0.8851927016620282 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0013503992729429273, 'batch_size': 16, 'epochs': 75}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:37:58,980] Trial 14 finished with value: 1.4314440488815308 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 640, 'lr': 0.002205595411878749, 'batch_size': 112, 'epochs': 82}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:38:14,344] Trial 15 finished with value: 1.101423516869545 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 16, 'lr': 0.002465920854472524, 'batch_size': 112, 'epochs': 58}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:38:26,008] Trial 16 finished with value: 1.0938037995781218 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'lr': 0.00510221457321979, 'batch_size': 32, 'epochs': 90}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:38:33,908] Trial 17 finished with value: 1.5365715026855469 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0019824830731770974, 'batch_size': 496, 'epochs': 65}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:38:42,308] Trial 18 finished with value: 1.2974063903093338 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.0030930998470946242, 'batch_size': 128, 'epochs': 88}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:38:50,235] Trial 19 finished with value: 1.457052230834961 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 64, 'lr': 0.005302031122405756, 'batch_size': 384, 'epochs': 66}. Best is trial 13 with value: 0.8851927016620282.\n",
      "[I 2024-06-14 23:39:32,999] A new study created in memory with name: no-name-c0aa2996-5390-4658-a786-78ef6546e428\n",
      "[I 2024-06-14 23:40:47,924] Trial 0 finished with value: 1.9885454177856445 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 576, 'lr': 0.007259564561825218, 'batch_size': 496, 'epochs': 62}. Best is trial 0 with value: 1.9885454177856445.\n",
      "[I 2024-06-14 23:41:31,310] Trial 1 finished with value: 1.4242533445358276 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 64, 'lr': 0.005019774717608361, 'batch_size': 432, 'epochs': 87}. Best is trial 1 with value: 1.4242533445358276.\n",
      "[I 2024-06-14 23:41:51,088] Trial 2 finished with value: 1.2832545240720112 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 32, 'lr': 0.001826600977176746, 'batch_size': 160, 'epochs': 104}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:41:57,063] Trial 3 finished with value: 1.643075317144394 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 16, 'lr': 0.00667178605648112, 'batch_size': 128, 'epochs': 72}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:15,330] Trial 4 finished with value: 1.3639055490493774 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 16, 'lr': 0.004451083325573393, 'batch_size': 320, 'epochs': 52}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:22,342] Trial 5 finished with value: 1.6744529349463326 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'lr': 0.003852683808986777, 'batch_size': 64, 'epochs': 104}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:29,850] Trial 6 finished with value: 1.9067644476890564 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 64, 'lr': 0.009721997780688732, 'batch_size': 384, 'epochs': 66}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:43,116] Trial 7 finished with value: 1.4867500364780426 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 640, 'lr': 0.009292976262733833, 'batch_size': 128, 'epochs': 96}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:54,216] Trial 8 finished with value: 1.441846787929535 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 448, 'lr': 0.0030894377121359414, 'batch_size': 288, 'epochs': 137}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:42:59,374] Trial 9 finished with value: 1.7686012387275696 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'lr': 0.0068075350379711635, 'batch_size': 240, 'epochs': 101}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:43:36,137] Trial 10 finished with value: 1.3539437916543748 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 256, 'lr': 0.0011414223576864174, 'batch_size': 16, 'epochs': 126}. Best is trial 2 with value: 1.2832545240720112.\n",
      "[I 2024-06-14 23:44:05,013] Trial 11 finished with value: 1.2685630066054208 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 256, 'lr': 0.0011029235187483112, 'batch_size': 32, 'epochs': 134}. Best is trial 11 with value: 1.2685630066054208.\n",
      "[I 2024-06-14 23:44:16,969] Trial 12 finished with value: 1.2633617917696636 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'lr': 0.0011102882897586234, 'batch_size': 192, 'epochs': 118}. Best is trial 12 with value: 1.2633617917696636.\n",
      "[I 2024-06-14 23:44:34,999] Trial 13 finished with value: 0.9987750848134359 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'lr': 0.0025827793697754957, 'batch_size': 208, 'epochs': 148}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:44:48,976] Trial 14 finished with value: 1.1026841402053833 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.0026841103539950173, 'batch_size': 208, 'epochs': 149}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:45:10,954] Trial 15 finished with value: 1.2288737297058105 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.0026543407721305616, 'batch_size': 224, 'epochs': 148}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:45:24,059] Trial 16 finished with value: 1.2554633617401123 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.003044959639437027, 'batch_size': 336, 'epochs': 147}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:45:56,906] Trial 17 finished with value: 1.6336669921875 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 640, 'lr': 0.005779123406009827, 'batch_size': 96, 'epochs': 118}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:47:07,592] Trial 18 finished with value: 1.2590633034706116 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 576, 'lr': 0.002327650560055916, 'batch_size': 272, 'epochs': 139}. Best is trial 13 with value: 0.9987750848134359.\n",
      "[I 2024-06-14 23:47:45,274] Trial 19 finished with value: 1.1806827584902446 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 512, 'lr': 0.003854174053857991, 'batch_size': 192, 'epochs': 125}. Best is trial 13 with value: 0.9987750848134359.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 448, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_64:\n",
    "    print('Starting KAN reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 64, f'./models/KAN/64_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization 448AP - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth',\n",
    "        '64': './models/MLP/64_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth',\n",
    "        '64': './models/KAN/64_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '128': ['./models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth'],\n",
    "        '64': ['./models/MLP/64_encoder_256.pth', './models/MLP/64_encoder_128.pth', './models/MLP/64_encoder_64.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '128': ['./models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth'],\n",
    "        '64': ['./models/KAN/64_encoder_256.pth', './models/KAN/64_encoder_128.pth', './models/KAN/64_encoder_64.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 64, the SAE will have 256 -> 128 -> 64\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 448\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 256 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [448, 640], 0.20361991807188823, 448)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [576, 576], 0.2849251153555756, 128)\n",
    "    mlp_64 = load_MLP_model(model_paths['MLP']['64'], [640, 448, 576], 0.27340071951882833, 64)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [576, 128, 576, 448], 448)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [256, 32], 128)\n",
    "    kan_64 = load_KAN_model(model_paths['KAN']['64'], [128, 448, 32, 32], 64)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    mlp_SAE_64 = load_SAE(SAE_paths['MLP']['64'], 64)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    kan_SAE_64 = load_SAE(SAE_paths['KAN']['64'], 64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy() # Predictions from the model (on CPU) - we need to move them to CPU to use numpy\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1)) # Euclidean distance between predictions and ground-truth\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    mlp_test_data_encoded_64 = stacked_encode_data(X_test_tensor, mlp_SAE_64)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    kan_test_data_encoded_64 = stacked_encode_data(X_test_tensor, kan_SAE_64)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    mlp_64_distances = evaluate_model(mlp_64, mlp_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    kan_64_distances = evaluate_model(kan_64, kan_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    print(mlp_64_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_128_distances.shape)\n",
    "    print(kan_64_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Statistics for  MLP_64\n",
      "MLP_64: 25th percentile: 1.27 - 50th percentile: 1.98 - 75th percentile: 2.87 - Outliers: 366 (2.82%)\n",
      "Max distance: 9.44\n",
      "====================================\n",
      "Statistics for  MLP_128\n",
      "MLP_128: 25th percentile: 1.21 - 50th percentile: 1.87 - 75th percentile: 2.72 - Outliers: 358 (2.76%)\n",
      "Max distance: 8.82\n",
      "====================================\n",
      "Statistics for  MLP_full\n",
      "MLP_full: 25th percentile: 1.02 - 50th percentile: 1.66 - 75th percentile: 2.55 - Outliers: 548 (4.23%)\n",
      "Max distance: 9.51\n",
      "====================================\n",
      "Statistics for  KAN_64\n",
      "KAN_64: 25th percentile: 0.96 - 50th percentile: 1.54 - 75th percentile: 2.42 - Outliers: 635 (4.90%)\n",
      "Max distance: 10.40\n",
      "====================================\n",
      "Statistics for  KAN_128\n",
      "KAN_128: 25th percentile: 0.91 - 50th percentile: 1.35 - 75th percentile: 2.24 - Outliers: 954 (7.36%)\n",
      "Max distance: 9.96\n",
      "====================================\n",
      "Statistics for  KAN_full\n",
      "KAN_full: 25th percentile: 0.86 - 50th percentile: 1.28 - 75th percentile: 1.94 - Outliers: 1154 (8.90%)\n",
      "Max distance: 9.82\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_64': mlp_64_distances,\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_64': kan_64_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[1].set_title('Reduced models (128)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_64'], data['KAN_64']], labels=['MLP_64', 'KAN_64'])\n",
    "    axes[2].set_title('Reduced models (64)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print model statistics: 25th, 50th (median), 75th percentiles, and the amount or percentage of outliers\n",
    "    for key in data:\n",
    "        print(\"====================================\")\n",
    "        print(\"Statistics for \", key)\n",
    "        distances = data[key]\n",
    "        q25, q50, q75 = np.percentile(distances, [25, 50, 75])\n",
    "        iqr = q75 - q25\n",
    "        lower_bound = q25 - 1.5 * iqr\n",
    "        upper_bound = q75 + 1.5 * iqr\n",
    "        outliers = np.sum((distances < lower_bound) | (distances > upper_bound))\n",
    "        print(f'{key}: 25th percentile: {q25:.2f} - 50th percentile: {q50:.2f} - 75th percentile: {q75:.2f} - Outliers: {outliers} ({outliers/len(distances)*100:.2f}%)')\n",
    "        \n",
    "        # Print max predicted distance\n",
    "        max_distance = np.max(distances)\n",
    "        print(f'Max distance: {max_distance:.2f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
