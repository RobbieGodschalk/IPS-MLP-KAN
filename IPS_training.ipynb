{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (7200, 623)\n",
      "df_train_x: (6480, 620)\n",
      "df_train_y: (6480, 2)\n",
      "df_val_x: (720, 620)\n",
      "df_val_y: (720, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test01-02-03_full: (21600, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01', '02', '03'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test01-02-03_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=620):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_256 = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 00:42:09,227] A new study created in memory with name: no-name-2bd7c723-abdc-402d-9b72-b5e9327c4be9\n",
      "[I 2024-06-13 00:42:17,506] Trial 0 finished with value: 3.7385453581809998 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 512, 'dropout_rate': 0.530777174709295, 'lr': 0.009414480539151126, 'batch_size': 208, 'epochs': 116}. Best is trial 0 with value: 3.7385453581809998.\n",
      "[I 2024-06-13 00:42:21,299] Trial 1 finished with value: 18.04011599222819 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 700, 'dropout_rate': 0.38645061834211925, 'lr': 0.005518290200840876, 'batch_size': 272, 'epochs': 137}. Best is trial 0 with value: 3.7385453581809998.\n",
      "[I 2024-06-13 00:42:29,011] Trial 2 finished with value: 4.738371133804321 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'dropout_rate': 0.5236375502019952, 'lr': 0.008413492676234348, 'batch_size': 224, 'epochs': 63}. Best is trial 0 with value: 3.7385453581809998.\n",
      "[I 2024-06-13 00:42:31,317] Trial 3 finished with value: 60.771902084350586 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 700, 'dropout_rate': 0.5639556789636927, 'lr': 0.0019744771600992486, 'batch_size': 496, 'epochs': 119}. Best is trial 0 with value: 3.7385453581809998.\n",
      "[I 2024-06-13 00:42:43,221] Trial 4 finished with value: 1.65137775739034 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 620, 'dropout_rate': 0.500715044695168, 'lr': 0.001615280446617703, 'batch_size': 352, 'epochs': 119}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:42:48,966] Trial 5 finished with value: 2.982417027155558 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 700, 'dropout_rate': 0.5528722753352846, 'lr': 0.008390203233261936, 'batch_size': 128, 'epochs': 62}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:42:53,386] Trial 6 finished with value: 3.281627893447876 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.5448168728703389, 'lr': 0.006732506696484511, 'batch_size': 432, 'epochs': 90}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:42:59,459] Trial 7 finished with value: 8.340719858805338 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 64, 'dropout_rate': 0.26873712664026267, 'lr': 0.006139055583881628, 'batch_size': 288, 'epochs': 110}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:43:08,407] Trial 8 finished with value: 3.2074350118637085 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 256, 'dropout_rate': 0.202956563765317, 'lr': 0.008203946746627392, 'batch_size': 128, 'epochs': 110}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:43:10,378] Trial 9 finished with value: 11.05473279953003 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'dropout_rate': 0.21488344483910585, 'lr': 0.0033660726981156954, 'batch_size': 192, 'epochs': 145}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:43:44,909] Trial 10 finished with value: 2.619760338465373 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 620, 'dropout_rate': 0.4473915282910941, 'lr': 0.00120204286705576, 'batch_size': 16, 'epochs': 87}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:00,178] Trial 11 finished with value: 2.760970986407736 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 620, 'dropout_rate': 0.4470904599868111, 'lr': 0.0016106935237140648, 'batch_size': 32, 'epochs': 87}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:01,603] Trial 12 finished with value: 30.604533513387043 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 620, 'dropout_rate': 0.4441934597529514, 'lr': 0.0034937631595695195, 'batch_size': 352, 'epochs': 82}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:02,508] Trial 13 finished with value: 34.77249336242676 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 32, 'dropout_rate': 0.3670361492811675, 'lr': 0.0035709406741323644, 'batch_size': 368, 'epochs': 130}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:40,615] Trial 14 finished with value: 2.2977279583613077 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'dropout_rate': 0.47008412347273154, 'lr': 0.0012304156892892538, 'batch_size': 16, 'epochs': 99}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:45,455] Trial 15 finished with value: 5.591506242752075 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'dropout_rate': 0.4827827726035081, 'lr': 0.002727831550820869, 'batch_size': 368, 'epochs': 104}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:46,865] Trial 16 finished with value: 26.62000560760498 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'dropout_rate': 0.34408832472327877, 'lr': 0.004727447471328472, 'batch_size': 464, 'epochs': 127}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:50,166] Trial 17 finished with value: 11.375117897987366 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 128, 'dropout_rate': 0.3174690954276981, 'lr': 0.0010131077551227378, 'batch_size': 96, 'epochs': 96}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:55,521] Trial 18 finished with value: 2.3129540284474692 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.5904448962951927, 'lr': 0.002456528886047492, 'batch_size': 320, 'epochs': 73}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:44:56,894] Trial 19 finished with value: 43.776926040649414 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 16, 'dropout_rate': 0.4746489874144483, 'lr': 0.00445863246357381, 'batch_size': 400, 'epochs': 101}. Best is trial 4 with value: 1.65137775739034.\n",
      "[I 2024-06-13 00:45:20,582] A new study created in memory with name: no-name-ee3046a0-d9e0-4d11-9fbf-8448cbf09f89\n",
      "[I 2024-06-13 00:45:23,112] Trial 0 finished with value: 7.851912975311279 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 16, 'dropout_rate': 0.23836724423321112, 'lr': 0.0019106299315388988, 'batch_size': 496, 'epochs': 119}. Best is trial 0 with value: 7.851912975311279.\n",
      "[I 2024-06-13 00:45:29,622] Trial 1 finished with value: 3.674237542682224 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'dropout_rate': 0.5877046852888282, 'lr': 0.004275948572595034, 'batch_size': 80, 'epochs': 119}. Best is trial 1 with value: 3.674237542682224.\n",
      "[I 2024-06-13 00:45:34,381] Trial 2 finished with value: 3.0588985284169516 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 256, 'dropout_rate': 0.4582296731052154, 'lr': 0.006451044444405008, 'batch_size': 320, 'epochs': 66}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:37,310] Trial 3 finished with value: 8.594282468159994 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 620, 'dropout_rate': 0.38394145874036945, 'lr': 0.0022309631874366125, 'batch_size': 304, 'epochs': 52}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:40,284] Trial 4 finished with value: 4.820269266764323 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 128, 'dropout_rate': 0.38194487273580713, 'lr': 0.009780919586600639, 'batch_size': 288, 'epochs': 59}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:42,959] Trial 5 finished with value: 14.31418228149414 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 256, 'dropout_rate': 0.49609359837813005, 'lr': 0.00807889952167827, 'batch_size': 416, 'epochs': 88}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:43,870] Trial 6 finished with value: 16.571351051330566 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 700, 'dropout_rate': 0.22931360094015277, 'lr': 0.005140992743091078, 'batch_size': 352, 'epochs': 75}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:47,579] Trial 7 finished with value: 8.986717224121094 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 256, 'dropout_rate': 0.20957250881429973, 'lr': 0.008532841194167171, 'batch_size': 480, 'epochs': 99}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:52,175] Trial 8 finished with value: 12.608696195814344 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 16, 'dropout_rate': 0.20633408465471348, 'lr': 0.0010362559241049773, 'batch_size': 80, 'epochs': 84}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:45:58,368] Trial 9 finished with value: 3.2736333847045898 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'dropout_rate': 0.33099512022284955, 'lr': 0.005315532664763311, 'batch_size': 176, 'epochs': 119}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:01,605] Trial 10 finished with value: 3.5362008810043335 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'dropout_rate': 0.493979095416327, 'lr': 0.007322415873873108, 'batch_size': 192, 'epochs': 138}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:12,584] Trial 11 finished with value: 11.300464868545532 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 620, 'dropout_rate': 0.3269133316644339, 'lr': 0.006450189491662265, 'batch_size': 192, 'epochs': 126}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:15,276] Trial 12 finished with value: 12.811706066131592 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.3151001904145756, 'lr': 0.004395546924709335, 'batch_size': 192, 'epochs': 106}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:31,717] Trial 13 finished with value: 8.256176264389701 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'dropout_rate': 0.46327655008220064, 'lr': 0.0062599607986630525, 'batch_size': 32, 'epochs': 141}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:32,696] Trial 14 finished with value: 27.096797307332356 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'dropout_rate': 0.43632423810877896, 'lr': 0.0033612280025295525, 'batch_size': 240, 'epochs': 65}. Best is trial 2 with value: 3.0588985284169516.\n",
      "[I 2024-06-13 00:46:39,045] Trial 15 finished with value: 2.5528886318206787 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.29939913412687036, 'lr': 0.005908888500861161, 'batch_size': 368, 'epochs': 103}. Best is trial 15 with value: 2.5528886318206787.\n",
      "[I 2024-06-13 00:46:41,023] Trial 16 finished with value: 8.148159980773926 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 256, 'dropout_rate': 0.5637074466171019, 'lr': 0.006749213158022975, 'batch_size': 384, 'epochs': 101}. Best is trial 15 with value: 2.5528886318206787.\n",
      "[I 2024-06-13 00:46:49,676] Trial 17 finished with value: 2.6643450260162354 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2656093617791827, 'lr': 0.00802792312818847, 'batch_size': 432, 'epochs': 72}. Best is trial 15 with value: 2.5528886318206787.\n",
      "[I 2024-06-13 00:46:51,267] Trial 18 finished with value: 17.578109741210938 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2804739003492392, 'lr': 0.009822034188678894, 'batch_size': 432, 'epochs': 93}. Best is trial 15 with value: 2.5528886318206787.\n",
      "[I 2024-06-13 00:47:06,216] Trial 19 finished with value: 5.898510217666626 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2685808805903301, 'lr': 0.008579141246657625, 'batch_size': 448, 'epochs': 77}. Best is trial 15 with value: 2.5528886318206787.\n",
      "[I 2024-06-13 00:47:24,789] A new study created in memory with name: no-name-6c0635a7-46ec-4a3c-b951-365e767806bb\n",
      "[I 2024-06-13 00:47:29,343] Trial 0 finished with value: 8.52212142944336 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 256, 'dropout_rate': 0.5999022002537737, 'lr': 0.0027631011513908187, 'batch_size': 480, 'epochs': 67}. Best is trial 0 with value: 8.52212142944336.\n",
      "[I 2024-06-13 00:47:30,018] Trial 1 finished with value: 17.02139377593994 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'dropout_rate': 0.5000871207186144, 'lr': 0.008831562277445765, 'batch_size': 464, 'epochs': 80}. Best is trial 0 with value: 8.52212142944336.\n",
      "[I 2024-06-13 00:47:36,433] Trial 2 finished with value: 2.8242153326670327 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.2091428602134823, 'lr': 0.004750298809837333, 'batch_size': 64, 'epochs': 130}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:47:45,772] Trial 3 finished with value: 11.623406410217285 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 620, 'dropout_rate': 0.27762437070255575, 'lr': 0.008034063500505673, 'batch_size': 384, 'epochs': 84}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:47:55,027] Trial 4 finished with value: 7.635500223740287 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 620, 'dropout_rate': 0.30730037563418916, 'lr': 0.00712178487473958, 'batch_size': 32, 'epochs': 148}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:01,154] Trial 5 finished with value: 3.201138496398926 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'dropout_rate': 0.524024081033535, 'lr': 0.008087547954868476, 'batch_size': 480, 'epochs': 136}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:01,820] Trial 6 finished with value: 23.17561912536621 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 64, 'dropout_rate': 0.2912297276518034, 'lr': 0.0010166298961704022, 'batch_size': 512, 'epochs': 145}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:14,663] Trial 7 finished with value: 5.977435906728108 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 620, 'dropout_rate': 0.2877177759553172, 'lr': 0.006431237246903325, 'batch_size': 256, 'epochs': 128}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:17,321] Trial 8 finished with value: 3.465321445465088 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'dropout_rate': 0.280027562684923, 'lr': 0.007718423807458602, 'batch_size': 176, 'epochs': 120}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:20,958] Trial 9 finished with value: 8.933559962681361 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 16, 'dropout_rate': 0.30573586191762214, 'lr': 0.006950133874275851, 'batch_size': 112, 'epochs': 54}. Best is trial 2 with value: 2.8242153326670327.\n",
      "[I 2024-06-13 00:48:31,715] Trial 10 finished with value: 2.6346285343170166 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.20605008101636504, 'lr': 0.004368576614810367, 'batch_size': 32, 'epochs': 108}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:48:47,425] Trial 11 finished with value: 3.078440523147583 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.21313765925274203, 'lr': 0.004276999183126501, 'batch_size': 16, 'epochs': 114}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:48:51,996] Trial 12 finished with value: 2.916839463370187 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.20217755243223334, 'lr': 0.004654935444754131, 'batch_size': 112, 'epochs': 102}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:48:55,291] Trial 13 finished with value: 11.156408854893275 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'dropout_rate': 0.399710286728585, 'lr': 0.0032499336217392512, 'batch_size': 112, 'epochs': 105}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:49:02,093] Trial 14 finished with value: 2.814746697743734 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.37768272398977126, 'lr': 0.005278829626561553, 'batch_size': 240, 'epochs': 91}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:49:06,761] Trial 15 finished with value: 2.99979035059611 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'dropout_rate': 0.3867317533140116, 'lr': 0.005846924057914879, 'batch_size': 272, 'epochs': 88}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:49:09,293] Trial 16 finished with value: 3.4929099082946777 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.4730255236080352, 'lr': 0.009829234008762417, 'batch_size': 272, 'epochs': 94}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:49:12,209] Trial 17 finished with value: 3.069017171859741 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.3546529988789235, 'lr': 0.0032063393335465228, 'batch_size': 352, 'epochs': 69}. Best is trial 10 with value: 2.6346285343170166.\n",
      "[I 2024-06-13 00:49:27,557] Trial 18 finished with value: 2.2705852687358856 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 512, 'dropout_rate': 0.4381069933899059, 'lr': 0.0013293703886718194, 'batch_size': 224, 'epochs': 107}. Best is trial 18 with value: 2.2705852687358856.\n",
      "[I 2024-06-13 00:49:39,679] Trial 19 finished with value: 2.552836298942566 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 512, 'dropout_rate': 0.4324498383369471, 'lr': 0.0013378828630828754, 'batch_size': 192, 'epochs': 112}. Best is trial 18 with value: 2.2705852687358856.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=620):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_256:\n",
    "    print('Starting MLP reduced grid search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/256_MLP.pth', encoders, 256)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [620] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_256 = True\n",
    "SEARCH_KAN_REDUCED_128 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 00:49:55,788] A new study created in memory with name: no-name-0e103cb3-9e59-4861-874b-b3aeeea408fb\n",
      "[I 2024-06-13 00:51:17,756] Trial 0 finished with value: 1.302079478899638 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 620, 'lr': 0.00885344912219116, 'batch_size': 256, 'epochs': 100}. Best is trial 0 with value: 1.302079478899638.\n",
      "[I 2024-06-13 00:53:37,884] Trial 1 finished with value: 0.896842360496521 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'hidden_layer_size_3': 620, 'lr': 0.0017933479474781522, 'batch_size': 96, 'epochs': 89}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 00:55:32,178] Trial 2 finished with value: 1.111131529013316 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'lr': 0.008749632027708927, 'batch_size': 240, 'epochs': 133}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 00:57:22,785] Trial 3 finished with value: 0.9767742604017258 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 700, 'lr': 0.0024994855177033607, 'batch_size': 208, 'epochs': 112}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 00:57:47,501] Trial 4 finished with value: 1.0553333014249802 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 512, 'lr': 0.00907058570830005, 'batch_size': 192, 'epochs': 69}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 00:58:14,238] Trial 5 finished with value: 1.305189073085785 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'lr': 0.006561574343013946, 'batch_size': 448, 'epochs': 88}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 00:59:45,140] Trial 6 finished with value: 0.9829405281278822 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 16, 'lr': 0.004727591660912659, 'batch_size': 80, 'epochs': 94}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 01:00:57,096] Trial 7 finished with value: 1.8095215956370037 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 620, 'lr': 0.009232654710981454, 'batch_size': 336, 'epochs': 88}. Best is trial 1 with value: 0.896842360496521.\n",
      "[I 2024-06-13 01:02:39,696] Trial 8 finished with value: 0.8142674684524536 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 32, 'lr': 0.0021266328903453728, 'batch_size': 144, 'epochs': 101}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:03:08,620] Trial 9 finished with value: 0.8882777571678162 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'lr': 0.004709903918051273, 'batch_size': 160, 'epochs': 55}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:04:11,316] Trial 10 finished with value: 1.0056074158005093 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 32, 'lr': 0.003715385521813358, 'batch_size': 32, 'epochs': 146}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:04:41,367] Trial 11 finished with value: 0.9347357948621114 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'lr': 0.006275151485310757, 'batch_size': 128, 'epochs': 52}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:06:55,818] Trial 12 finished with value: 1.004106044769287 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 256, 'lr': 0.0011841860459916042, 'batch_size': 384, 'epochs': 116}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:07:13,455] Trial 13 finished with value: 0.8557746171951294 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'lr': 0.003388396541016653, 'batch_size': 144, 'epochs': 71}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:09:50,773] Trial 14 finished with value: 0.9658180620935228 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 64, 'lr': 0.003077447686916134, 'batch_size': 16, 'epochs': 74}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:10:23,505] Trial 15 finished with value: 1.1655948758125305 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 128, 'lr': 0.0037100394807025386, 'batch_size': 512, 'epochs': 73}. Best is trial 8 with value: 0.8142674684524536.\n",
      "[I 2024-06-13 01:11:34,898] Trial 16 finished with value: 0.7574800848960876 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 32, 'lr': 0.0022734451141979937, 'batch_size': 304, 'epochs': 114}. Best is trial 16 with value: 0.7574800848960876.\n",
      "[I 2024-06-13 01:12:35,301] Trial 17 finished with value: 0.8289628426233927 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 32, 'lr': 0.002039872825996086, 'batch_size': 336, 'epochs': 116}. Best is trial 16 with value: 0.7574800848960876.\n",
      "[I 2024-06-13 01:13:45,869] Trial 18 finished with value: 0.9996962745984396 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 16, 'lr': 0.0011575167503607682, 'batch_size': 320, 'epochs': 130}. Best is trial 16 with value: 0.7574800848960876.\n",
      "[I 2024-06-13 01:15:18,916] Trial 19 finished with value: 1.0584994355837505 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 32, 'lr': 0.007390200764959205, 'batch_size': 304, 'epochs': 105}. Best is trial 16 with value: 0.7574800848960876.\n",
      "[I 2024-06-13 01:16:23,900] A new study created in memory with name: no-name-4c34ab4d-55a7-4bfc-a0ae-25040f7e7dc8\n",
      "[I 2024-06-13 01:17:09,178] Trial 0 finished with value: 1.3352412051624722 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'lr': 0.001482617864710781, 'batch_size': 80, 'epochs': 60}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:17:44,799] Trial 1 finished with value: 1.872126817703247 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'lr': 0.004088757439851188, 'batch_size': 400, 'epochs': 112}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:19:02,353] Trial 2 finished with value: 1.7436213493347168 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'lr': 0.0056450222788289655, 'batch_size': 320, 'epochs': 116}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:19:29,324] Trial 3 finished with value: 1.803584893544515 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 16, 'lr': 0.00955660070219735, 'batch_size': 256, 'epochs': 94}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:19:47,352] Trial 4 finished with value: 1.6354888379573822 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'lr': 0.003505326088310844, 'batch_size': 64, 'epochs': 53}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:20:58,782] Trial 5 finished with value: 1.7244788110256195 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'lr': 0.0064871147479666075, 'batch_size': 192, 'epochs': 94}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:21:19,084] Trial 6 finished with value: 1.4980319887399673 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'lr': 0.005038521363445815, 'batch_size': 96, 'epochs': 137}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:22:29,081] Trial 7 finished with value: 1.8427951335906982 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 700, 'lr': 0.009571380093048438, 'batch_size': 288, 'epochs': 136}. Best is trial 0 with value: 1.3352412051624722.\n",
      "[I 2024-06-13 01:22:54,111] Trial 8 finished with value: 1.2822179794311523 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 64, 'lr': 0.009151238685193363, 'batch_size': 320, 'epochs': 110}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:23:12,256] Trial 9 finished with value: 1.486981362104416 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'lr': 0.0075071639314980435, 'batch_size': 192, 'epochs': 146}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:23:24,826] Trial 10 finished with value: 1.6678319573402405 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 64, 'lr': 0.007651085437716378, 'batch_size': 512, 'epochs': 74}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:24:55,825] Trial 11 finished with value: 1.3415989564524757 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'lr': 0.0012790057321584282, 'batch_size': 16, 'epochs': 50}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:25:35,513] Trial 12 finished with value: 1.462690532207489 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'lr': 0.0021599827082311174, 'batch_size': 400, 'epochs': 74}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:26:40,073] Trial 13 finished with value: 1.7479844570159913 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 620, 'lr': 0.0081367146036543, 'batch_size': 144, 'epochs': 77}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:26:52,140] Trial 14 finished with value: 1.5917252898216248 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 64, 'lr': 0.0025602395190586595, 'batch_size': 384, 'epochs': 114}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:27:34,163] Trial 15 finished with value: 1.4927949905395508 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 32, 'lr': 0.004529474688718832, 'batch_size': 512, 'epochs': 104}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:27:47,195] Trial 16 finished with value: 1.864621599515279 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'lr': 0.0010180050459359252, 'batch_size': 240, 'epochs': 64}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:28:27,636] Trial 17 finished with value: 1.3104330499966939 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'lr': 0.006312266461232839, 'batch_size': 336, 'epochs': 85}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:29:03,788] Trial 18 finished with value: 1.6385842561721802 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'lr': 0.008691336855100259, 'batch_size': 336, 'epochs': 85}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:29:40,842] Trial 19 finished with value: 1.5750921964645386 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'lr': 0.006388543926710319, 'batch_size': 432, 'epochs': 125}. Best is trial 8 with value: 1.2822179794311523.\n",
      "[I 2024-06-13 01:30:08,936] A new study created in memory with name: no-name-cb926b7b-940f-4259-a274-7e5660805d8d\n",
      "[I 2024-06-13 01:30:36,066] Trial 0 finished with value: 1.7140275835990906 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 32, 'lr': 0.0022154561697259306, 'batch_size': 448, 'epochs': 83}. Best is trial 0 with value: 1.7140275835990906.\n",
      "[I 2024-06-13 01:30:54,807] Trial 1 finished with value: 1.6478183031082154 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'lr': 0.005147293303843396, 'batch_size': 160, 'epochs': 114}. Best is trial 1 with value: 1.6478183031082154.\n",
      "[I 2024-06-13 01:31:36,441] Trial 2 finished with value: 1.5780016660690308 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 620, 'lr': 0.008895442660654094, 'batch_size': 176, 'epochs': 133}. Best is trial 2 with value: 1.5780016660690308.\n",
      "[I 2024-06-13 01:32:05,255] Trial 3 finished with value: 1.8169276118278503 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'lr': 0.009991322299448276, 'batch_size': 480, 'epochs': 142}. Best is trial 2 with value: 1.5780016660690308.\n",
      "[I 2024-06-13 01:33:22,198] Trial 4 finished with value: 1.4469130039215088 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 700, 'hidden_layer_size_3': 16, 'lr': 0.0022967520659661752, 'batch_size': 272, 'epochs': 94}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:33:54,909] Trial 5 finished with value: 1.6616588433583577 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 128, 'lr': 0.009813869248252446, 'batch_size': 272, 'epochs': 91}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:34:06,478] Trial 6 finished with value: 1.6442941427230835 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'lr': 0.006704456549726175, 'batch_size': 496, 'epochs': 117}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:34:17,599] Trial 7 finished with value: 1.7468775510787964 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'lr': 0.0016806815845535703, 'batch_size': 416, 'epochs': 117}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:34:25,368] Trial 8 finished with value: 1.863441824913025 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'lr': 0.009441296621082399, 'batch_size': 224, 'epochs': 101}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:34:41,701] Trial 9 finished with value: 1.4694762059620448 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'lr': 0.002945570404504717, 'batch_size': 112, 'epochs': 133}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:37:34,651] Trial 10 finished with value: 1.5450287077162002 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 700, 'hidden_layer_size_3': 16, 'lr': 0.003843742041134051, 'batch_size': 16, 'epochs': 55}. Best is trial 4 with value: 1.4469130039215088.\n",
      "[I 2024-06-13 01:38:34,832] Trial 11 finished with value: 1.4441890716552734 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 700, 'lr': 0.0033433257906198863, 'batch_size': 320, 'epochs': 71}. Best is trial 11 with value: 1.4441890716552734.\n",
      "[I 2024-06-13 01:39:32,434] Trial 12 finished with value: 1.8334948619206746 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 700, 'lr': 0.004274265724696937, 'batch_size': 320, 'epochs': 68}. Best is trial 11 with value: 1.4441890716552734.\n",
      "[I 2024-06-13 01:39:44,412] Trial 13 finished with value: 1.8197628657023113 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 128, 'lr': 0.001211718931816523, 'batch_size': 352, 'epochs': 77}. Best is trial 11 with value: 1.4441890716552734.\n",
      "[I 2024-06-13 01:40:09,869] Trial 14 finished with value: 1.915532648563385 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'lr': 0.006283741686660638, 'batch_size': 368, 'epochs': 54}. Best is trial 11 with value: 1.4441890716552734.\n",
      "[I 2024-06-13 01:41:50,497] Trial 15 finished with value: 1.3514817158381145 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 256, 'lr': 0.003054589379121092, 'batch_size': 288, 'epochs': 68}. Best is trial 15 with value: 1.3514817158381145.\n",
      "[I 2024-06-13 01:43:06,566] Trial 16 finished with value: 1.8165444731712341 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 256, 'lr': 0.0036206759404488024, 'batch_size': 384, 'epochs': 66}. Best is trial 15 with value: 1.3514817158381145.\n",
      "[I 2024-06-13 01:43:41,838] Trial 17 finished with value: 2.8688391844431558 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'lr': 0.0048836505740468585, 'batch_size': 304, 'epochs': 70}. Best is trial 15 with value: 1.3514817158381145.\n",
      "[I 2024-06-13 01:44:15,111] Trial 18 finished with value: 1.6032154262065887 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 512, 'lr': 0.00716940222204587, 'batch_size': 224, 'epochs': 51}. Best is trial 15 with value: 1.3514817158381145.\n",
      "[I 2024-06-13 01:45:51,059] Trial 19 finished with value: 1.3041589770998274 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'lr': 0.0030088257557528537, 'batch_size': 112, 'epochs': 83}. Best is trial 19 with value: 1.3041589770998274.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 620, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_256:\n",
    "    print('Starting KAN reduced search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 256, f'./models/KAN/256_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '256': './models/MLP/256_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '256': './models/KAN/256_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '256': ['./models/MLP/256_encoder_512.pth', './models/MLP/256_encoder_256.pth'],\n",
    "        '128': ['./models/MLP/128_encoder_512.pth', './models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '256': ['./models/KAN/256_encoder_512.pth', './models/KAN/256_encoder_256.pth'],\n",
    "        '128': ['./models/KAN/128_encoder_512.pth', './models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 128, the SAE will have 512 -> 256 -> 128\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 620\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 512 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [256, 256, 256, 620], 0.500715044695168, 620)\n",
    "    mlp_256 = load_MLP_model(model_paths['MLP']['256'], [256, 256], 0.29939913412687036, 256)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [620, 620, 512], 0.4381069933899059, 128)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [256, 128, 256, 32], 620)\n",
    "    kan_256 = load_KAN_model(model_paths['KAN']['256'], [32, 64, 256, 64], 256)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [700, 620, 32], 128)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_256 = load_SAE(SAE_paths['MLP']['256'], 256)\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    kan_SAE_256 = load_SAE(SAE_paths['KAN']['256'], 256)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy()\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1))\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_256 = stacked_encode_data(X_test_tensor, mlp_SAE_256)\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    kan_test_data_encoded_256 = stacked_encode_data(X_test_tensor, kan_SAE_256)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_256_distances = evaluate_model(mlp_256, mlp_test_data_encoded_256, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_256_distances = evaluate_model(kan_256, kan_test_data_encoded_256, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_256_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_256_distances.shape)\n",
    "    print(kan_128_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaIAAAIQCAYAAABzOBkOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIvUlEQVR4nOzde1yUZf7/8TcH5SRgoigeIdRk85CHylBK07XVMFmlditLa01LO1jaQXfL3EyyssNaZvbrtJlupsS6ptZmUph08pCyQZKJmqKYJqBykJn794fNfBkBHfAeZhhez8djHs3c9+ee+cwYc839ua77unwMwzAEAAAAAAAAAICL+Lo7AQAAAAAAAACAd6MQDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApCtEAAAAAAAAAAJeiEA0AAAAAAAAAcCkK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRAAAAAAAAAACXohANuEh6erp8fHyUnp5u3zZ+/HhFR0e7LSdnPf744/Lx8anTsQ3lPQIA6tdbb70lHx8f5eXluTuVGvn4+Ojxxx93dxrnFB0drfHjx9fpWLPf49NPP61u3brJarWa9pznY9GiRerYsaPKysrcnQoANEi01+ahva7ZunXr1KxZMx0+fNjdqaCeUYgGKrE1utXdHnnkEXenBwCAqc5s9/z9/dWuXTuNHz9e+/fvd3d68HBFRUWaN2+eHn74Yfn6nj6tOHLkiJ555hldeeWVatWqlZo3b67+/fvrvffeq3K8rdO+utuXX35ZJb68vFxz585Vt27dFBgYqNatW+vaa6/Vzz//bI8ZP368ysvL9eqrr7rujQNAPaO9xvmorr2WpPfee09jx45Vly5d5OPjo0GDBlV7/DfffKO7775bF198sUJCQtSxY0fdcMMN2rlzZ7Xxy5cvV//+/dW8eXNFREToqquu0ocffugQ84c//EGdO3dWSkqKae8TDYO/uxMAPNHf//53xcTEOGzr3r27m7IBAMC1bO1eaWmpvvzyS7311lvauHGjsrKyFBgY6O704KHeeOMNVVRU6MYbb7Rvy8zM1F//+leNGDFCf/vb3+Tv76+VK1fqz3/+s77//nvNnj27yvPce++9uvTSSx22de7c2eHxqVOndO2112rTpk2644471LNnT/3666/66quvVFhYqPbt20uSAgMDNW7cOD333HO655576nyFFwB4Itpr1EV17bUkvfLKK9q8ebMuvfRSHTlypMbj582bpy+++ELXX3+9evbsqYMHD+qll15Snz599OWXXzrUShYsWKB7771X1157rZ566imVlpbqrbfeUmJiolauXKnRo0fbYydNmqTp06dr9uzZCg0NNf+NwyNRiAaqMXz4cPXr18/daQAAUC8qt3sTJkxQy5YtNW/ePK1atUo33HCDm7ODp3rzzTd13XXXORQ/Lr74YuXm5qpTp072bZMnT9bQoUM1b948PfTQQwoJCXF4noSEBCUnJ5/1tZ5//nl99tln2rhxoy677LKzxt5www16+umntWHDBl199dV1eGcA4Jlor1EX1bXXkvTOO++oXbt28vX1PevAuwceeEBLly5V06ZN7dv+9Kc/qUePHnrqqae0ZMkS+/YFCxbo0ksv1X/+8x97Z/Dtt9+udu3a6e2333YoRI8ZM0b33HOP3n//fd1+++1mvV14OKbmAGqpprmazmf+p+qeKzExUenp6erXr5+CgoLUo0cP+3zTqamp6tGjhwIDA9W3b19t3bq1ynN8+umnSkhIUEhIiJo3b65Ro0YpOzu7StzGjRt16aWXKjAwULGxsWe9lHXJkiXq27evgoKC1KJFC/35z3/Wvn37zvl+/vWvf6lv374KDQ1VWFiYevTooRdffNH5DwQAUK8SEhIkSbt27XLYnpOTo+TkZLVo0UKBgYHq16+fVq1aVeX4//3vf7r66qsVFBSk9u3ba86cOdXOSVibNvXYsWO6//77FR0drYCAALVv31633nqrfvnlF3tMWVmZZs2apc6dOysgIEAdOnTQQw89VGW+4LKyMt1///1q1aqVQkNDdd111zlM73A2tukkli9frtmzZ6tdu3YKDQ1VcnKyCgsLVVZWpqlTpyoyMlLNmjXTbbfdVuX1Kyoq9MQTTyg2NlYBAQGKjo7WzJkzq8QZhqE5c+aoffv2Cg4O1uDBg/W///2v2ryOHTumqVOnqkOHDgoICFDnzp01b968c84FWVxcrKlTp9o/18jISP3+97/Xli1bznrc7t27tX37dg0dOtRhe0xMjEMRWjr975yUlKSysjL99NNPNeZRUVFR7T6r1aoXX3xRf/zjH3XZZZepoqJCJ0+erDG3vn37qkWLFvr3v/991vcAAA0d7XXNaK9Pq6m9lqQOHTo4TNVRk/j4eIcitCR16dJFF198cZUaQ1FRkSIjIx2uSAoLC1OzZs0UFBTkEBsZGamePXvSXjcyjIgGqlFYWOjQUEpSy5Yt6zWHH3/8UTfddJMmTZqksWPH6tlnn9XIkSO1aNEizZw5U5MnT5YkpaSk6IYbbtAPP/xgb0Q++eQTDR8+XBdeeKEef/xxlZSUaMGCBRowYIC2bNliX0xwx44dGjZsmFq1aqXHH39cFRUVmjVrllq3bl0lnyeffFKPPvqobrjhBk2YMEGHDx/WggULdOWVV2rr1q1q3rx5te/jv//9r2688UYNGTJE8+bNkyRlZ2friy++0H333Wf+BwcAOG+2BYouuOAC+7b//e9/GjBggNq1a6dHHnlEISEhWr58uZKSkrRy5Ur98Y9/lCQdPHhQgwcPVkVFhT1u8eLFVU4+auP48eNKSEhQdna2br/9dvXp00e//PKLVq1apZ9//lktW7aU1WrVddddp40bN2rixImKi4vTjh079Pzzz2vnzp1KS0uzP9+ECRO0ZMkS3XTTTYqPj9enn36qa6+9tlY5paSkKCgoSI888oh+/PFHLViwQE2aNJGvr69+/fVXPf744/bLpmNiYvTYY485vP7bb7+t5ORkTZs2TV999ZVSUlKUnZ2tDz74wB732GOPac6cORoxYoRGjBihLVu2aNiwYSovL3fI5eTJk7rqqqu0f/9+TZo0SR07dtSmTZs0Y8YM5efn64UXXqjxfdx5551asWKF7r77bv3ud7/TkSNHtHHjRmVnZ6tPnz41Hrdp0yZJOmtMZQcPHpRU/e+p2267TcePH5efn58SEhL0zDPPOFyZ9v333+vAgQPq2bOnJk6cqLffflvl5eX2ju3BgwdXec4+ffroiy++cCo3AGioaK/Pjfa6du21swzD0KFDh3TxxRc7bB80aJBWrFihBQsWaOTIkSotLdWCBQtUWFhY7fl/3759Hf7N0QgYAOzefPNNQ1K1NxtJxqxZs6oc26lTJ2PcuHH2xxs2bDAkGRs2bLBvGzdunNGpU6dz5tGpUydDkrFp0yb7to8++siQZAQFBRl79uyxb3/11VervM4ll1xiREZGGkeOHLFv++677wxfX1/j1ltvtW9LSkoyAgMDHZ7v+++/N/z8/Bzec15enuHn52c8+eSTDnnu2LHD8Pf3d9h+5nu87777jLCwMKOiouKc7xsAUL9s7d4nn3xiHD582Ni3b5+xYsUKo1WrVkZAQICxb98+e+yQIUOMHj16GKWlpfZtVqvViI+PN7p06WLfNnXqVEOS8dVXX9m3FRQUGOHh4YYkY/fu3fbtzrapjz32mCHJSE1NrRJrtVoNwzCMd955x/D19TUyMjIc9i9atMiQZHzxxReGYRjGtm3bDEnG5MmTHeJuuummGvOpzNa+d+/e3SgvL7dvv/HGGw0fHx9j+PDhDvFXXHGFQ7toe/0JEyY4xE2fPt2QZHz66aeGYZz+zJo2bWpce+219vdoGIYxc+ZMQ5LD5/PEE08YISEhxs6dOx2e85FHHjH8/PyMvXv32red+R7Dw8ONKVOmnPU9V+dvf/ubIckoLi4+Z+yRI0eMyMhIIyEhwWH7F198YYwZM8Z4/fXXjX//+99GSkqKERERYQQGBhpbtmyxx6WmphqSjIiICKNLly7Gm2++abz55ptGly5djKZNmxrfffddldecOHGiERQUVOv3BQCeiPb6/9Be146z7fXFF19sXHXVVU4/7zvvvGNIMl5//XWH7YcOHTKGDBniUEtp2bKlQ22jsrlz5xqSjEOHDjn92mjYmJoDqMbLL7+s//73vw63+va73/1OV1xxhf3x5ZdfLkm6+uqr1bFjxyrbbZe65ufna9u2bRo/frxatGhhj+vZs6d+//vfa82aNZIki8Wijz76SElJSQ7PFxcXp2uuucYhl9TUVFmtVt1www365Zdf7Lc2bdqoS5cu2rBhQ43vo3nz5jpx4oRbPkMAgHOGDh2qVq1aqUOHDkpOTlZISIhWrVplXwDu6NGj+vTTT3XDDTeouLjY3g4cOXJE11xzjXJzc7V//35J0po1a9S/f3+HeXxbtWqlm2++uc75rVy5Ur169bKP4qrMdunn+++/r7i4OHXr1s2hrbLNEWxrq2zt4L333uvwPFOnTq1VTrfeequaNGlif3z55ZfLMIwqcxxefvnl2rdvn33aCdvrP/DAAw5x06ZNkyT7qvKffPKJysvLqyy4V12e77//vhISEnTBBRc4vPehQ4fKYrHo888/r/F9NG/eXF999ZUOHDhQi3cvHTlyRP7+/mrWrNlZ46xWq26++WYdO3ZMCxYscNgXHx+vFStW6Pbbb9d1112nRx55RF9++aV8fHw0Y8YMe9zx48clnb4sef369Ro/frzGjx+vTz75RIZh6Omnn67yuhdccIFKSkrOOoUHADQ0tNe0165qr2sjJydHU6ZM0RVXXKFx48Y57AsODtZFF12kcePG6f3339cbb7yhqKgojR49Wj/++GOV57KN5j/zinR4L6bmAKpx2WWXuX2xwsrFYUkKDw+XdHoep+q2//rrr5KkPXv2SJIuuuiiKs8ZFxenjz76SCdOnFBxcbFKSkrUpUuXKnEXXXSRveGVpNzcXBmGUW2sJIeG/UyTJ0/W8uXLNXz4cLVr107Dhg3TDTfcoD/84Q81HgMAqF8vv/yyunbtqsLCQr3xxhv6/PPPFRAQYN//448/yjAMPfroo3r00UerfY6CggK1a9dOe/bssXeSVlZdu+SsXbt2acyYMWeNyc3NVXZ2tlq1alVjftLpdtLX11exsbHnlV9t2mmr1arCwkJFRETYX79z584OcW3atFHz5s3t7bjtv2e2va1atXK4BFs6/d63b99+zvdenaefflrjxo1Thw4d1LdvX40YMUK33nqrLrzwwhqPqY177rlH69at0z//+U/16tXrnPGdO3fWqFGjlJqaKovFIj8/P/tl4gMGDHD4fDt27KiBAwfaLzuuzDAMSXIoCgBAQ0d7TXvtqvbaWQcPHtS1116r8PBwrVixQn5+fg77r7/+evn7++s///mPfduoUaPUpUsX/fWvf9V7773nEE973fhQiAZMYrFYTH2+M7/Qz7Xd9gXuClarVT4+Plq7dm21r3+23tXIyEht27ZNH330kdauXau1a9fqzTff1K233qq3337bZTkDAJxXuQM2KSlJAwcO1E033aQffvhBzZo1sy+gM3369CpXzdiceaJ2PurSplqtVvXo0UPPPfdctfvPPOE8X+fbTpt5wmW1WvX73/9eDz30ULX7u3btWuOxN9xwgxISEvTBBx/o448/1jPPPKN58+YpNTVVw4cPr/G4iIgIVVRUqLi4WKGhodXGzJ49WwsXLtRTTz2lW265xen306FDB5WXl+vEiRMKCwtT27ZtJanaNSwiIyOrXbT5119/VXBw8HnNdQoAnob2uvZor8/dXjursLBQw4cP17Fjx5SRkWFvn21++uknrVu3TosXL3bY3qJFCw0cOLDatRtsA+rqe00uuA+FaKCWLrjgAh07dsxhW3l5ufLz892T0BlsK9X/8MMPVfbl5OSoZcuWCgkJUWBgoIKCgpSbm1sl7sxjY2NjZRiGYmJizto41qRp06YaOXKkRo4cKavVqsmTJ+vVV1/Vo48+auoPIQDA+fPz81NKSooGDx6sl156SY888oh9tE2TJk2qXXW9sk6dOjnVtkjOt6mxsbHKyso66+vGxsbqu+++05AhQ8560tipUydZrVbt2rXLYVRVdfm5gu31c3NzFRcXZ99+6NAhHTt2zN6O2/6bm5vrMNrp8OHD9pM2m9jYWB0/fvyc/zY1iYqK0uTJkzV58mQVFBSoT58+evLJJ896YtutWzdJ0u7du9WzZ88q+19++WU9/vjjmjp1qh5++OFa5fPTTz8pMDDQ3tHdo0cPNWnSxH45eWUHDhyodmTZ7t27HT5fAPA2tNeu1Vjaa2eVlpZq5MiR2rlzpz755BP97ne/qxJz6NAhSdV3UJw6dco+7Ullu3fvVsuWLWscJQ7vwxzRQC3FxsZWmbtp8eLFpo+IrquoqChdcsklevvttx1+LGRlZenjjz/WiBEjJJ3+4XLNNdcoLS1Ne/futcdlZ2fro48+cnjO0aNHy8/PT7Nnz67SQ2wYho4cOVJjPmfu8/X1tTeAZWVldXqPAADXGjRokC677DK98MILKi0tVWRkpAYNGqRXX3212o7Xw4cP2++PGDFCX375pb7++muH/e+++26V45xtU8eMGaPvvvvOYYV6G1u7dMMNN2j//v167bXXqsSUlJToxIkTkmQ/WfvHP/7hEHO2lerNZGuHz3w928iwa6+9VtLpeUCbNGmiBQsWOLS91eV5ww03KDMzs0r7LUnHjh2r9sRPOn2iWFhY6LAtMjJSbdu2PWcbbVvH4ttvv62y77333tO9996rm2++ucYRb5Lj/zc23333nVatWqVhw4bJ1/f0qUpoaKhGjBihTZs2KScnxx6bnZ2tTZs26fe//32V59myZYvi4+PP+h4AoKGjvXadxtBeO8tisehPf/qTMjMz9f777zusZVVZ586d5evrq/fee8/hs/j555+VkZGh3r17Vzlm8+bNNT4fvBMjooFamjBhgu68806NGTNGv//97/Xdd9/po48+8qhLSZ555hkNHz5cV1xxhf7yl7+opKRECxYsUHh4uB5//HF73OzZs7Vu3TolJCRo8uTJqqio0IIFC3TxxRdr+/bt9rjY2FjNmTNHM2bMUF5enpKSkhQaGqrdu3frgw8+0MSJEzV9+vRqc5kwYYKOHj2qq6++Wu3bt9eePXu0YMECXXLJJYxUAgAP9uCDD+r666/XW2+9pTvvvFMvv/yyBg4cqB49euiOO+7QhRdeqEOHDikzM1M///yzvvvuO0nSQw89pHfeeUd/+MMfdN999ykkJESLFy9Wp06dHNoWyfk29cEHH9SKFSt0/fXX6/bbb1ffvn119OhRrVq1SosWLVKvXr10yy23aPny5brzzju1YcMGDRgwQBaLRTk5OVq+fLk++ugj9evXT5dccoluvPFGLVy4UIWFhYqPj9f69eurXUDHFXr16qVx48Zp8eLFOnbsmK666ip9/fXXevvtt5WUlKTBgwdLOj235PTp05WSkqLExESNGDFCW7du1dq1a6v9fFatWqXExESNHz9effv21YkTJ7Rjxw6tWLFCeXl51f5OKS4uVvv27ZWcnKxevXqpWbNm+uSTT/TNN99o/vz5Z30fF154obp3765PPvnEYcGnr7/+WrfeeqsiIiI0ZMiQKgWN+Ph4+4ixP/3pTwoKClJ8fLwiIyP1/fffa/HixQoODtZTTz3lcNzcuXO1fv16XX311faFq/7xj3+oRYsWmjlzpkPs5s2bdfToUY0aNeqs7wEAvAHttWt4e3stSZ9//rm9g+Hw4cM6ceKE5syZI0m68sordeWVV0o6vUDjqlWrNHLkSB09elRLlixxeJ6xY8faP4vbb79d/+///T8NGTJEo0ePVnFxsRYuXKiSkhKHhYil03Nib9++XVOmTDnre4CXMQDYvfnmm4Yk45tvvqkxxmKxGA8//LDRsmVLIzg42LjmmmuMH3/80ejUqZMxbtw4e9yGDRsMScaGDRvs28aNG2d06tTpnHl06tTJuPbaa6tsl2RMmTLFYdvu3bsNScYzzzzjsP2TTz4xBgwYYAQFBRlhYWHGyJEjje+//77Kc3722WdG3759jaZNmxoXXnihsWjRImPWrFlGdV8PK1euNAYOHGiEhIQYISEhRrdu3YwpU6YYP/zwQ43vccWKFcawYcOMyMhIo2nTpkbHjh2NSZMmGfn5+ef8HAAArnW2ds9isRixsbFGbGysUVFRYRiGYezatcu49dZbjTZt2hhNmjQx2rVrZyQmJhorVqxwOHb79u3GVVddZQQGBhrt2rUznnjiCeP11183JBm7d+92eA1n2lTDMIwjR44Yd999t9GuXTujadOmRvv27Y1x48YZv/zyiz2mvLzcmDdvnnHxxRcbAQEBxgUXXGD07dvXmD17tlFYWGiPKykpMe69914jIiLCCAkJMUaOHGns27fPkGTMmjXrrJ+ZrX1///33nfosbW3q4cOH7dtOnTplzJ4924iJiTGaNGlidOjQwZgxY4ZRWlpa5d9g9uzZRlRUlBEUFGQMGjTIyMrKqvbzKS4uNmbMmGF07tzZaNq0qdGyZUsjPj7eePbZZ43y8nJ7XOX3WFZWZjz44INGr169jNDQUCMkJMTo1auXsXDhwrN+BjbPPfec0axZM+PkyZNVPoeabm+++aY99sUXXzQuu+wyo0WLFoa/v78RFRVljB071sjNza329TZv3mwMHTrUCAkJMUJDQ41Ro0YZO3furBL38MMPGx07djSsVqtT7wMAPB3tNe212e115fdc3a3y53vVVVedtW2v7NSpU8aCBQuMSy65xGjWrJnRrFkzY/Dgwcann35aJa9XXnnFCA4ONoqKipx6H/AOPobhwhXOAAAAAHilwsJCXXjhhXr66af1l7/8xd3pSDo97Vd0dLQeeeQR3Xfffe5OBwAAt/PE9lqSevfurUGDBun55593dyqoR8wRDQAAAKDWwsPD9dBDD+mZZ56R1Wp1dzqSpDfffFNNmjTRnXfe6e5UAADwCJ7YXq9bt065ublVpuuA92NENAAAAAAAAADApRgRDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApCtEAAAAAAAAAAJfyd3cCZ7JarTpw4IBCQ0Pl4+Pj7nQAAF7IMAwVFxerbdu28vWlT7auaLMBAK5Ee20O2msAgCvVpr32uEL0gQMH1KFDB3enAQBoBPbt26f27du7O40GizYbAFAfaK/PD+01AKA+ONNee1whOjQ0VNLp5MPCwtycDQDAGxUVFalDhw72Ngd1Q5sNAHAl2mtz0F4DAFypNu21xxWibZcKhYWF0UgCAFyKy1PPD202AKA+0F6fH9prAEB9cKa9ZqItAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBL+bs7ATQMFotFGRkZys/PV1RUlBISEuTn5+futAAAAIA64zcuAHfh+wdAY0QhGueUmpqqadOmKS8vz74tOjpa8+fP1+jRo92XGAAAcMBJLeA8fuMCcBe+fwA0VkzNgbNKTU1VcnKyevTooczMTBUXFyszM1M9evRQcnKyUlNT3Z0iAADQ6Ta7c+fOGjx4sG666SYNHjxYnTt3pq0GqsFvXADuwvcPgMbMxzAMw91JVFZUVKTw8HAVFhYqLCzM3ek0ahaLRZ07d1aPHj2UlpYmX9//67ewWq1KSkpSVlaWcnNzGW0FoEGhrTEHn6PnsJ3UJiYmaubMmerevbuysrI0d+5crV69WitWrGCEFfAbfuM2HLQz5uBz9Bx8/wDwRrVpZxgRjRplZGQoLy9PM2fOdGggJcnX11czZszQ7t27lZGR4aYMAQCAxWLRtGnTlJiYqLS0NPXv31/NmjVT//79lZaWpsTERE2fPl0Wi8XdqQIegd+4ANyF7x8AjR2FaNQoPz9fktS9e/dq99u22+IAAED946QWqB1+4wJwF75/ADR2FKJRo6ioKElSVlZWtftt221xAACg/nFSC9QOv3EBuAvfPwAaOwrRqFFCQoKio6M1d+5cWa1Wh31Wq1UpKSmKiYlRQkKCmzIEAACc1AK1w29cAO7C9w9QdxaLRenp6Vq2bJnS09OZdq6BohCNGvn5+Wn+/PlavXq1kpKSHFb0TUpK0urVq/Xss8+yiAIAAG7ESS1QO/zGBeAufP8AdZOamqrOnTtr8ODBuummmzR48GB17txZqamp7k4NtUQhGmc1evRorVixQjt27FB8fLzCwsIUHx+vrKwsrVixQqNHj3Z3igAANGqc1AK1x29cAO7C9w9QO6mpqUpOTlaPHj0cfuf26NFDycnJFKMbGB/DMAx3J1FZUVGRwsPDVVhYqLCwMHeng99YLBZlZGQoPz9fUVFRSkhI4IQWQINFW2MOPkfPkpqaqmnTpikvL8++LSYmRs8++ywntUAN+I3r2WhnzMHn6Jn4/gHOzWKxqHPnzurRo4fS0tIcFua2Wq1KSkpSVlaWcnNz+ftxo9q0MxSiAQCNDm2NOfgcPQ8ntQC8Ce2MOfgcATRU6enpGjx4sDIzM9W/f/8q+zMzMxUfH68NGzZo0KBB9Z8gJNWunfGvp5wAAADgYn5+fvwIBwAAgFfIz8+XJHXv3r3a/bbttjh4PuaIBgAAAAAAAOBRoqKiJElZWVnV7rdtt8XB81GIBgAAAAAAAOBREhISFB0drblz58pqtTrss1qtSklJUUxMjBISEtyUIWqLQjQAAAAAAAAAj+Ln56f58+dr9erVSkpKUmZmpoqLi5WZmamkpCStXr1azz77LGuiNCDMEQ0AAAAAAADA44wePVorVqzQtGnTFB8fb98eExOjFStWaPTo0W7MDrVFIRoAAABAo2SxWJSRkaH8/HxFRUUpISGBUVUAAHiY0aNHa9SoUbTZXoBCNAAAAIBGJzU1VdOmTVNeXp59W3R0tObPn8/oKgAAPIyfn58GDRrk7jRwnpgjGgAAAECjkpqaquTkZPXo0cNhvskePXooOTlZqamp7k4RAADA61CIBgAAANBoWCwWTZs2TYmJiUpLS1P//v3VrFkz9e/fX2lpaUpMTNT06dNlsVjcnSoAAIBXoRANAAAAoNHIyMhQXl6eZs6cKcMwlJ6ermXLlik9PV2GYWjGjBnavXu3MjIy3J0qAACAV2GOaAAAAACNRn5+viRp165duvHGG6vMET1nzhyHOAAAAJiDEdEAAAAAGo2oqChJ0i233FLtHNG33HKLQxwAAADMwYhoAAAAAI1GfHy8/P39FRERodTUVPn7nz4l6t+/v1JTU9W+fXsdOXJE8fHxbs4UAADAuzAiGgAAAECjsWnTJlVUVOjQoUMaPXq0w4jo0aNH69ChQ6qoqNCmTZvcnSoAAIBXoRANAC5gsVgcFj+yWCzuTgkAAOj/5n5esmSJduzYofj4eIWFhSk+Pl5ZWVlasmSJQxwAAADMwdQcAGCy1NRUTZs2rcriR/Pnz9fo0aPdlxgAALDP/RwbG6sff/xRGRkZys/PV1RUlBISEvT11187xAEAAMAcjIgGABOlpqYqOTm52sWPkpOTlZqa6u4UAQBo1BISEhQdHa25c+fKarU67LNarUpJSVFMTIwSEhLclCEAADgTVx17B0ZEA4BJLBaLpk2bpsTERKWlpcnX93RfX//+/ZWWlqakpCRNnz5do0aNkp+fn5uzBQCgcfLz89P8+fOVnJys8PBwlZSU2PcFBQWptLRUK1asoK0GAMBDcNWx92BENACYJCMjQ3l5eZo5c6a9CG3j6+urGTNmaPfu3crIyHBThgAAwMYwjCrbfHx8qt0OAADcg6uOvQuFaAAwiW1Ro+7du1e737adxY9QHz7//HONHDlSbdu2lY+Pj9LS0hz2G4ahxx57TFFRUQoKCtLQoUOVm5vrnmQBoB7ZrmAaOXKkjh49queff1533323nn/+eR05ckQjR47U9OnTueQXAAA3O/Oq4/79+6tZs2b2q44TExNpsxsYCtEAYBLbokZZWVnV7rdtZ/Ej1IcTJ06oV69eevnll6vd//TTT+sf//iHFi1apK+++kohISG65pprVFpaWs+ZAkD9sl3BFB8fr7i4ON1///166aWXdP/99ysuLk5XXHEFVzCh3tBxDAA146pj70MhGgBMUnnxo1OnTjkspHDq1CkWP0K9Gj58uObMmaM//vGPVfYZhqEXXnhBf/vb3zRq1Cj17NlT//znP3XgwIEqJ8AA4G1sVybNnDmz2st8//rXvzrEAa5ExzEA1Iyrjr0PixUCgElsix+NGTOm2sWPSkpKtHLlShY/gtvt3r1bBw8e1NChQ+3bwsPDdfnllyszM1N//vOf3ZgdALhWZGSkJGnAgAFauXKlvvjiC/3nP/9RVFSUVq5cqauvvlobN260xwGuNHz4cA0fPrzafWd2HEvSP//5T7Vu3VppaWm01wC8XuWrjvv3719lP1cdNzwUogHABc4cpcKoFXiSgwcPSpJat27tsL1169b2fdUpKytTWVmZ/XFRUZFrEgSAevDLL7+oc+fO2rt3r31bx44dFRwc7MasgP9DxzGAxq7yVcdpaWkO03NYrVauOm6AKEQDgEksFovuuusuSdKIESM0YsQI+0joNWvW6MMPP9Rdd92lUaNGMSoaDVJKSopmz57t7jQA4LwUFBRIknJycqrsq1yUtsUB7kLHMYDGznbVcXJyspKSkjRjxgx1795dWVlZSklJ0erVq7VixQrOrxsQCtEAYJL09HQVFBRo4MCBWrVqlUNv7Z133qkrr7xSX3zxhdLT0zVkyBA3ZorGrk2bNpKkQ4cOOVzGdujQIV1yySU1Hjdjxgw98MAD9sdFRUXq0KGDy/IEAFeoPOWGr6+vrFZrtY+ZmgMNFR3HALzJ6NGjtWLFCj3wwAOKj4+3b4+OjtaKFSs0evRoN2aH2mKxQgAwSXp6uiRp9uzZ1a7o+/jjjzvEAe4SExOjNm3aaP369fZtRUVF+uqrr3TFFVfUeFxAQIDCwsIcbgDQ0JSXl0s6Pcqqbdu2Dvvatm1rH1VliwPcpXLHcWWHDh2y76vOjBkzVFhYaL/t27fPpXkCgDsYhuHuFFAHtS5Ef/755xo5cqTatm0rHx8fpaWlOew3DEOPPfaYoqKiFBQUpKFDhyo3N9esfAEAgBOOHz+ubdu2adu2bZJOzzO5bds27d27Vz4+Ppo6darmzJmjVatWaceOHbr11lvVtm1bJSUluTVvAHC1pUuXSjo9pdaBAwcc9h04cEAWi8UhDnAXOo4BQEpNTVVycnKVKbMKCgqUnJys1NRUN2WGuqh1IfrEiRPq1auXXn755Wr3P/300/rHP/6hRYsW6auvvlJISIiuueYaFuoC4PUGDRokSZo1a5bDZb7S6YUUbJdI2uIAV/r222/Vu3dv9e7dW5L0wAMPqHfv3nrsscckSQ899JDuueceTZw4UZdeeqmOHz+udevWKTAw0J1pA4DLFRcX2+8HBAQ47Kv8HVg5DnAVOo4BoGa2dZgMw6gyAtq27a677rJ3IsPz1XqO6OHDh2v48OHV7jMMQy+88IL+9re/adSoUZKkf/7zn2rdurXS0tJY1ReAVxs0aJBatWqljRs3atSoUZo5c6Z9IYW5c+dq48aNioyMpBCNejFo0KCzXq7m4+Ojv//97/r73/9ej1kBgPvZpjQIDg7W0aNHlZmZqfz8fEVFRemKK65QixYtdPLkybNOfQCY5dtvv9XgwYPtj21rMYwbN05vvfWWHnroIZ04cUITJ07UsWPHNHDgQDqOATQatnWYJOnqq69Wly5dVFJSoqCgIOXm5mrNmjUqKChgHaYGxNTFCnfv3q2DBw9q6NCh9m3h4eG6/PLLlZmZWW0hmhV9AXgLPz8/LVq0SGPGjNH69eu1evVq+77g4GBJ0iuvvMKKvgAAuNEFF1wgSTp58qTGjBmjv/71r0pMTFRWVpbGjBmjkydPOsQBrkTHMQDU7NNPP5UkRUVF6eOPP9aaNWvs+/z9/RUVFaX8/Hx9+umnFKIbCFMXKzx48KAkqXXr1g7bW7dubd93ppSUFIWHh9tvHTp0MDMlAKhXo0eP1sqVKxUZGemwPTIyUitXrmRFXwAA3Mzf///G4qxdu1bx8fEKCwtTfHy81q1bV20cAACof3v37pUk5efnKyIiQq+99pry8/P12muvKSIiQvn5+Q5x8HymFqLrghV9AXib0aNHa9euXdqwYYOWLl2qDRs26Mcff6QIDQCAB7BNkdWuXTv5+jqeDvn4+Khdu3YOcQAAwD3atm0rSWrSpIn27t2rCRMmqE2bNpowYYL27t2rJk2aOMTB85nazW+bR+3QoUOKioqybz906JAuueSSao8JCAioskgIAAAAALjCoEGDFBkZqf3792v48OEKCgrSsWPH1Lx5c5WUlGjt2rWs6QAAgAewTd976tQpjRkzpso6TKdOnXKIg+cztRAdExOjNm3aaP369fbCc1FRkb766ivdddddZr4UAHis1NRUPfDAA9qzZ499W6dOnfTcc88xKhoAADfz8/PTK6+8ojFjxmjt2rUO+3x8fCSxpgMAAJ6g8pVLZ67DFBQUVG0cPFut/6WOHz+ubdu2adu2bZJOL1C4bds27d27Vz4+Ppo6darmzJmjVatWaceOHbr11lvVtm1bJSUlmZw6AHie1NRUjRkzxr6yr01BQYHGjBmj1NRUN2UGAADOZCs8AwAAz9OlSxf7/ZKSEod9lR9XjoNn8zHOtkRvNdLT0zV48OAq28eNG6e33npLhmFo1qxZWrx4sY4dO6aBAwdq4cKF6tq1q1PPX1RUpPDwcBUWFiosLKw2qQGAW1ksFkVFRenw4cNKTEzUX//6V/tlQ08++aRWr16tyMhIHThwgFFWbkZbYw4+RwANUeX2esSIEerSpYtKSkoUFBSk3NxcrVmzhvbaQ9DOmIPPEUBDVV5erqCgIFmtVgUEBKisrMy+LzAwUKWlpfL19VVJSYmaNm3qxkwbt9q0M7UuRLsajSSAhmr9+vUaOnSoBg4cqM8++8zh8iCr1aqrrrpKGzdu1CeffKIhQ4a4MVPQ1piDzxFAQ2Rrr+Pi4nTy5MkqU2kFBQUpJyeH9toD0M6Yg88RQENlsVjUokULFRUVKTIyUmPHjtWFF16on376SUuWLFFBQYHCwsJ09OhROo/dqDbtDJOoAIBJ0tPTJUmzZ8+uMkeVr6+vZs2a5RAHAADqn60dzs7OVs+ePZWZmani4mJlZmaqZ8+eysnJcYgDAADukZGRoaKiIt188806evSonnvuOd1999167rnndPToUd10000qKipSRkaGu1OFkyhEA4ALWCwWpaena9myZUpPT5fFYnF3SgAAQKevUpKkK664Qmlpaerfv7+aNWum/v37Ky0tTZdffrlDHAAAcI/8/HxJ0qJFi1RUVKQpU6Zo2LBhmjJlioqKirRo0SKHOHg+f3cnAADeYtCgQZozZ46mTJmikpKSai/1tcUBAAD3aNGihSTpxIkT1e63LX5kiwMAAO4RFRUlSXrppZe0aNEi+zn2xx9/rNWrV2vSpEkOcfB8FKIBwCSDBg1SeHi4cnJyqkzNsW/fPlmtVoWHh1OIBgDAjdq0aSNJ2r59u6677joNHz5cQUFBKikp0dq1a7V9+3aHOAAA4B4JCQmKjIzUjBkzquzbs2ePZs6cqcjISCUkJLghO9QFhWgAMNG51n/1sPVhAQBodNq1a2e/v2bNGn344Yf2xz4+PtXGAQAA9yguLpYkNW3aVA888ID+8pe/6PXXX9dzzz2n8vJy+340DMwRDQAmSU9PV1FRkdq1a+dwIiudPrFt166dioqKWPwIAAA3SkhIUKtWrSRJgYGBDvts02gxugoAAPdbv369SkpK1KxZM7Vt21ZPPfWUunTpoqeeekrt2rVTSEiISkpKtH79enenCicxIhoATGIrMO/fv1+JiYlVLvVdvXq1PW7IkCFuzBQAgMbN1mF89dVXV2mvK4+QBgAA7vPOO+9IklJSUnTXXXcpIyND+fn5ioqKUkJCgl5++WXdd999eueddzRs2DA3ZwtnUIgGAJNYrVZJ0hVXXKF///vfDvNE33nnnYqPj9dXX31ljwMAAPUvIyNDBQUFSklJ0aJFixwKz9HR0Zo7d65mzpypjIwM1nUAAMCNjh8/LkmKiYmRn59flXY5OjraIQ6ej6k5AMAkLVq0kCSdOHGi2v0lJSUOcQBgNovFovT0dC1btkzp6emyWCzuTgnwOPn5+ZKkDh06VJlKS5I6duzoEAcAANxj4MCBkqSZM2dWGdBltVr16KOPOsTB8zEiGgBM0qZNG0nS9u3bdd1111W51Hf79u0OcQBgptTUVE2bNk15eXn2bdHR0Zo/f75Gjx7tvsQADxMVFSVJGjt2rBITE/Xggw86tNdjx451iAMAAO5xzz336KGHHtL27ds1cuRIjRgxwt5mr1mzRtu3b5evr6/uueced6cKJ1GIBgCTtGvXzn5/zZo1Dpf6Vh5xVTkOAMyQmpqq5ORkJSYmatmyZerevbuysrI0d+5cJScna8WKFRSjgd/Ex8fL399fISEh2rFjh30NB0nq1KmTwsPDdeLECcXHx7sxSwAA0LRpU02bNk3PPPOM1qxZozVr1lSJmTZtmpo2beqG7FAXTM0BACZJSEhQq1atJEmBgYEO+2yPIyMjlZCQUO+5AfBeFotF06ZNU2JiotLS0tS/f381a9ZM/fv3V1pamhITEzV9+nSm6QB+s2nTJlVUVKiwsFClpaVavHixDhw4oMWLF6u0tFSFhYWqqKjQpk2b3J0qAACNXv/+/c9rPzwLI6IBwES2kc9XX311lak5Ko+QBgCzZGRkKC8vT8uWLXNYJFWSfH19NWPGDMXHx7PwGvCb/fv3S5J69+6tX3/9VRMnTrTvi4mJUe/evbV161Z7HAAAcA+LxaK77rpLkjRixAh16dJFJSUlCgoKUm5urtasWaO77rpLo0aNkp+fn5uzhTMoRAOASTIyMlRQUKCUlBS9+uqrDoXnmJgYzZ07VzNnzqQYBMBUtgXVunfvXu1+23YWXgNOO3z4sKTTI6jOvMTXarXq8ssv19atW+1xAADAPdLT01VQUKCBAwfqP//5j8OgC6vVqiuvvFJffPGF0tPTNWTIEDdmCmcxNQcAmMRW5Ln77rv1448/asOGDVq6dKk2bNig3Nxc3X333Q5xAGAG24JqWVlZ1e63bWfhNeA02zRar7zyinr06KHMzEwVFxcrMzNTPXr00KJFixziAACAe6Snp0uSZs+eXe2Vf48//rhDHDwfhWgAMAnFIADukJCQoOjoaM2dO1dWq9Vhn9VqVUpKimJiYpifHvhNmzZtHB4bhmG/nS0OAAC4j8ViUXp6upYtW6b09HTWP2mgmJoDAExiKwbdc889Onz4sPbs2WPf16lTJ7Vq1YpiEADT+fn5af78+UpOTlZSUpJmzJih7t27KysrSykpKVq9erVWrFjBvHnAGeLi4rRt2zbFx8fbt3Xo0EHdunVTTk6OGzMDAACSNGjQIM2ZM0d33323Tpw4ob1799r3dezYUcHBwfY4NAyMiAYAk/j5+en666/Xt99+q9LSUi1evFgHDhzQ4sWLVVpaqm+//VbJyckUgwCYbvTo0VqxYoV27Nih+Ph4hYWFKT4+XllZWVqxYoVGjx7t7hQBj1FQUCBJys7O1s8//+ywb9++ffYitC0OAAC4x6BBgxQWFqbs7GyHIrQk7d27Vzk5OQoLC6MQ3YAwIhoATGKxWPT++++rX79++uWXXzRx4kT7vpiYGPXr108rVqxQSkoKxWgAphs9erRGjRqljIwM5efnKyoqSgkJCXzfAGdwdoosptICAMD9Kioqzms/PAuFaAAwSUZGhvLy8rRs2TL16dNHCxcu1K5duxQbG6vJkydr8+bNio+PV0ZGBj22AFzCz8+P7xfgHPr162e/7+Pj4zA3dOXHleMAAED9W79+vU6ePKlmzZqpRYsWDqOiO3XqpCNHjuj48eNav369hg0b5sZM4Sym5gAAk+Tn50uSdu3apYsuukj333+/XnrpJd1///266KKL9NNPPznEAQCA+vfII4/Y75+5QGHlx5XjAABA/XvnnXckSSkpKfrpp5+0YcMGLV26VBs2bNCuXbv05JNPOsTB8zEiGgBMYruE95ZbbtG1116rBx98UEFBQSopKdHatWt1yy23OMQBAID698MPP9jvBwYGqrS0tNrHleMAAED9O378uKTTU11WJzo62iEOno9CNACYJD4+Xv7+/goJCdH27du1evVq+76OHTsqLCxMJ06cUHx8vBuzBACgcTt58qQkKSgoSOXl5Q77Tp06Ze9EtsUBAAD3GDhwoNLS0nT33XfLMAzt2bPHvq9Tp04OcWgYmJoDAEyyadMmVVRUqLCwUD///LPDvp9//lmFhYWqqKjQpk2b3JQhAACIiIiQJJWUlMhisTjss1gsKikpcYgDAADucc8998jHx0d5eXk6efKkpk2bppdfflnTpk3TyZMntWfPHvn4+Oiee+5xd6pwEiOiAcAk+/fvt98PCAiwn8hKpy/1tY2sqhwHAADqV7NmzUyNAwAAruHn56fQ0FAVFRXp8OHDmj9/fpWY0NBQ+fn5uSE71AUjogHAJAcPHpQk9ezZU4WFhQ4LKRw7dkw9evRwiAMAAPUvKCjI1DgAAOAaGRkZKioqkiT5+Pg47LM9LioqUkZGRr3nhrphRDQAmOTo0aOSpJCQEPn5+WnQoEH2fVarVSEhIQ5xAACg/mVmZpoaBwAAXMN2NfHw4cOVmpqqRYsWadeuXYqNjdWdd96p0aNHa+3atVx13IBQiAYAk/j6nr7I5Msvv1RSUpJmzJih7t27KysrSykpKfrqq68c4gAAQP1z9mSVk1oAANzr8OHDkqTRo0crMDBQU6dOddiflJSktWvX2uPg+aiGAIBJbCOgL7roIu3YsUPx8fEKCwtTfHy8srKydNFFFznEAQCA+tekSRNT4wAAgGu0atVKkpSamiqr1eqwz2q1Ki0tzSEOno8R0QBgkkGDBqlVq1bKyclRYGCgw778/HyVlpYqMjKSQjQAAG4UFhamI0eOOBUHAADcp127dpKkdevWaeTIkQoMDNSxY8fUvHlzlZaWat26dQ5x8HwUogHAJH5+fho/fryeeeYZlZeXO+w7deqUJGncuHGs6AsAgBudOHHC1DgAAOAaCQkJio6O1q+//qo1a9ZU2R8eHq4WLVooISHBDdmhLpiaAwBMYrFY9P7776tfv35q3769w7727durX79+WrFihSwWi5syBAAAZWVlpsYBAADX8PPzU6tWrVRYWFjt/sLCQrVs2ZLBXg0II6IBwCQZGRnKy8vTsmXLdOmllyojI0P5+fmKiopSQkKCvv76a8XHxysjI4PpOQAAcBN/f+dOgZyNAwAArlFSUqJvvvlGkhQQEODQSWx7/M0336ikpERBQUHuShO1wIhoADBJfn6+JKl79+6yWCzatm2bNm3apG3btslisah79+4OcQAAoP41bdrU1DgAAOAaDzzwgCQpNjZWx44d0/PPP6+7775bzz//vI4dO6bY2FiHOHg+uvkBwCRRUVGSpDvvvFPvvfeeKioq7PsefPBB3XDDDQ5xAGA2i8VS5WoMLlUEHJ08edLUOAAA4BrffvutJCkpKUlxcXHKy8uz73vxxRf1xz/+Uc8//7w9Dp6PEdEAYJKEhASFhYXp3XffVUREhF577TXl5+frtddeU0REhJYuXaqwsDAWUgDgEqmpqercubMGDx6sm266SYMHD1bnzp2Vmprq7tQAj8Ic0QAANAzNmzeXJM2fP1+HDh1y2Hfo0CE9//zzDnHwfBSiAcAkFotFx48flyT17dtXZWVlWrNmjcrKytS3b19J0vHjx1msEIDpUlNTlZycrB49eigzM1PFxcXKzMxUjx49lJycTDEaqIQ5ogEAaBimTp1qv3/llVfqvvvu08SJE3XffffpyiuvrDYOno1fVwBgkoULF8pqteqaa67RRx99pDVr1tj3+fv7a9iwYfr444+1cOFCGkoAprFYLJo2bZoSExOVlpYmX9/T4wz69++vtLQ0JSUlafr06Ro1ahTTdACSoqOjlZWV5VQcAABwnyZNmtjvf/TRR/roo4/OGQfPxohoADDJrl27JEkff/xxlQWOmjRpov/+978OcQBghoyMDOXl5WnmzJn2IrSNr6+vZsyYod27dysjI8NNGQKexdkrk7iCCQAA93L29yu/cxsOCtEAYJKYmBhJkmEYGjJkiMPl8UOGDJFhGA5xAGCG/Px8SVL37t2r3W/bbosDGrvCwkJT4wAAgGtUVFTY7585ZVblx5Xj4NkoRAOASS6++GJJpxvE5cuXq7S0VP/5z39UWlqq5cuX2xtKWxwAmCEqKkqSlJWVpfLycr3wwgu655579MILL6i8vNw+BYEtDmjsGBENAEDD8Ouvv0o6fY5tG9hlYxiG/RzbFgfPxxzRAGCSjRs3SjrdGxsSEuLQUPr4+Ngfb9y4Uddcc41bcgTgfRISEhQdHa2xY8dqz549DiNCHnzwQXXq1EkxMTFKSEhwY5aA52jTpo0OHTrkVBwAuIrFYlFGRoby8/MVFRWlhIQE1nIAznDw4EFJp8+x/f39dfXVV6tt27Y6cOCAPv/8c/vvXlscPB8jogHABarrrQUAV/Dz81OvXr20a9cu+fr66pFHHlFubq4eeeQR+fr6ateuXerZsycnt8BvTp48aWocANRWamqqOnfurMGDB+umm27S4MGD1blzZ6Wmpro7NcCjBAcH2+9XVFTo008/1ZIlS/Tpp586DL6oHAfPRiEaAExSebRhkyZN1Lt3bw0cOFC9e/d2WMWXUYkAzFReXq4PP/xQ4eHhatOmjZ566il16dJFTz31lKKiohQeHq4PP/xQ5eXl7k4V8AhnzjF5vnEAUBupqalKTk5Wjx49HNaU6dGjh5KTkylGA5WEhYXZ7/v4+Djsq/y4chw8G4VoADBJ5R5ZPz8/bd26VRs3btTWrVsdRiKykAIAMy1cuFAVFRV69tln9dNPP2nDhg1aunSpNmzYoF27dunpp59WRUWFFi5c6O5UAQBo1CwWi6ZNm6bExESlpaWpf//+atasmfr376+0tDQlJiZq+vTpzFEP/ObM6S4rq/yYK5AbDgrRAGCSF154wX6/rKzMYV/lkYiV4wDgfO3atUuSlJiYKIvFom3btmnTpk3atm2bLBaLEhMTHeKAxs7ZqwO4igCA2TIyMpSXl6eZM2fKMAylp6dr2bJlSk9Pl2EYmjFjhnbv3q2MjAx3pwp4hIKCAvt9q9XqsK/y48px8GxcbwYAJjl69KgkqVWrVjp69KjDSAZfX19FRETo8OHD9jgAMENsbKwkafz48Vq/fn2VxQoHDx7sEAc0dqWlpabGAYCz8vPzJZ3uHL7xxhuVl5dn3xcdHa05c+Y4xAGNnbMLB7PAcMPBiGgAMEm7du0kSYcPH9Y111yjMWPG6Oqrr9aYMWM0bNgwHT582CEOAMwwefJk+fj46KOPPqoy9U9FRYX++9//ysfHR5MnT3ZThoBnOXXqlKlxAOCsqKgoSdLYsWOrnSN67NixDnFAY+fsYtssyt1wMCIaAEySlJSkVatWSZLWrFlz1jgAMJNtXrymTZvq/vvv11/+8he9/vrrev7551VeXs68eUAlAQEBpsYBgLPi4+Pl7++viIgIpaam2hdF7d+/v1JTU9W+fXsdOXJE8fHxbs4U8Ax9+/Y1NQ7ux4hoADBJYWGhqXEA4IwFCxZIOj0tkNVq1bx589S1a1fNmzdPVqtVLVu2dIgDGjtfX+dOgZyNAwBnbdq0SRUVFSooKNDo0aMdRkSPHj1aBQUFqqio0KZNm9ydKuARMjMzTY2D+/HrCgBMcsEFF5gaBwDO2LhxoyTpzTff1IkTJ/T888/r7rvv1vPPP68TJ07o9ddfd4gDGruQkBBT4wDAWba5n9955x3t2LFD8fHxCgsLU3x8vLKysvTOO+84xAGNnbO/X/md23AwNQcAmOTLL7+032/ZsqWsVqtKS0sVGBgoX19f/fLLL/a4cePGuStNAF6mWbNmkqTdu3eradOmmjp1qsN+20JItjigsYuMjNT333/vVBwAmMk293NsbKx++OEHLVy4ULt27VJsbKwmT56szZs3O8QBjd3PP/9sahzcj0I0AJjkwIEDkuRQdJakkydP2rdbrVZ7HACY4ZZbbtGSJUs0a9Ys3XHHHcrMzFR+fr6ioqJ0xRVXaPbs2fY4ANKRI0dMjQMAZyUkJCg6Olr33HOPfvnlF3tnsSS9+OKLatmypWJiYpSQkOC+JAEP4uPjY2oc3I9CNJxisViUkZFhP7FNSEhgVVLgDLaCs9VqrXa/bbstDgDMMGTIEIWFheno0aMKDg52+A6ydYCFhYVpyJAhbswS8By7du0yNQ4AnOXn56frr79ezzzzjFq3bq3FixcrMTFRq1ev1qOPPqpvv/1WDz74IOfawG+aNm1qahzcjzmicU6pqanq3LmzBg8erJtuukmDBw9W586dlZqa6u7UAI/SvXt3U+MAwBl+fn6aNGmSpKodYbbHkyZN4qQW+E1paampcQDgLIvFovfff1/9+vVT06ZNNXHiRLVt21YTJ05UQECA+vXrpxUrVshisbg7VcAjnDp1ytQ4uB+FaJxVamqqkpOT1b17d7388st644039PLLL6t79+5KTk6mGA1UUnmOaDPiAMAZFotFb7/9tqSqo0ECAgIkSW+//TYntcBvnO2UofMGgNkyMjKUl5eniy66qMqChAcOHFDXrl21e/duZWRkuClDwLOUlZWZGgf3oxCNGlksFk2bNk19+/bVjh07NGXKFN1+++2aMmWKduzYob59+2r69Omc2AK/2b17t6lxAOCM9PR0FRQUaODAgTp+/Lg2bNigpUuXasOGDSouLtaAAQNUUFCg9PR0d6cKeISaptCqaxwAOMtWfH733XcVERGh1157Tfn5+XrttdcUERGhpUuXOsQBjZ2vr3NlS2fj4H78S6FGtt7ab7/9Vj179lRmZqaKi4uVmZmpnj176ttvv6W3FqikvLzc1DgAcIatwDx79mw1adJEgwYN0o033qhBgwapSZMmevzxxx3igMbOMAxT4wDAWREREZKkFi1a6Oeff9aECRPUpk0bTZgwQT///LNatGjhEAc0dra/CbPi4H4UolGj/fv3S5KGDx+utLQ09e/fX82aNVP//v2Vlpam4cOHO8QBjV3r1q3t98/ska38uHIcAACoX/7+zq3X7mwcADhrx44dkqT27dvLMAylp6dr2bJlSk9Pl2EYateunUMc0Ng52ylD503DQSEaNTp8+LAkafTo0dU2kklJSQ5xQGMXFRVlv1/TgmFnxgHuYrFY9OijjyomJkZBQUGKjY3VE088wQjABmjQoEGSpFmzZunUqVMO7fWpU6c0e/ZshzigsQsODjY1DgCclZeXJ0navn27wsPDNXjwYN10000aPHiwwsPD7QVoWxzQ2AUFBZkaB/ejmx81atWqlSRp4cKFmjNnjvbs2WPf16lTJ/ulD7Y4oLFj/io0JPPmzdMrr7yit99+WxdffLG+/fZb3XbbbQoPD9e9997r7vRQC4MGDVKrVq20ceNGhYeHq6SkxL4vKChIJSUlioyMpBAN/KaoqMjUOMCVLBaLHn/8cS1ZskQHDx5U27ZtNX78eP3tb3+Tj4+Pu9NDLcXGxta4r/K/59nigMaktLTU1Di4H9UQ1Mh2WdDWrVtVWlqqadOm6eWXX9a0adNUWlqqrVu3OsQBjV1ISIipcYArbdq0SaNGjdK1116r6OhoJScna9iwYfr666/dnRpqyc/PT+PHj5dUdcVw25z048aNk5+fX32nBngkFitEQ2LrOH7ppZeUnZ2tefPm6emnn9aCBQvcnRrqYNKkSZJOT/3TsmVLh30RERH2KYFscUBjd+Zv2/ONg/tRiEaN4uPj5e/vr+DgYP3yyy+aP3++pkyZovnz5+vIkSMKDg6Wv7+/4uPj3Z0q4BGcnXKDqTngCeLj47V+/Xrt3LlTkvTdd99p48aN9vn/0XBYLBa9//771Y6eMgxDsbGxWrFihSwWixuyAwCcDzqOvctXX30lSaqoqNChQ4f08MMPa+fOnXr44Yd16NAhVVRUOMQBjZ2zU8EyZWzDwdQcqNGmTZtUUVGhiooKRUZG6pZbbtGFF16on376Se+8844KCgrscVzuCzB/FRqWRx55REVFRerWrZv8/PxksVj05JNP6uabb67xmLKyMofRBly27hkyMjLsc0kmJiZq+PDh9ik51q5dq9WrV9vjaK8BqWnTpvarBc4VB7hbfHy8Fi9erJ07d6pr1672juPnnnuuxmNorz3X/v37JUkxMTHat2+f5s2bp3nz5kk6PUo6JiZGu3fvtscBjd3x48dNjYP7UYhGjWyNX+/evfXrr79q/vz59n0xMTHq3bu3tm7dSiMJ/OY///mP03HPP/+8i7MBzm758uV69913tXTpUl188cXatm2bpk6dqrZt22rcuHHVHpOSkmJf+A6ew9YODx8+XP/+978d5qG/8847lZiYqLVr19JeA78JDAx0qhAdGBhYD9kAZ1eXjmPaa89lG7U5c+ZM3XrrrVq4cKF27dql2NhYTZ48WW+99ZYmTZrE6E7gN0zN4X1Mn5rDYrHo0UcfVUxMjIKCghQbG6snnnhChmGY/VJwMVvj179//ypz5FksFl122WUOcUBj52yRh2IQPMGDDz6oRx55RH/+85/Vo0cP3XLLLbr//vuVkpJS4zEzZsxQYWGh/bZv3756zBg1sbXDo0ePrrIYqq+vr5KSkhzigMau8oKeZsQBrlS543jLli16++239eyzz+rtt9+u8Rjaa8/VqlUrSVJqaqp8fHx0ySWXKD4+Xpdccol8fHyUlpbmEAc0ds7WEqk5Nhymj4i2Labw9ttv6+KLL9a3336r2267TeHh4br33nvNfjm4kK3xe+WVV6pMJXD48GG9+uqrDnEAgIbj5MmTVYqWfn5+Z12cKyAgQAEBAa5ODbVU+aT29ttvd/h3tVqtnNQCZzh16pSpcYArVe44lqQePXpoz549SklJqfEKJtprz9WuXTtJ0tq1axUWFqbS0lL7vsDAQPtjWxzQ2Pn6+jq1ePCZ5zXwXKYXoisvpiBJ0dHRWrZsGYspNEBt2rSx3w8JCdHll18uwzDk4+OjrKws+yiRynFAY3bBBRcoPz/fqTjA3UaOHKknn3xSHTt21MUXX6ytW7fqueee0+233+7u1FBLtpPVdevWKSkpSTNmzFD37t2VlZWllJQUrVu3ziEOANBw1KXjGJ4rISFBkZGRKigocChCS7I/joyMVEJCgjvSAzxOkyZNnJp2o0mTJvWQDcxgeiG6tospsJCC57JYLJJO/9D55ZdflJ6e7rDfNkeZLQ5o7Jo3b+5UIbp58+auTwY4hwULFujRRx/V5MmTVVBQoLZt22rSpEl67LHH3J0aaikhIUHR0dFq2bKlduzYofj4ePu+mJgY9e3bV0eOHOGkFgAaIDqOvY+t4HzmSE/b4zML1EBj5syaDrWJg/uZXoiu7WIKLKTguTIyMiSpxkKzbXtGRoaGDRtWb3kBAM5faGioXnjhBb3wwgvuTgXnyc/PT/Pnz1dycnKVS7Hz8/OVl5enFStWyM/Pz00ZAgDqio5j75Kenm4ffBcQEOAwF73tcVFRkdLT0zVkyBB3pQl4DB8fH6fmf/bx8amHbGAG0ydRqe1iCiyk4LnoeQJqp7Cw0NQ4AKgN2/RZlfn6+rJ4CwA0YLaO4z179qikpES7du3SnDlz1LRpU3enhjr49NNPJUlXXHGFCgsLtWHDBi1dulQbNmxQYWGh+vfv7xAHNHaBgYGmxsH9TC9EV15MoUePHrrlllt0//33KyUlpdr4gIAAhYWFOdzgGbKzs+33z1yssPLjynFAY3by5ElT4wDAGRaLRdOmTdPIkSOrnNQeO3ZMI0eO1PTp05lKCwAAN9u7d68k6aabbqoy97evr69uvPFGhzigsXNmfujaxMH9TJ+ag8UUvMf+/fvt92taSOHMOKAxc7bIQzEIgJkyMjKUl5enZcuWyTAMbdu2Tbt27VJsbKzi4+M1Y8YMxcfHKyMjQ4MGDXJ3ugAANFodO3aUdHrKlWeffVZ79uyx7+vUqZN9pLstDmjsOMf2PqaPiLYtpvDhhx8qLy9PH3zwgZ577jn98Y9/NPulAMCjVJ7jzYw4AHCGbZHUf/3rXwoJCdH999+vl156Sffff79CQkL03nvvOcQBAAD3uPrqqyVJO3fuVElJiRYvXqwDBw5o8eLFKikpUW5urkMcAHgb0wvRCxYsUHJysiZPnqy4uDhNnz5dkyZN0hNPPGH2S8HFunTpYr9/5vySlR9XjgMaM2cXSGAhBQBmioqKkiS9+OKLioiI0Guvvab8/Hy99tprioiI0IsvvugQBwAA3CMhIcF+BfmxY8c0ceJEtW3bVhMnTrSvI+Pr66uEhAR3pgkALmP61By2xRReeOEFs58a9czf37n/PZyNA7xdZGSkU1PVREZG1kM2ABqLyy+/XJLUtGlT7d27135Z74QJE3TrrbcqNDRU5eXl9jgAAOAemzZtsk9bWl5e7rDPNset1WrVpk2bmE4L0Ompfp2ZdsPPz68esoEZqCCiRm3btjU1DvB2zs6Fz5z5AMz06quvSjp9QjtmzBj94Q9/UFBQkEpKSrRu3Tr7ie6rr76qqVOnujFTAAAaN2enyWI6LeA0zrG9D4Vo1Cg7O9vUOMDbHTp0yNQ4AHDGrl27JEl33XWXXnvtNa1evdq+z9/fX3fddZdeeeUVexwAAHCPiIgI+/3AwECVlpZW+7hyHNCYnTlN7PnGwf0oRKNGP/zwg6lxgLejtxaAO8TGxkqSXnnlFSUmJmr48OH2EdFr167VK6+84hAHAADc47vvvrPfP9s6TN99952GDRtWb3kBQH0xfbFCeI+AgAD7/TMXV7MtsHBmHNCYsVghAHeYNGmSpNNzRK9cuVKTJ0/WbbfdpsmTJ2vlypX2OaNtcQAAwD2++OIL+/3mzZtr8eLFOnDggBYvXqzmzZtXGwcA3oQR0ajR5ZdfrqysLElShw4dtHfvXvu+Dh06aM+ePfY4AKcLzM5cEkQhGoCZvvrqK0mn54ju0KGDrrrqKoWEhOjEiRP67LPP7HNEf/XVVyx8BEhq0qSJTp065VQcAJjp+PHjkk4vXh4UFKSJEyfa98XExCgyMlIFBQX2OADwNhSiUaNWrVrZ7+/fv1+XXHKJ/cR2x44d1cYBAID6ZVvQqE+fPtqyZYvef/99h/227Sx8BJxW+co+M+IAwFmRkZGSpJKSEv3444+aMWOGcnNz1aVLF6WkpKhdu3YOcUBj5+vr69TUlrTZDQeFaNTI3////vewWCzatm3bOeOAxqxZs2YqKipyKg4AzBIVFSVJ2rJli1q1aqW2bduqrKxMAQEBOnDggLZs2eIQBzR2oaGhKisrcyoOAMwUExMjSSouLlZYWJh9+8cff6yXX365ShzQ2IWEhKi4uNipODQMdBmgRs5evstlvsBpLVq0MDUOAJxhmyLL19dXR48e1XfffaecnBx99913Onr0qH2ECFNpAac5e8k7l8YDMNvVV19tahzg7ZzpOK5NHNyPQjRqFB8fb78/bNgwJSQk6He/+50SEhIcVvCtHAc0ZocPHzY1DgCc8eqrr0qSrFarLBaLwz6LxWK/nNEWBzR2nNQCcJfK585nrhtT+THn2MBpzqzBVJs4uB+FaNSo8gnrxx9/rIyMDH3//ffKyMjQxx9/XG0c0JidOHHC1DgAcMYPP/xgahwAAHCNhQsX2u+fWTir/LhyHNCYBQcHmxoH96MQjRrt2rXL1DjA27H4EQB3OHDggP3+2UZXVY4DGjNn1zdhHRQAZtu4caOpcYC3Y7CX96Eaghp16tTJ1DjA251ZADrfOABwRuWVxIcPH67MzEwVFxcrMzNTw4cPrzYOaMy4zBeAuwQGBpoaB3i7iooKU+PgfnTzo0ZnzjN5vnGAt/P393fq74ERVgDMVHmk8zfffKNHHnlEhmHIx8dH33//fbVxQGPGb1wA7lJUVGRqHAA0NFRDUKMvvvjC1DjA2zE1BwB3sI2a8vHx0eHDh/XZZ5857Pfx8ZFhGIyuAgDAzfLz802NA7ydr6+vU1f1cY7dcPAvhRrl5OSYGgd4O6bmAOAOMTExkv5vGoHw8HBFRkYqPDzcYbstDmjsmCMagLsUFBSYGgd4O2enlmMKuoaDQjRqxKTwQO2UlJSYGgcAzvjTn/7k8LiwsFAFBQUqLCw8axzQWDFHNAB3adq0qalxANDQUIhGjfz8/EyNA7wdJ7YA3GHNmjWmxgHejoWPALhL5U5iHx8fDR06VE8++aSGDh3qcNXkmZ3JAOAtuN4MNeKyRaB2mL8KgDvk5uaaGgcAAFzj5MmT9vuGYeiTTz7RJ598ctY4APAmVENQo+LiYlPjAG/Xpk0bU+MAwBm2BY18fHyqdHRVfszCRwAAuBdrygBo7ChEA4BJnB25wAgHAGYqLy+XdHpk1ZlXZVR+bIsDGrsmTZqYGgcAzurataupcQDQ0FCIRo3atWtnahzg7SwWi6lxAOCMwMBAU+MAb8dvXADuMmfOHFPjAKChoRCNGrVo0cLUOMDbUYgG4A59+/Y1NQ7wdq1atTI1DgCc1bRpU1PjAG/n5+dnahzcj0I0ahQSEmJqHODtnF2EkMUKAZhp3759psYB3u777783NQ4AnLV+/XpT4wBvFxAQYGoc3I9qCGrEQgpA7ZSVlZkaBwDO+Pnnn02NA7zdiRMnTI0DAGd99NFH9vsdOnRw2NexY8dq44DGrLS01NQ4uB+FaNQoMjLS1DjA29F5A8AdgoODTY0DAACukZeXJ6n6q4oNw7C31bY4oLE7cyHu842D+/m7OwF4ri+//NLUOMDbRUREKD8/36k4ADBL8+bNTY0DAACuYZui78SJE1Wuuqg8hRZT+QHwVny7oUbHjx83NQ7wdseOHTM1DgCcERYWZmoc4O38/Z0bi+NsHAA4a8CAAabGAUBDQyEaNWJFX6B2SkpKTI0DAGe0atXK1DjA2/n5+ZkaBwDOmjBhgqlxANDQUIhGjXr27GlqHAAAMN/u3btNjQO8XXl5ualxAOCs5cuXmxoHAA0NhWjU6KeffjI1DgAAmI9pgYDaMQzD1DgAcNa2bdtMjQOAhoZCNGpUUFBgahwAADAfU2kBANAwHD582NQ4AGhoKESjRqdOnTI1DvB2Pj4+psYBgDOsVqupcQAAwDWKi4tNjQOAhoZCNGrUpEkTU+MAb8elvgDc4eDBg6bGAd7O19e5UyBn4wDAWYGBgabGAUBDw68r1OiXX34xNQ7wdoyIBuAOhYWFpsYBAADXsFgspsYBQENDIRo1YkVxoHb8/PxMjQMAZ1RUVJgaB3g7prMB4C5MzQGgsaMQDQAmad26talxAAAAALwHV1ACaOwoRKNGwcHBpsYB3i4gIMDUOABwBpf5AgDQMAQFBZkaB3g7Om+8D4Vo1IiFFIDaKSkpMTUOAJxRWlpqahwAAHCNyMhIU+MAb9eyZUtT4+B+FKJRIxY/AmqnqKjI1DgAAAAA3uPAgQOmxgHe7siRI6bGwf0oRKNGLH4E1E5ZWZmpcQAAAAC8B+fYQO2wwLD3oRANACahkQTgDsydBwBAw+Dr61wJxtk4AGho+HZDjWgkgdrx9/c3NQ4AnEEhGgCAhqFdu3amxgFAQ0MFETVq3ry5qXGAt6PzBoA7+Pn5mRoHAABc4+TJk6bGAUBDQzUENaKoBtQOoxIBuAPzTQIA0DCUlJSYGgcADQ0VRNSouLjY1DjA29F5AwAAAKAmFKIBNHZUQ1AjRlgBtcPl8QDcoUmTJqbGAQAAAIArUIhGjSwWi6lxgLfjbwYAAAAAAKB6FKIBAAAasMDAQFPjAACAazBwBUBjRyEaAEzSqlUrU+MAwBlMpQUAAACgIaAQDQAmKSgoMDUOAJzBiGgAAAAADQGFaNTIx8fH1DjA25WWlpoaBwDOCA4ONjUOAAAAAFyBQjRq5OfnZ2ocAAAw3+HDh02NAwAAADxBkyZNTI2D+1GIRo0oRAO106JFC1PjAMAZZWVlpsYBAAAAgCtQiEaN6HkCaicoKMjUOAAAAAAAGqtTp06ZGgf3oxCNGlGIBmrn+PHjpsYBAAAAAAB4CwrRqNGJEydMjQO8ndVqNTUOAAAAAADAW1CIRo3Ky8tNjQO8XUVFhalxAAAAAAAA3oJCNGrk6+vc/x7OxgHerqSkxNQ4AAAAAAAAb0EFETUyDMPUOMDbMTUHAAAAAABA9ShEo0YUooHa8fHxMTUOAAAAAADAW1CIBgCThIWFmRoHAAAAAADgLShEA4BJAgICTI0DXG3//v0aO3asIiIiFBQUpB49eujbb791d1oAAAAAAC9EIRoATFJQUGBqHOBKv/76qwYMGKAmTZpo7dq1+v777zV//nxdcMEF7k4NAABUQscxgMaK6S+9j7+7EwAAAPVv3rx56tChg9588037tpiYGDdmBAAAzmTrOB48eLDWrl2rVq1aKTc3l45jAI0Ca5d5HwrRAAA0QqtWrdI111yj66+/Xp999pnatWunyZMn64477nB3agAA4Dd0HAMAvAlTc6BGvr7O/e/hbBwAwHP89NNPeuWVV9SlSxd99NFHuuuuu3Tvvffq7bffrvGYsrIyFRUVOdwAAIDrrFq1Sv369dP111+vyMhI9e7dW6+99tpZj6G9BgB4KpdUEJnDyjswFw8AeC+r1ao+ffpo7ty56t27tyZOnKg77rhDixYtqvGYlJQUhYeH228dOnSox4wBAGh86tJxTHsNAPBUpheiWfzIe1itVlPjAACeIyoqSr/73e8ctsXFxWnv3r01HjNjxgwVFhbab/v27XN1mgAANGp16TimvQYAeCrT54hmDivvwaTwAOC9BgwYoB9++MFh286dO9WpU6cajwkICFBAQICrUwMAAL+pqeN45cqVNR5Dew0A8FSmj4iu7RxWzF8FAED9u//++/Xll19q7ty5+vHHH7V06VItXrxYU6ZMcXdqAADgN3XpOAYAwFOZXoiu7RxWzF/lufz9nRsw72wcAMBzXHrppfrggw+0bNkyde/eXU888YReeOEF3Xzzze5ODQAA/IaOYwCAN/ExTJ5XoWnTpurXr582bdpk33bvvffqm2++UWZmZpX4srIylZWV2R8XFRWpQ4cOKiwsVFhYmJmpoZaaNm2qU6dOnTOuSZMmKi8vr4eMAM9Wm4U7mdLGvYqKihQeHk5bc574HD0D3z1A7fA303DQzpy2evVqzZgxQ7m5uYqJidEDDzygO+64w+nj+Rw9B98/QO3wN9Mw1KadMX0oa23nsGL+Ks/FYoUAAAAA4F6JiYlKTEx0dxoAAJw306fmYA4r72GxWEyNAwAAAAAAANA4mV6IZg4rAAAAAAAAAEBlpheiWfwIAAAAAAAAAFCZ6XNES8xhBQAAAAAAAAD4P6aPiAYAAAAAAAAAoDIK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApCtEAAAAAAAAAAJeiEA0AAAAAAAAAcCkK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApf3cnAAAAgNo5efKkcnJyan3cli1b7Pe7deum4OBgM9MCAAAAgBpRiAYAAGhgcnJy1Ldv31ofV/mYzZs3q0+fPmamBQAAAAA1ohANAADQwHTr1k2bN2+WJN14443auXPnOY/p2rWrli1b5vAcAAAAAFBfKEQDAAA0MMHBwfbRzF9//bWaN29+zmO+/vprhYeHuzgzAAAAAKgeixUCAAA0YOHh4YqNjT1rTGxsLEVoAAAAAG5FIRoAAKCB+/HHH2ssRsfGxurHH3+s54wAAAAAwBGFaAAAAC/w448/6tixY+rVq5ckqVevXjp27BhFaAAAAAAegUI0AACAlwgPD9cbb7whSXrjjTeYjgMAAACAx6AQDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApCtEAAAAAAAAAAJeiEA0AAAAAAAAAcCkK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRAAAAAAAAAACX8nd3AgAAAAAAAN7m5MmTysnJqdOxW7ZskSR169ZNwcHBZqYFAG5DIRoAAAAAAMBkOTk56tu3b52OtR23efNm9enTx8y0AMBtKEQDAAAAAACYrFu3btq8ebP9cWJiovLz8895XFRUlFavXm1/DgDwFhSiAQAAAAAATBYcHOwwmjkrK0sRERHnPC4rK0stWrRwZWoA4BYsVggAAAAAAOBiLVq0UOvWrc8a07p1a4rQALwWhWgAAAAAAIB6cPDgwRqL0a1bt9bBgwfrOSMAqD8UogEAAAAAAOrJwYMHdeTIEcXGxkqSYmNjdeTIEYrQALwehWgAAAAAAIB61KJFCy1fvlyStHz5cqbjANAoUIgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS/m7OwF4jpMnTyonJ6dOx27ZssV+v1u3bgoODjYrLQAAAAAAAAANHIVo2OXk5Khv3751OrbycZs3b1afPn3MSgsAAAAAAABAA0chGnbdunXT5s2b7Y+vu+467d+//5zHtWvXTqtWrXJ4HgAAAAAAAACwoRANu+DgYIeRzNu3b1dERMQ5j9u+fbtatGjhytQAAAAAAAAANGAsVogatWjRQq1btz5rTOvWrSlCAwAAAAAAADgrCtE4q4MHD9ZYjG7durUOHjxYzxkBAAAAAAAAaGiYmgPndPDgQR09elSXXXaZdu3apdjYWH399deMhAYAAECDcPLkSeXk5NT6uC1bttjvd+vWTcHBwWamBQAA0KhQiIZTWrRooeXLl6tv375avnw5RWgAAAA0GDk5Oerbt2+tj6t8zObNmx3WUwEAAEDtUIgGAAAA4NW6deumzZs3S5K+/fZbTZo06ZzHvPrqq+rXr5/DcwAAAKDuKEQDAAAA8GrBwcH20cx9+vRxqhA9ceJEV6cFAADQqLBYIQAAAIBGxTCM89oPAACA2qMQDQAAAKDRMQxDGRkZDtsyMjIoQgMAALgIhWgAAAAAjdLAgQPtc0dv3rxZAwcOdHNGAAAA3otCNAAAAAAAAADApVisEAAAAAAAAIBbnTx5Ujk5OXU6dsuWLZKkbt26KTg42My0YCIK0QAAAAAAAADcKicnR3379q3TsbbjNm/erD59+piZFkxEIRoAAAAAAACAW3Xr1s2+doMkDRs2TEeOHDnncREREfr444/tzwHPxRzRAABATz31lHx8fDR16lR3pwIAAACgEQoODlafPn3st507dzp13M6dO+3HMC2HZ6MQDQBAI/fNN9/o1VdfVc+ePd2dCgAAOAs6jgE0Ji1atFDr1q3PGtO6dWu1aNGinjLC+aIQDQBAI3b8+HHdfPPNeu2113TBBRe4Ox0AAFADOo4BNEYHDx6ssRjdunVrHTx4sJ4zwvlweSGaHlsAADzXlClTdO2112ro0KHnjC0rK1NRUZHDDQAAuB4dxwAas4MHD+rIkSOKjY2VJMXGxurIkSMUoRsglxai6bEFAMBz/etf/9KWLVuUkpLiVHxKSorCw8Pttw4dOrg4QwAAINFxDAAtWrTQ8uXLJUnLly9nOo4GymWFaHpsAQDwXPv27dN9992nd999V4GBgU4dM2PGDBUWFtpv+/btc3GWAACAjmMAgLdwWSHa2R5bemsBAKh/mzdvVkFBgfr06SN/f3/5+/vrs88+0z/+8Q/5+/vLYrFUOSYgIEBhYWEONwAA4Dp0HAMAvIm/K57U1mP7zTffnDM2JSVFs2fPdkUaAACgBkOGDNGOHTsctt12223q1q2bHn74Yfn5+bkpMwAAYFO549jGYrHo888/10svvaSysrIqbXZAQIACAgLqO1UAAM7J9EK0rcf2v//9r1M9tjNmzNADDzxgf1xUVMSlQwAAuFhoaKi6d+/usC0kJEQRERFVtgMAAPeg4xgA4E1ML0TXtseW3loAAAAAAKqi4xgA4E1ML0TTYwsAQMOUnp7u7hQAAAAAAF7K9EI0PbYAAAAAALgGHccAgIbK190JAAAAAAAAAAC8m+kjoqtDjy0AAAAAAAAANF6MiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBL+bs7AQBoqE6ePKmcnJw6Hbtlyxb7/W7duik4ONistAAAAAAAADwOhWgAqKOcnBz17du3TsdWPm7z5s3q06ePWWkBAAAAAAB4HArRAFBH3bp10+bNm+2PL730Ulmt1nMe5+vrq2+++cbheQAAAAAAALwZhWgAqKPg4GCHkcy7d+9Wp06dznnc7t271bFjR1emBgAAAAAA4FFYrBAATNKxY0f5+5+9f8/f358iNAAAAAAAaHQoRAOAiU6dOlVjMdrf31+nTp2q54wAAAAAAADcj0I0AJjs1KlT2rNnj4KDgyWdnsJjz549FKEBAAAAAECjRSEaAFygY8eOysjIkCRlZGQwHQcAAAAAAGjUKEQDAAAAAAAAAFyKQjQAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApShEAwAAAAAAAABcikI0AAAAAAAAAMClKEQDAAAAAAAAAFyKQjQAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApfzdnQAAAACqys3NVXFxca2Py87OdvhvXYSGhqpLly51Ph4AAAAAzkQhGgAAwMPk5uaqa9eu5/UcY8eOPa/jd+7cSTEaAAAAgGkoRAMAAHgY20joJUuWKC4urlbHlpSUKC8vT9HR0QoKCqr1a2dnZ2vs2LF1Go0NAAAAADWhEA0AAOCh4uLi1KdPn1ofN2DAABdkAwAAAAB1x2KFAAAAAAAAAACXohANAAAAAAAAAHApCtEAAAAAAAAAAJeiEA0AAAAAAAAAcCkK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRAAAAAAAAAACXohANAAAAAAAAAHApf3cnAAAAAAAA0BDl5uaquLi4TsdmZ2c7/Le2QkND1aVLlzodCwDuQCEaAAAAAACglnJzc9W1a9fzfp6xY8fW+didO3dSjAbQYFCIBgAAAAAAqCXbSOglS5YoLi6u1seXlJQoLy9P0dHRCgoKqtWx2dnZGjt2bJ1HYwOAO1CIBgAAAAAAqKO4uDj16dOnTscOGDDA5GwAwHOxWCEAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApShEAwAAAAAAAABcikI0AAAAAAAAAMCl/N2dAOpfbm6uiouLa31cdna2w39rKzQ0VF26dKnTsQAAAAAAAAAaLgrRjUxubq66du16Xs8xduzYOh+7c+dOitEAAAAAAABAI0MhupGxjYResmSJ4uLianVsSUmJ8vLyFB0draCgoFodm52drbFjx9ZpJDYAAAAAAACAho1CdCMVFxenPn361Pq4AQMGuCAbAAAAAAAAAN6MxQoBAAAAAAAAAC5FIRoAAAAAAAAA4FIUogEAAAAAAAAALsUc0QAAAAAavNzc3DotjJ2dne3w37oIDQ1Vly5d6nw8AABAY0AhGgAAAECDlpubq65du57Xc4wdO/a8jt+5cyfFaAAAgLOgEA0AAACgQbONhF6yZIni4uJqdWxJSYny8vIUHR2toKCgWr92dna2xo4dW6fR2AAAAI0JhWgAAAAAXiEuLk59+vSp9XEDBgxwQTYAAACojEI0AACAh/GpKFXvNr4KOrZTOlC/a0sHHdup3m185VNRWq+vCwAAAMC7UYgGAADwMIHH92rLpGbS55Okz+v3teMkbZnUTNnH90qKr98XBwAAAOC1KEQDAAB4mNJmHdXn1eN69913FdetW72+dnZOjm6++Wa9PqJjvb4uAAAAvF9ubm6d11XIzs52+G9thYaGsrCwm1GIBgAA8DCGf6C2HrSqpHlXqe0l9fraJQet2nrQKsM/sF5fFwAAAN4tNzdXXbt2Pe/nGTt2bJ2P3blzJ8VoN6IQDQBAI5SSkqLU1FTl5OQoKChI8fHxmjdvni666CJ3pwYAACqhzQbgLWwjoZcsWaK4uLhaH19SUqK8vDxFR0crKCioVsdmZ2dr7NixdR6NDXNQiAYAoBH67LPPNGXKFF166aWqqKjQzJkzNWzYMH3//fcKCQlxd3oAAOA3tNkAvE1cXJz69OlTp2MHDBhgcjaoT6YXoumtBQDA861bt87h8VtvvaXIyEht3rxZV155pZuyAgAAZ6LNBgB4C1+zn9DWW/vll1/qv//9r06dOqVhw4bpxIkTZr8UAAAwSWFhoSSpRYsWNcaUlZWpqKjI4QYAAOqXM202AACeyPQR0fTWAgDQsFitVk2dOlUDBgxQ9+7da4xLSUnR7Nmz6zEzAABQmTNtdllZmcrKyuyP6TgGAHgK00dEn+lcvbWMrgIAwL2mTJmirKws/etf/zpr3IwZM1RYWGi/7du3r54yBAAAknNtdkpKisLDw+23Dh061GOGAADUzKWFaGd6a2kkAQBwn7vvvlurV6/Whg0b1L59+7PGBgQEKCwszOEGAADqh7NtNh3HAABP5dJCtDO9tTSSAADUP8MwdPfdd+uDDz7Qp59+qpiYGHenBAAAqlHbNpuOYwCApzJ9jmgbW2/t559/ftbe2oCAAAUEBLgqDQAAUI0pU6Zo6dKl+ve//63Q0FAdPHhQkhQeHq6goCA3ZwcAAGxoswEA3sL0QrRhGLrnnnv0wQcfKD09nRFWABq83NxcFRcX1/q47Oxsh//WVmhoqLp06VKnY4FzeeWVVyRJgwYNctj+5ptvavz48fWfEAAAqBZttufyqShV7za+Cjq2Uzrg8iW4HAQd26nebXzlU1Far68LAOfD9EI0vbUAvElubq66du16Xs8xduzYOh+7c+dOitFwCcMw3J0CAABwAm225wo8vldbJjWTPp8kfV6/rx0nacukZso+vldSfP2+OADUkemFaHprAXgT20joJUuWKC4urlbHlpSUKC8vT9HR0bXuiMvOztbYsWPrNBIbAAAAgOuVNuuoPq8e17vvvqu4bt3q9bWzc3J088036/URHev1dQHgfLhkag4A8DZxcXHq06dPrY8bMGCAC7IBAAAA4G6Gf6C2HrSqpHlXqe0l9fraJQet2nrQKsM/sF5fFwDOR/1OYgQAAAAAAAAAaHQoRAMAAAAAAAAAXMr0qTng2dy1qi8r+gIAAAAAAACNF4XoRsZdq/qyoi8AAAAAAADQeFGIbmTctaovK/oCAAAAAAAAjReF6EbGXav6sqIvAAAAAAAA0HixWCEAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApShEAwAAAAAAAABcikI0AAAAAAAAAMClKEQDAAAAAAAAAFzK390JAAAAwNHJkyclSVu2bKn1sSUlJcrLy1N0dLSCgoJqfXx2dnatjwEAAACAc6EQDQAA4GFycnIkSXfccYfbcggNDXXbawMAAADwPhSiAQAAPExSUpIkqVu3bgoODq7VsdnZ2Ro7dqyWLFmiuLi4Or1+aGiounTpUqdjAQAAAKA6FKIBAAA8TMuWLTVhwoTzeo64uDj16dPHpIwAz+ZTUarebXwVdGyndKB+l8EJOrZTvdv4yqeitF5fFwAAoKGhEA0AAACgQQs8vldbJjWTPp8kfV6/rx0nacukZso+vldSfP2+OAAAQANCIRoAAABAg1barKP6vHpc7777ruK6davX187OydHNN9+s10d0rNfXBQAAaGgoRAMAAABo0Az/QG09aFVJ865S20vq9bVLDlq19aBVhn9gvb4uAAANDVNpgUI0AAAAAAAAAJdiKi1QiAYAAAAAAADgUkylBQrRAAAAAAAAAFyKqbRAIRoAzsJdc1gxfxUAAAAAAPAmFKIB4CzcNYcV81cBAAAAnu3kyZOSpC1bttTp+JKSEuXl5Sk6OlpBQUG1OjY7O7tOrwkA7kQhGgDOwl1zWDF/FQAAAODZcnJyJEl33HGH23IIDQ1122sDQG1RiAaAs3DXHFbMXwUAAAB4tqSkJElSt27dFBwcXOvjs7OzNXbsWC1ZskRxcXG1Pj40NFRdunSp9XEA4C4UohuZ87l0iMuGAAAAAAA4rWXLlpowYcJ5P09cXJz69OljQkYA4NkoRDcy7r50iMuGAAAAAAAAgMaHQnQjcz6XDnHZEAAAAAAAAIC6oBDdyJhx6RCXDQEAAAAAAACoDV93JwAAAAAAAAAA8G4UogEAAAAAAAAALkUhGgAAAAAAAADgUhSiAQAAAAAAAAAuxWKFAAAAABq0kydPSpK2bNlS62NLSkqUl5en6OhoBQUF1fr47OzsWh8DAADQGFGIBgAAANCg5eTkSJLuuOMOt+UQGhrqttcGAABoCChEAwAAAGjQkpKSJEndunVTcHBwrY7Nzs7W2LFjtWTJEsXFxdXp9UNDQ9WlS5c6HQsAANBYUIgGAAAA0KC1bNlSEyZMOK/niIuLU58+fUzKCAAAnOl8ptKSzm86LabS8gwUogEAAAAAAAC4FFNpgUI0AJyFuxY/orcWAAAAAOBNzmcqLen8p9NiKi33oxANAGfh7h5bemsBAAAAAN7AjKm0JKbTasgoRAPAWbhz8SN6awEAAAAAgLegEA0AZ8HiRwAAAAAAAOfP190JAAAAAAAAAAC8G4VoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEtRiAYAAAAAAAAAuBSFaAAAAAAAAACAS1GIBgAAAAAAAAC4FIVoAAAAAAAAAIBLUYgGAAAAAAAAALgUhWgAAAAAAAAAgEu5rBD98ssvKzo6WoGBgbr88sv19ddfu+qlAABAHdFeAwDg+WivAQDewCWF6Pfee08PPPCAZs2apS1btqhXr1665pprVFBQ4IqXAwAAdUB7DQCA56O9BgB4C39XPOlzzz2nO+64Q7fddpskadGiRfrwww/1xhtv6JFHHnHFS8IEJ0+eVE5OTo37s7OzHf5bk27duik4ONjU3AAA5qO9brjO1mbTXgOAd6G9BgB4C9ML0eXl5dq8ebNmzJhh3+br66uhQ4cqMzOzSnxZWZnKysrsj4uKisxOCU7KyclR3759zxk3duzYs+7fvHmz+vTpY1ZagMei8wYNWW3ba4k225M402bTXgP/h84bNFS01w2bGecLfPegMeFvxvuZXoj+5ZdfZLFY1Lp1a4ftrVu3rvZ/ppSUFM2ePdvsNFAH3bp10+bNm2vcX1JSory8PEVHRysoKOiszwM0BnTeoCGrbXst0WZ7krO12bTXQFV03qChor1u2Mw4X+C7B40JfzPezyVTc9TGjBkz9MADD9gfFxUVqUOHDm7MqPEKDg4+5x/rgAED6ikbwPPReYPGhjbbc5yrzaa9BhzReYPGhPbac5hxvsB3DxoT/ma8n+mF6JYtW8rPz0+HDh1y2H7o0CG1adOmSnxAQIACAgLMTgMAXI7OGzRktW2vJdpsAA0XnTdoqGivGzbOF4Da4W/G+/ma/YRNmzZV3759tX79evs2q9Wq9evX64orrjD75QAAQB3QXgMA4PlorwEA3sQlU3M88MADGjdunPr166fLLrtML7zwgk6cOGFf5RcAALgf7TUAAJ6P9hoA4C1cUoj+05/+pMOHD+uxxx7TwYMHdckll2jdunVVFlgAAADuQ3sNAIDno70GAHgLH8MwDHcnUVlRUZHCw8NVWFiosLAwd6cDAPBCtDXm4HMEALgS7Yw5+BwBAK5Um3bG9DmiAQAAAAAAAACojEI0AAAAAAAAAMClKEQDAAAAAAAAAFyKQjQAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApShEAwAAAAAAAABcikI0AAAAAAAAAMClKEQDAAAAAAAAAFyKQjQAAAAAAAAAwKUoRAMAAAAAAAAAXIpCNAAAAAAAAADApShEAwAAAAAAAABcyt/dCZzJMAxJUlFRkZszAQB4K1sbY2tzUDe02QAAV6K9NgftNQDAlWrTXntcIbq4uFiS1KFDBzdnAgDwdsXFxQoPD3d3Gg0WbTYAoD7QXp8f2msAQH1wpr32MTyse9lqterAgQMKDQ2Vj4+Pu9NBJUVFRerQoYP27dunsLAwd6cDeDz+ZjyXYRgqLi5W27Zt5evLLFV1RZvtmfjuAWqHvxnPRXttDtprz8X3D1A7/M14ptq01x43ItrX11ft27d3dxo4i7CwMP7ggVrgb8YzMbLq/NFmeza+e4Da4W/GM9Fenz/aa8/H9w9QO/zNeB5n22u6lQEAAAAAAAAALkUhGgAAAAAAAADgUhSi4bSAgADNmjVLAQEB7k4FaBD4mwHgDnz3ALXD3wwAd+H7B6gd/mYaPo9brBAAAAAAAAAA4F0YEQ0AAAAAAAAAcCkK0QAAAAAAAAAAl6IQDQAAAAAAAABwKQrRMNXjjz+u1q1by8fHR2lpaU4dEx0drRdeeMH+uDbHAg1dTk6O+vfvr8DAQF1yySVOHfPWW2+pefPm9sePP/6408cCAAAAAAC4A4VoLzB+/Hj5+PjozjvvrLJvypQp8vHx0fjx4+2xSUlJNT5XdHS0fHx85OPjo5CQEPXp00fvv/++U3lkZ2dr9uzZevXVV5Wfn6/hw4fX5e0ALlfd38GKFSsUGBio+fPn27elpKTIz89PzzzzTJXneOutt+Tj46M//OEPDtuPHTsmHx8fpaenO5XLrFmzFBISoh9++EHr16+v9XsB4Pk8pZ1+7bXXlJCQoAsuuEAXXHCBhg4dqq+//rraXCvfzvyek6QPP/xQl19+uYKCgnTBBRecNWegtjylnc7Ly9Nf/vIXxcTEKCgoSLGxsZo1a5bKy8sdYs78m/Hx8dGXX35Z5XWnTJmiqKgoBQQEqGvXrlqzZo0TnwaA+uIp7fX//vc/jRkzxv4clQdt2aSkpOjSSy9VaGioIiMjlZSUpB9++MEh5uDBg7rlllvUpk0bew4rV650KgfAGZ7SXkvSk08+qfj4eAUHBzsM2rL57rvvdOONN6pDhw4KCgpSXFycXnzxxSpx7777rnr16qXg4GBFRUXp9ttv15EjR5zKAc6hEO0lOnTooH/9618qKSmxbystLdXSpUvVsWPHWj3X3//+d+Xn52vr1q269NJL9ac//UmbNm0653G7du2SJI0aNUpt2rRRQEBA7d4E4Cb/7//9P91888165ZVXNG3aNPv2N954Qw899JDeeOONao/z9/fXJ598og0bNtT5tXft2qWBAweqU6dOioiIqPPzAPBsntBOp6en68Ybb9SGDRuUmZmpDh06aNiwYdq/f79D3B/+8Afl5+fbb8uW/f/27jyoyuqNA/j3sojUdSGWLiCBVwbJBSQERnGSpQRkIItQbHABUzQEjDQ0lEUT2bRxwjCTRcYUlEAJzXLpMqY2CUWxJIYOkgiNE4kZNSDw+8PhnW5cFC5c7/3h9zPzzug5x/M+rzPnfXgP5z3vYbn6zz77DEuWLEFISAh+/PFHXLhwAW+88cagroFoMNSVp69cuYLu7m58/PHHqKmpwQcffIC9e/fivffe69P2zJkzcuPG0dFRqOvo6MDLL7+MhoYGFBYWoq6uDp988gnMzc2ViouIVEcT8nV7ezukUimSk5MhkUgUtikrK0N4eDi+/fZbnD59Gp2dnZg3bx7++usvoc3SpUtRV1eHkpISVFVV4bXXXsPChQvxww8/DOo6iAZKnc/VHR0dCAwMxJo1axTWV1RUwMTEBAcPHkRNTQ1iY2OxadMmZGRkCG0uXLiApUuXYsWKFaipqcHRo0fx3XffYeXKlUrHRX1xInqEeOGFF2BhYYGioiKhrKioCM899xwcHBwG1deYMWMgkUhgY2ODPXv2QF9fH59//vlD/01CQgL8/PwAAFpaWhCJRAAANzc3rFu3Tq7tggULhN8kE6lbamoqIiIikJ+fj5CQEKG8rKwMf//9N7Zu3Yq7d+8q/KHx6aefRmhoKDZu3KjUuUUiESoqKrB161aIRCIkJCRAJpNBJBLhzp07QrvKykqIRCI0NDQodR4iUj9152ngwQqPt956CzNmzICtrS3279+P7u7uPm9j6OnpQSKRCIeBgYFQd//+fURFRSEtLQ2rV6+GjY0NpkyZgoULFw7qGogGSp152tvbGzk5OZg3bx6kUin8/f2xfv16uXHcy9DQUG7c6OrqCnXZ2dlobW3FsWPH4OrqCisrK8ydOxf29vZKxUVEqqMJ+drJyQlpaWkICgrqd3HXqVOnsHz5ckydOhX29vbIzc1FY2MjKioqhDYXL15EREQEnJ2dIZVKsXnzZowfP16uDdFwUWe+BoDExES8/fbbmD59usL60NBQ7N69G3PnzoVUKkVwcDBCQkLkxvqlS5dgZWWFyMhITJw4EXPmzEFYWFifNwhpaDgRPYKEhoYiJydH+Ht2drbcDUAZOjo60NXVlXsFUZH169cL5+5dCUKk6WJiYrBt2zaUlpbi1VdflavLysrC4sWLoauri8WLFyMrK0thHwkJCaiqqkJhYeGgz9/c3IypU6finXfeQXNzM9avX6/UdRDR/wd15mlF2tvb0dnZiWeeeUauXCaTwcTEBJMnT8aaNWvkXkf8/vvv0dTUBC0tLTg4OMDU1BQ+Pj6orq4e0nUQKaLuPK1IW1tbnzEDAP7+/jAxMcGcOXNQUlIiV1dSUoJZs2YhPDwczz77LKZNm4akpCR0dXUNS0xENLw0LV8PRFtbGwDI3Z9mz56NgoICtLa2oru7G/n5+fjnn3/g5uamkhjoyaWJ+Xog/pvTZ82ahV9//RUnT55ET08PfvvtNxQWFmL+/PmPLaYnASeiR5Dg4GB88803uHHjBm7cuIELFy4gODhY6f46OjqwY8cOtLW1wcPD46FtxWKxsA9P70oQIk32xRdfIDU1FcePH4enp6dc3d27d1FYWCiMn+DgYBw5cgT37t3r04+ZmRmioqIQGxuL+/fvDyoGiUQCHR0diMViSCQSiMVi5S+IiDSeOvO0IjExMTAzM8NLL70klHl7eyMvLw9nz55FSkoKysrK4OPjI0yYXb9+HcCDh4XNmzejtLQUBgYGcHNzQ2trq9LXQvRfmpCn/6u+vh4ffvghwsLChDKxWIydO3fi6NGjOHHiBObMmYMFCxbITUZfv34dhYWF6OrqwsmTJ7Flyxbs3LkT77///pDiISLV0LR8/Sjd3d1Yt24dXF1dMW3aNKH8yJEj6OzshKGhIfT09BAWFobi4mJYW1sPewz05NLEfD0QFy9eREFBAVatWiWUubq64tNPP8WiRYswatQoSCQSjBs3Dnv27FF5PE8STkSPIMbGxvD19UVubi5ycnLg6+sLIyOjQfcTExMDsViMp556CikpKUhOToavr68KIiZSHzs7O1hZWSE+Pr5PIjx8+DAmTZokvDI7Y8YMWFpaoqCgQGFfMTExuH37dr97XhERAZqVp5OTk5Gfn4/i4mKMHj1aKA8KCoK/vz+mT5+OBQsWoLS0FJcvXxY+FNPd3Q0AiI2NRUBAABwdHZGTkwORSDTgjzARDYSm5emmpiZ4e3sjMDBQbq9IIyMjREdHw8XFBU5OTkhOTkZwcLDcB5m6u7thYmKCffv2wdHREYsWLUJsbCz27t2rdDxEpDqalK8HIjw8HNXV1cjPz5cr37JlC+7cuYMzZ86gvLwc0dHRWLhwIaqqqoY9BnpyaVq+Hojq6mq88soriI+Px7x584Ty2tpaREVFIS4uDhUVFTh16hQaGhoUfsCUlMeJ6BEmNDQUubm5OHDgAEJDQ5XqY8OGDaisrMTNmzfxxx9/ICYmRul4tLS00NPTI1fW2dmpdH9Ew8Xc3BwymUx4sPzzzz+FuqysLNTU1EBHR0c4amtr+02I48ePx6ZNm5CYmIj29vYhxaWl9eC2/O9xwzFDNHJoQp5OT09HcnIyvvrqK9jZ2T20rVQqhZGREerr6wEApqamAIApU6YIbfT09CCVStHY2DjIKyHqnybl6Vu3bsHd3R2zZ8/Gvn37HtnexcVFGDPAg3FjY2MDbW1toez5559HS0uLyl7TJ6Kh0YR8PRBr165FaWkpvv76a0yYMEEov3btGjIyMpCdnQ1PT0/Y29sjPj4eM2fO5OpOGlaalK8Hora2Fp6enli1ahU2b94sV7djxw64urpiw4YNsLOzg5eXFz766CNkZ2dz+9lhxInoEcbb2xsdHR3o7OyEl5eXUn0YGRnB2toaEolE+OigsoyNjeUGbFdXF/eRJI1haWmJsrIytLS0CEmzqqoK5eXlkMlkqKysFA6ZTIZLly7hypUrCvuKiIiAlpYWdu/ePaSYjI2NAUBu3FRWVg6pTyLSHOrO06mpqdi2bRtOnTqFmTNnPrL9zZs38fvvvwsT0I6OjtDT00NdXZ3QprOzEw0NDbC0tBzchRA9gibk6aamJri5uQmr/3t/YfwwlZWVwpgBHrzqW19fL7xRAABXr16FqakpRo0aNah4iOjxUHe+fpSenh6sXbsWxcXFOHfuHCZOnChX3zuJ9997lra2tty9iGg4aEK+Hoiamhq4u7tj2bJl2L59e5/69vZ2hWMGQJ8FlqQ8HXUHQMNLW1sbP//8s/BnRdra2vpMbBkaGsLCwmLY4/Hw8EB0dDROnDiBSZMmYdeuXbhz586wn4dIWRYWFpDJZHB3d4eXlxdsbW3h7OyMF198sU9bJycnZGVlyb1u22v06NFITExEeHj4kOKxtraGhYUFEhISsH37dly9ehU7d+4cUp9EpDnUmadTUlIQFxeHQ4cOwcrKCi0tLQAe7HErFotx7949JCYmIiAgABKJBNeuXcO7774La2tr4SF87NixWL16NeLj42FhYQFLS0vhnhgYGDik+IgUUWee7p2EtrS0RHp6Om7fvi3U9X4P5cCBAxg1ahQcHBwAAEVFRcjOzsb+/fuFtmvWrEFGRgaioqIQERGBX375BUlJSYiMjBxwLET0eKkzX3d0dKC2tlb4c1NTEyorKyEWi4X9ncPDw3Ho0CEcP34cY8aMEXL6uHHjoK+vD1tbW1hbWyMsLAzp6ekwNDTEsWPHcPr0aZSWlg4pPiJF1P1c3djYiNbWVjQ2NqKrq0sYm9bW1hCLxaiuroaHhwe8vLwQHR0tjBltbW1hMZifnx9WrlyJzMxMeHl5obm5GevWrYOzszPMzMwG+T9C/eGK6BFo7NixGDt2bL/1MpkMDg4OckdiYqJKYgkNDcWyZcuwdOlSzJ07F1KpFO7u7io5F5GyJkyYAJlMhpaWFhQXF8PHx0dhu4CAAOTl5fW7VcayZcsglUqHFIuuri4OHz6MK1euwM7ODikpKfyYEdEIo648nZmZiY6ODrz++uswNTUVjvT0dAAPfhD/6aef4O/vDxsbG6xYsQKOjo44f/489PT0hH7S0tIQFBSEJUuWwMnJCTdu3MC5c+dgYGAw5BiJFFFXnj59+jTq6+tx9uxZTJgwQW7c/Nu2bdvg6OgIFxcXHD9+HAUFBQgJCRHqLSws8OWXX+Ly5cuws7NDZGQkoqKisHHjxgHHQkSPn7ry9a1bt4T+mpubkZ6eDgcHB7z55ptCm8zMTLS1tcHNzU3u3tS7966uri5OnjwJY2Nj+Pn5wc7ODnl5eThw4ADmz58/5BiJFFHnc3VcXBwcHByEvap7x1B5eTkAoLCwELdv38bBgwflxoyTk5PQx/Lly7Fr1y5kZGRg2rRpCAwMxOTJk1FUVDSoWOjhRD1cX05EREREREREREREKsQV0URERERERERERESkUpyIpgHr3UNS0XH+/Hl1h0ekcZKSkvodM/29pkREpCzmaaLBYZ4mInVgviYaHObrkYVbc9CA1dfX91tnbm4OfX39xxgNkeZrbW1Fa2urwjp9fX2Ym5s/5oiIaCRjniYaHOZpIlIH5muiwWG+Hlk4EU1EREREREREREREKsWtOYiIiIiIiIiIiIhIpTgRTUREREREREREREQqxYloIiIiIiIiIiIiIlIpTkQTERERERERERERkUpxIpqIiIiIiIiIiIiIVIoT0URERERERERERESkUpyIJiIiIiIiIiIiIiKV4kQ0EREREREREREREanU/wAycvzbPii/KQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_256': mlp_256_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_256': kan_256_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_256'], data['KAN_256']], labels=['MLP_256', 'KAN_256'])\n",
    "    axes[1].set_title('Reduced models (256)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[2].set_title('Reduced models (128)')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
