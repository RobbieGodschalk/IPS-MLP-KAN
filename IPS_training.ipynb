{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (7200, 623)\n",
      "df_train_x: (6480, 620)\n",
      "df_train_y: (6480, 2)\n",
      "df_val_x: (720, 620)\n",
      "df_val_y: (720, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test01-02-03_full: (21600, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "sets = ['01', '02', '03'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test01-02-03_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=620):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_256 = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-12 23:38:57,264] A new study created in memory with name: no-name-381a09bd-5914-43d3-93c4-dc04a0a16feb\n",
      "[I 2024-06-12 23:38:58,376] Trial 0 finished with value: 11.449127515157064 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 700, 'dropout_rate': 0.31976106143934335, 'lr': 0.009749711155494944, 'batch_size': 304, 'epochs': 132}. Best is trial 0 with value: 11.449127515157064.\n",
      "[I 2024-06-12 23:39:04,509] Trial 1 finished with value: 7.119401693344116 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 700, 'dropout_rate': 0.34088888559281244, 'lr': 0.009948123110353757, 'batch_size': 464, 'epochs': 148}. Best is trial 1 with value: 7.119401693344116.\n",
      "[I 2024-06-12 23:39:06,697] Trial 2 finished with value: 3.0400272210439048 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'dropout_rate': 0.2485248391784196, 'lr': 0.009558885256906634, 'batch_size': 272, 'epochs': 106}. Best is trial 2 with value: 3.0400272210439048.\n",
      "[I 2024-06-12 23:39:15,624] Trial 3 finished with value: 2.547054648399353 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'dropout_rate': 0.4139169431744873, 'lr': 0.004378275635083045, 'batch_size': 448, 'epochs': 113}. Best is trial 3 with value: 2.547054648399353.\n",
      "[I 2024-06-12 23:39:18,934] Trial 4 finished with value: 5.459771394729614 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 512, 'dropout_rate': 0.29631911580233683, 'lr': 0.009389670008390801, 'batch_size': 448, 'epochs': 78}. Best is trial 3 with value: 2.547054648399353.\n",
      "[I 2024-06-12 23:39:20,857] Trial 5 finished with value: 3.578303575515747 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'dropout_rate': 0.3778332822447813, 'lr': 0.00732224021736892, 'batch_size': 288, 'epochs': 147}. Best is trial 3 with value: 2.547054648399353.\n",
      "[I 2024-06-12 23:39:21,614] Trial 6 finished with value: 17.952659606933594 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'dropout_rate': 0.29365942155453256, 'lr': 0.00301638531269351, 'batch_size': 352, 'epochs': 127}. Best is trial 3 with value: 2.547054648399353.\n",
      "[I 2024-06-12 23:39:26,936] Trial 7 finished with value: 2.1722165743509927 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.3189817082032988, 'lr': 0.009429366736387182, 'batch_size': 336, 'epochs': 92}. Best is trial 7 with value: 2.1722165743509927.\n",
      "[I 2024-06-12 23:39:43,486] Trial 8 finished with value: 2.3549116168703352 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 256, 'dropout_rate': 0.4064868439107232, 'lr': 0.006427863590640759, 'batch_size': 112, 'epochs': 64}. Best is trial 7 with value: 2.1722165743509927.\n",
      "[I 2024-06-12 23:39:51,352] Trial 9 finished with value: 1.8319326043128967 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'dropout_rate': 0.25284606918803687, 'lr': 0.00611206466219449, 'batch_size': 224, 'epochs': 52}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:09,143] Trial 10 finished with value: 2.041552186012268 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 620, 'dropout_rate': 0.5391245462394895, 'lr': 0.001881009529818743, 'batch_size': 128, 'epochs': 52}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:12,257] Trial 11 finished with value: 57.83051347732544 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 32, 'dropout_rate': 0.5527525876886507, 'lr': 0.0011156037048321464, 'batch_size': 96, 'epochs': 51}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:26,931] Trial 12 finished with value: 3.2521392345428466 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 620, 'dropout_rate': 0.5824217023048602, 'lr': 0.004388087109072587, 'batch_size': 160, 'epochs': 52}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:31,085] Trial 13 finished with value: 9.430859565734863 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 16, 'dropout_rate': 0.494538303347476, 'lr': 0.0013469584757914215, 'batch_size': 192, 'epochs': 73}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:51,479] Trial 14 finished with value: 8.169750308990478 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 64, 'dropout_rate': 0.4763275991698954, 'lr': 0.007684547519687962, 'batch_size': 16, 'epochs': 90}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:53,736] Trial 15 finished with value: 9.526580333709717 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'dropout_rate': 0.24513306695160855, 'lr': 0.005116166168510293, 'batch_size': 224, 'epochs': 65}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:40:58,246] Trial 16 finished with value: 5.146513938903809 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'dropout_rate': 0.2050223973458833, 'lr': 0.002746780377816889, 'batch_size': 48, 'epochs': 61}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:41:11,959] Trial 17 finished with value: 2.642675828933716 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 128, 'dropout_rate': 0.483260536898099, 'lr': 0.0060165135028624386, 'batch_size': 144, 'epochs': 80}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:41:15,707] Trial 18 finished with value: 23.134253184000652 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 700, 'dropout_rate': 0.5285744983674537, 'lr': 0.002890439207202152, 'batch_size': 80, 'epochs': 53}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:41:21,916] Trial 19 finished with value: 2.621865600347519 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 256, 'dropout_rate': 0.43985778316527896, 'lr': 0.007979883381534953, 'batch_size': 224, 'epochs': 87}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:41:23,958] Trial 20 finished with value: 9.170560359954834 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 16, 'dropout_rate': 0.3668658253755357, 'lr': 0.003950472421537107, 'batch_size': 384, 'epochs': 69}. Best is trial 9 with value: 1.8319326043128967.\n",
      "[I 2024-06-12 23:41:29,016] Trial 21 finished with value: 1.636327862739563 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.25736470346468987, 'lr': 0.008587180359783972, 'batch_size': 352, 'epochs': 97}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:33,678] Trial 22 finished with value: 2.0745368003845215 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.25147809551299755, 'lr': 0.008402858814286162, 'batch_size': 400, 'epochs': 102}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:38,691] Trial 23 finished with value: 1.9268164932727814 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.20338384396794001, 'lr': 0.00641837901366089, 'batch_size': 224, 'epochs': 117}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:44,238] Trial 24 finished with value: 1.9678933024406433 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2091376464162655, 'lr': 0.006765360231903586, 'batch_size': 512, 'epochs': 119}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:48,353] Trial 25 finished with value: 2.1504503885904946 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2672096627110354, 'lr': 0.005589925575944163, 'batch_size': 240, 'epochs': 132}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:52,058] Trial 26 finished with value: 1.9393515825271606 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.22294990842357623, 'lr': 0.008534532831671068, 'batch_size': 176, 'epochs': 114}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:41:58,723] Trial 27 finished with value: 2.0579572121302285 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2778245377310358, 'lr': 0.006589583704422821, 'batch_size': 320, 'epochs': 99}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:06,783] Trial 28 finished with value: 1.6809882720311482 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.20780434245286855, 'lr': 0.007249631104370388, 'batch_size': 256, 'epochs': 140}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:07,946] Trial 29 finished with value: 9.082760492960611 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 700, 'dropout_rate': 0.34864187723948775, 'lr': 0.00876428093734989, 'batch_size': 304, 'epochs': 142}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:15,906] Trial 30 finished with value: 2.0586748123168945 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'dropout_rate': 0.3202649210630591, 'lr': 0.00717809209439726, 'batch_size': 368, 'epochs': 139}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:24,657] Trial 31 finished with value: 1.6564410130182903 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.22510997471013164, 'lr': 0.005376540639003076, 'batch_size': 256, 'epochs': 126}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:30,748] Trial 32 finished with value: 1.863727609316508 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.23855612925503974, 'lr': 0.005239019296269964, 'batch_size': 256, 'epochs': 126}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:31,487] Trial 33 finished with value: 10.788470268249512 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'dropout_rate': 0.22805452381823837, 'lr': 0.005810804533498883, 'batch_size': 272, 'epochs': 133}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:42,751] Trial 34 finished with value: 1.6406337022781372 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.27850646441398064, 'lr': 0.007878050716660137, 'batch_size': 192, 'epochs': 150}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:42:47,746] Trial 35 finished with value: 2.256311297416687 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.28031392349329015, 'lr': 0.007954226505025088, 'batch_size': 192, 'epochs': 150}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:02,192] Trial 36 finished with value: 1.9070319334665935 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 512, 'dropout_rate': 0.3080151751671627, 'lr': 0.008964900439322321, 'batch_size': 304, 'epochs': 142}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:09,305] Trial 37 finished with value: 2.905380964279175 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'dropout_rate': 0.2642094168071963, 'lr': 0.007344067412742855, 'batch_size': 432, 'epochs': 136}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:18,483] Trial 38 finished with value: 8.402427037556967 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'dropout_rate': 0.333960451978879, 'lr': 0.009994516100854165, 'batch_size': 272, 'epochs': 124}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:22,381] Trial 39 finished with value: 2.5983153581619263 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'dropout_rate': 0.28948943372823993, 'lr': 0.007003708923054807, 'batch_size': 416, 'epochs': 145}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:23,771] Trial 40 finished with value: 13.283348401387533 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'dropout_rate': 0.22456378688683237, 'lr': 0.008214941881264348, 'batch_size': 336, 'epochs': 106}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:33,073] Trial 41 finished with value: 1.7010255753993988 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.2609742373617432, 'lr': 0.004593691193767072, 'batch_size': 208, 'epochs': 137}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:40,775] Trial 42 finished with value: 1.7714355885982513 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.26517620739550424, 'lr': 0.004547555803974977, 'batch_size': 192, 'epochs': 138}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:44,844] Trial 43 finished with value: 2.0630327463150024 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.23150288389556117, 'lr': 0.003527812987337299, 'batch_size': 256, 'epochs': 129}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:43:52,056] Trial 44 finished with value: 2.0060245196024575 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'dropout_rate': 0.3132413669495557, 'lr': 0.0091121164931723, 'batch_size': 288, 'epochs': 122}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:03,586] Trial 45 finished with value: 1.7427895963191986 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.295495711173686, 'lr': 0.004667106457567707, 'batch_size': 208, 'epochs': 150}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:14,202] Trial 46 finished with value: 3.6577210903167723 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 620, 'dropout_rate': 0.2506948898002793, 'lr': 0.009412591011250282, 'batch_size': 160, 'epochs': 110}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:15,248] Trial 47 finished with value: 11.151433944702148 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'dropout_rate': 0.21654832812166244, 'lr': 0.005123279686991506, 'batch_size': 240, 'epochs': 145}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:31,417] Trial 48 finished with value: 2.4595910708109536 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 64, 'dropout_rate': 0.387773631025799, 'lr': 0.007800416543608971, 'batch_size': 336, 'epochs': 133}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:38,307] Trial 49 finished with value: 1.875792908668518 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 256, 'dropout_rate': 0.24111036766617452, 'lr': 0.007444186363109366, 'batch_size': 144, 'epochs': 145}. Best is trial 21 with value: 1.636327862739563.\n",
      "[I 2024-06-12 23:44:53,972] A new study created in memory with name: no-name-92855138-ad2b-4eb3-8091-5cf6b38e43ce\n",
      "[I 2024-06-12 23:44:55,297] Trial 0 finished with value: 15.698930740356445 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 620, 'dropout_rate': 0.2486600570745281, 'lr': 0.0028248746321964183, 'batch_size': 320, 'epochs': 146}. Best is trial 0 with value: 15.698930740356445.\n",
      "[I 2024-06-12 23:45:03,886] Trial 1 finished with value: 2.9026061216990153 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3423716935777949, 'lr': 0.006152141128138202, 'batch_size': 48, 'epochs': 137}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:11,967] Trial 2 finished with value: 4.543562889099121 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'dropout_rate': 0.49470186371937247, 'lr': 0.0076502728779025055, 'batch_size': 448, 'epochs': 102}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:14,907] Trial 3 finished with value: 18.11362075805664 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 620, 'dropout_rate': 0.45641074667951276, 'lr': 0.0063076109113382635, 'batch_size': 384, 'epochs': 101}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:25,932] Trial 4 finished with value: 3.6696122090021768 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 700, 'dropout_rate': 0.21076804844525948, 'lr': 0.00936568123240145, 'batch_size': 64, 'epochs': 98}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:28,705] Trial 5 finished with value: 3.483612298965454 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 128, 'dropout_rate': 0.35572233936652564, 'lr': 0.00949762863876815, 'batch_size': 432, 'epochs': 113}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:43,868] Trial 6 finished with value: 4.227385997772217 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'dropout_rate': 0.23949658715261304, 'lr': 0.005836070706258084, 'batch_size': 208, 'epochs': 112}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:45,847] Trial 7 finished with value: 8.229222456614176 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'dropout_rate': 0.4707797148870486, 'lr': 0.007853877239996749, 'batch_size': 272, 'epochs': 116}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:45:53,965] Trial 8 finished with value: 3.218884086608887 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 64, 'dropout_rate': 0.21357951859820323, 'lr': 0.00478790567900513, 'batch_size': 144, 'epochs': 68}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:46:06,173] Trial 9 finished with value: 5.453428626060486 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 700, 'hidden_layer_size_3': 700, 'dropout_rate': 0.5907337944953803, 'lr': 0.0034105912881108712, 'batch_size': 208, 'epochs': 115}. Best is trial 1 with value: 2.9026061216990153.\n",
      "[I 2024-06-12 23:46:23,449] Trial 10 finished with value: 2.5294201892355215 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3567138938883453, 'lr': 0.001029157391692812, 'batch_size': 32, 'epochs': 147}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:46:37,855] Trial 11 finished with value: 2.8330693085988363 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3403128636906095, 'lr': 0.0013945819610353212, 'batch_size': 16, 'epochs': 149}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:46:44,433] Trial 12 finished with value: 3.1979502543159155 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.31218906313651595, 'lr': 0.0013202741008110082, 'batch_size': 32, 'epochs': 148}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:46:51,850] Trial 13 finished with value: 2.918061876296997 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.4049021904341926, 'lr': 0.0012303104682292506, 'batch_size': 144, 'epochs': 133}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:46:53,919] Trial 14 finished with value: 5.893666676112583 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2915575788338169, 'lr': 0.002574793816071639, 'batch_size': 112, 'epochs': 50}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:46:54,722] Trial 15 finished with value: 16.7514066696167 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'dropout_rate': 0.39935569721459574, 'lr': 0.004326417091984537, 'batch_size': 512, 'epochs': 130}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:12,994] Trial 16 finished with value: 3.0743056535720825 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3906030853182158, 'lr': 0.002058815114368411, 'batch_size': 16, 'epochs': 83}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:18,948] Trial 17 finished with value: 4.720736980438232 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'dropout_rate': 0.2778574613066843, 'lr': 0.0037532220310476467, 'batch_size': 112, 'epochs': 127}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:21,499] Trial 18 finished with value: 35.44615364074707 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 512, 'dropout_rate': 0.5360043987267694, 'lr': 0.001126057421458165, 'batch_size': 192, 'epochs': 149}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:23,338] Trial 19 finished with value: 9.547061602274576 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'dropout_rate': 0.3411578380314282, 'lr': 0.002279725787127272, 'batch_size': 272, 'epochs': 138}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:37,281] Trial 20 finished with value: 3.9963766038417816 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 256, 'dropout_rate': 0.43101895186548445, 'lr': 0.001853046061407217, 'batch_size': 96, 'epochs': 124}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:48,906] Trial 21 finished with value: 3.9313774506251016 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3459673133086511, 'lr': 0.00691416231790176, 'batch_size': 16, 'epochs': 140}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:53,589] Trial 22 finished with value: 4.881009483337403 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'dropout_rate': 0.36862266319572506, 'lr': 0.004519968708485269, 'batch_size': 48, 'epochs': 139}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:47:58,537] Trial 23 finished with value: 3.066867768764496 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3145122859308827, 'lr': 0.005257738438706769, 'batch_size': 64, 'epochs': 150}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:00,508] Trial 24 finished with value: 4.017155090967814 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3158230131775224, 'lr': 0.0031419284356508197, 'batch_size': 80, 'epochs': 139}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:10,336] Trial 25 finished with value: 2.7026662826538086 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.27773234004248887, 'lr': 0.00840864661739712, 'batch_size': 160, 'epochs': 126}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:14,657] Trial 26 finished with value: 2.694057655334473 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.26871046032992035, 'lr': 0.008610861337195817, 'batch_size': 160, 'epochs': 124}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:18,770] Trial 27 finished with value: 2.714001703262329 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.26339091131751957, 'lr': 0.00878851865978337, 'batch_size': 160, 'epochs': 122}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:21,292] Trial 28 finished with value: 12.037177403767904 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 16, 'dropout_rate': 0.2884019588556108, 'lr': 0.008268249990779304, 'batch_size': 240, 'epochs': 89}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:23,905] Trial 29 finished with value: 3.1836419900258384 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 512, 'dropout_rate': 0.24714915807969917, 'lr': 0.009938150617170264, 'batch_size': 336, 'epochs': 124}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:24,959] Trial 30 finished with value: 13.618529510498046 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'dropout_rate': 0.25442941304498756, 'lr': 0.006950793282426353, 'batch_size': 176, 'epochs': 105}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:27,640] Trial 31 finished with value: 3.312145137786865 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.26729476973994276, 'lr': 0.008712920286776282, 'batch_size': 144, 'epochs': 123}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:29,927] Trial 32 finished with value: 3.254174073537191 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.22506218673879344, 'lr': 0.008814569675829297, 'batch_size': 240, 'epochs': 119}. Best is trial 10 with value: 2.5294201892355215.\n",
      "[I 2024-06-12 23:48:36,638] Trial 33 finished with value: 2.4940462907155356 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.30902402642194593, 'lr': 0.007359362083259485, 'batch_size': 304, 'epochs': 108}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:48:43,315] Trial 34 finished with value: 3.508190393447876 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3016905041505446, 'lr': 0.007442545737336488, 'batch_size': 304, 'epochs': 92}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:48:47,048] Trial 35 finished with value: 2.8939106464385986 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3239162815663228, 'lr': 0.006587885708233174, 'batch_size': 320, 'epochs': 131}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:48:49,447] Trial 36 finished with value: 3.340209643046061 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'dropout_rate': 0.37573373002921295, 'lr': 0.008210602046249134, 'batch_size': 288, 'epochs': 108}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:49:01,713] Trial 37 finished with value: 7.9329752922058105 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 256, 'dropout_rate': 0.41937570243770217, 'lr': 0.007621717086910617, 'batch_size': 336, 'epochs': 96}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:49:15,446] Trial 38 finished with value: 4.498899459838867 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.20014942754696693, 'lr': 0.0094156991736182, 'batch_size': 384, 'epochs': 109}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:49:16,520] Trial 39 finished with value: 11.727948188781738 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 16, 'dropout_rate': 0.24097047662009044, 'lr': 0.006012700090043177, 'batch_size': 240, 'epochs': 143}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:49:22,533] Trial 40 finished with value: 2.9540470838546753 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'dropout_rate': 0.3684908534122484, 'lr': 0.00716097286572423, 'batch_size': 384, 'epochs': 134}. Best is trial 33 with value: 2.4940462907155356.\n",
      "[I 2024-06-12 23:49:29,316] Trial 41 finished with value: 2.4758393287658693 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.26292473852852644, 'lr': 0.008514374489529084, 'batch_size': 160, 'epochs': 120}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:32,721] Trial 42 finished with value: 2.988340139389038 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.27711002033056176, 'lr': 0.008243957716627903, 'batch_size': 208, 'epochs': 115}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:38,304] Trial 43 finished with value: 2.5706008275349936 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.21954111513148272, 'lr': 0.009041405588603424, 'batch_size': 128, 'epochs': 118}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:42,262] Trial 44 finished with value: 13.614333152770996 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 128, 'dropout_rate': 0.23345359969440535, 'lr': 0.009927850783652213, 'batch_size': 112, 'epochs': 101}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:47,050] Trial 45 finished with value: 2.7750020821889243 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'dropout_rate': 0.22093383174711836, 'lr': 0.009081758320065206, 'batch_size': 80, 'epochs': 110}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:52,358] Trial 46 finished with value: 2.737265388170878 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.30017875357251356, 'lr': 0.007924701067745376, 'batch_size': 128, 'epochs': 119}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:49:57,770] Trial 47 finished with value: 2.9354649782180786 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'dropout_rate': 0.25345544194855935, 'lr': 0.005468352127643665, 'batch_size': 208, 'epochs': 80}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:50:02,145] Trial 48 finished with value: 2.8412075996398927 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'dropout_rate': 0.33263135991066584, 'lr': 0.009134988154236516, 'batch_size': 176, 'epochs': 118}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:50:05,779] Trial 49 finished with value: 4.2475244998931885 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'dropout_rate': 0.23278842619367546, 'lr': 0.00784483245000817, 'batch_size': 48, 'epochs': 104}. Best is trial 41 with value: 2.4758393287658693.\n",
      "[I 2024-06-12 23:50:24,667] A new study created in memory with name: no-name-b8eb8112-791d-4a1b-bcb3-c25fdb116e48\n",
      "[I 2024-06-12 23:50:27,853] Trial 0 finished with value: 19.679834365844727 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 700, 'dropout_rate': 0.47365743453704584, 'lr': 0.002660526564072451, 'batch_size': 416, 'epochs': 123}. Best is trial 0 with value: 19.679834365844727.\n",
      "[I 2024-06-12 23:50:29,719] Trial 1 finished with value: 8.974696159362793 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3289341435328813, 'lr': 0.0010248783564398613, 'batch_size': 464, 'epochs': 123}. Best is trial 1 with value: 8.974696159362793.\n",
      "[I 2024-06-12 23:50:50,396] Trial 2 finished with value: 8.575058460235596 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 512, 'dropout_rate': 0.3897535728157364, 'lr': 0.007108069022890501, 'batch_size': 384, 'epochs': 108}. Best is trial 2 with value: 8.575058460235596.\n",
      "[I 2024-06-12 23:50:55,284] Trial 3 finished with value: 8.469772974650065 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 256, 'dropout_rate': 0.5947144747413978, 'lr': 0.006972965400204055, 'batch_size': 64, 'epochs': 58}. Best is trial 3 with value: 8.469772974650065.\n",
      "[I 2024-06-12 23:50:57,779] Trial 4 finished with value: 22.45365619659424 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 700, 'dropout_rate': 0.41723913642227894, 'lr': 0.0063673906307593125, 'batch_size': 416, 'epochs': 138}. Best is trial 3 with value: 8.469772974650065.\n",
      "[I 2024-06-12 23:51:14,976] Trial 5 finished with value: 2.451686143875122 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'dropout_rate': 0.32694156540357544, 'lr': 0.006329534408596307, 'batch_size': 368, 'epochs': 147}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:17,954] Trial 6 finished with value: 51.30221430460612 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 16, 'dropout_rate': 0.41744084844827467, 'lr': 0.005527560213995081, 'batch_size': 288, 'epochs': 93}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:28,434] Trial 7 finished with value: 4.914132881164551 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 512, 'dropout_rate': 0.32532302025858295, 'lr': 0.003968032693391122, 'batch_size': 160, 'epochs': 125}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:48,891] Trial 8 finished with value: 3.7448811012765635 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 700, 'dropout_rate': 0.31544879545791404, 'lr': 0.0035193594741341674, 'batch_size': 32, 'epochs': 146}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:49,823] Trial 9 finished with value: 32.62592124938965 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'dropout_rate': 0.5833583818740793, 'lr': 0.0061059984009174464, 'batch_size': 480, 'epochs': 88}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:52,006] Trial 10 finished with value: 3.225742975870768 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'dropout_rate': 0.20032625134009735, 'lr': 0.009921135853925567, 'batch_size': 304, 'epochs': 68}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:54,672] Trial 11 finished with value: 3.108896334966024 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'dropout_rate': 0.20578427632483057, 'lr': 0.009959094700778876, 'batch_size': 304, 'epochs': 57}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:51:57,728] Trial 12 finished with value: 3.367527663707733 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'dropout_rate': 0.2277510675611115, 'lr': 0.00941427827971602, 'batch_size': 208, 'epochs': 77}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:01,247] Trial 13 finished with value: 2.7775933345158896 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2640617320742432, 'lr': 0.008367620718050436, 'batch_size': 352, 'epochs': 105}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:03,333] Trial 14 finished with value: 3.482309182484945 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2585255712689952, 'lr': 0.008219821578832432, 'batch_size': 336, 'epochs': 106}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:04,426] Trial 15 finished with value: 8.768755555152893 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 32, 'dropout_rate': 0.2818771140949411, 'lr': 0.00826189625749892, 'batch_size': 192, 'epochs': 147}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:05,448] Trial 16 finished with value: 25.818202018737793 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'dropout_rate': 0.371283067783007, 'lr': 0.008148698859271572, 'batch_size': 512, 'epochs': 115}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:07,324] Trial 17 finished with value: 11.97216272354126 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 128, 'dropout_rate': 0.47508170017353446, 'lr': 0.0049515208281819345, 'batch_size': 368, 'epochs': 137}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:10,028] Trial 18 finished with value: 13.631922245025635 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'dropout_rate': 0.28088714093046685, 'lr': 0.00884629288365144, 'batch_size': 224, 'epochs': 94}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:11,031] Trial 19 finished with value: 14.756182289123535 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'dropout_rate': 0.34667534735337063, 'lr': 0.007014629832246972, 'batch_size': 144, 'epochs': 81}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:13,638] Trial 20 finished with value: 3.5054866472880044 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'dropout_rate': 0.2470560285408988, 'lr': 0.004800795285406784, 'batch_size': 240, 'epochs': 133}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:16,792] Trial 21 finished with value: 3.099183718363444 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'dropout_rate': 0.20090193420962274, 'lr': 0.009928685227425786, 'batch_size': 320, 'epochs': 56}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:19,498] Trial 22 finished with value: 12.031312306722006 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.28754199447254264, 'lr': 0.007729519593929377, 'batch_size': 336, 'epochs': 50}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:22,446] Trial 23 finished with value: 7.86940860748291 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'dropout_rate': 0.24272798930960676, 'lr': 0.009104140388953479, 'batch_size': 416, 'epochs': 75}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:26,278] Trial 24 finished with value: 3.183223088582357 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'dropout_rate': 0.28168649359810277, 'lr': 0.008758994276563101, 'batch_size': 272, 'epochs': 106}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:28,969] Trial 25 finished with value: 5.987216154734294 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 620, 'dropout_rate': 0.22396294426857347, 'lr': 0.007700976983324139, 'batch_size': 352, 'epochs': 65}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:30,818] Trial 26 finished with value: 8.249724864959717 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'dropout_rate': 0.30350823452020104, 'lr': 0.009485339304408676, 'batch_size': 448, 'epochs': 100}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:46,423] Trial 27 finished with value: 2.5212595462799072 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.26878550540998275, 'lr': 0.006258106912312478, 'batch_size': 384, 'epochs': 84}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:47,906] Trial 28 finished with value: 16.41035747528076 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'dropout_rate': 0.45001030257765373, 'lr': 0.0060322171998018195, 'batch_size': 384, 'epochs': 83}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:48,931] Trial 29 finished with value: 20.15983009338379 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 700, 'dropout_rate': 0.361176799664853, 'lr': 0.004409759901478888, 'batch_size': 416, 'epochs': 115}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:50,437] Trial 30 finished with value: 24.992605209350586 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'dropout_rate': 0.5518143275139734, 'lr': 0.0027329100300437304, 'batch_size': 256, 'epochs': 72}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:52:58,704] Trial 31 finished with value: 2.7953224976857505 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.22847307209351442, 'lr': 0.0055303294497433975, 'batch_size': 320, 'epochs': 51}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:53:09,146] Trial 32 finished with value: 2.5668798685073853 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.25807696303264166, 'lr': 0.0051158313983887494, 'batch_size': 384, 'epochs': 122}. Best is trial 5 with value: 2.451686143875122.\n",
      "[I 2024-06-12 23:53:29,259] Trial 33 finished with value: 2.029489576816559 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.26420821474419715, 'lr': 0.001703782929709045, 'batch_size': 400, 'epochs': 124}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:53:33,673] Trial 34 finished with value: 4.218151092529297 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3465997559975665, 'lr': 0.001020545274742905, 'batch_size': 464, 'epochs': 125}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:53:34,483] Trial 35 finished with value: 17.504096031188965 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'dropout_rate': 0.3120315861647547, 'lr': 0.0018682163406885143, 'batch_size': 400, 'epochs': 116}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:53:51,406] Trial 36 finished with value: 2.1808260679244995 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2602694418054742, 'lr': 0.0017924015414368518, 'batch_size': 448, 'epochs': 132}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:53:55,319] Trial 37 finished with value: 3.7380868196487427 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3366291396734785, 'lr': 0.001955268242762051, 'batch_size': 448, 'epochs': 141}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:54:01,668] Trial 38 finished with value: 4.630011796951294 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'dropout_rate': 0.39367043540760155, 'lr': 0.0026378148693977583, 'batch_size': 480, 'epochs': 130}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:54:04,767] Trial 39 finished with value: 15.120826721191406 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 32, 'dropout_rate': 0.2972331826640595, 'lr': 0.0016050275546515885, 'batch_size': 432, 'epochs': 142}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:54:05,507] Trial 40 finished with value: 23.701181411743164 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'dropout_rate': 0.32808875371671575, 'lr': 0.003309803924237085, 'batch_size': 496, 'epochs': 148}. Best is trial 33 with value: 2.029489576816559.\n",
      "[I 2024-06-12 23:54:22,582] Trial 41 finished with value: 2.0147178769111633 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.26519376667322864, 'lr': 0.002438738604561344, 'batch_size': 400, 'epochs': 121}. Best is trial 41 with value: 2.0147178769111633.\n",
      "[I 2024-06-12 23:54:45,372] Trial 42 finished with value: 1.9457327723503113 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2738903779879456, 'lr': 0.001458136969116826, 'batch_size': 400, 'epochs': 130}. Best is trial 42 with value: 1.9457327723503113.\n",
      "[I 2024-06-12 23:55:08,952] Trial 43 finished with value: 1.9180856347084045 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.2419562642262567, 'lr': 0.0015496281955789584, 'batch_size': 432, 'epochs': 131}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:55:21,722] Trial 44 finished with value: 2.2839287519454956 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.23760495884392716, 'lr': 0.0014806461892578717, 'batch_size': 448, 'epochs': 130}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:55:24,774] Trial 45 finished with value: 11.99743366241455 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 700, 'dropout_rate': 0.21594372522626282, 'lr': 0.0024661394283541685, 'batch_size': 512, 'epochs': 121}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:55:47,789] Trial 46 finished with value: 1.9791213870048523 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.24753208464597853, 'lr': 0.0032255138248200536, 'batch_size': 432, 'epochs': 135}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:56:08,981] Trial 47 finished with value: 1.9658703804016113 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'dropout_rate': 0.23833651783649848, 'lr': 0.002248764169334116, 'batch_size': 416, 'epochs': 135}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:56:18,501] Trial 48 finished with value: 2.2885860204696655 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.24359652422851977, 'lr': 0.003101864336304814, 'batch_size': 480, 'epochs': 137}. Best is trial 43 with value: 1.9180856347084045.\n",
      "[I 2024-06-12 23:56:24,791] Trial 49 finished with value: 2.787163496017456 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'dropout_rate': 0.21923020558568312, 'lr': 0.00378913611394429, 'batch_size': 416, 'epochs': 143}. Best is trial 43 with value: 1.9180856347084045.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=620):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_256:\n",
    "    print('Starting MLP reduced grid search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/256_MLP.pth', encoders, 256)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [620] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_256 = True\n",
    "SEARCH_KAN_REDUCED_128 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-12 23:56:46,881] A new study created in memory with name: no-name-ad7c9d8e-ddda-4e22-86e4-6d75b693e483\n",
      "[I 2024-06-12 23:57:07,579] Trial 0 finished with value: 1.0675384004910786 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 16, 'lr': 0.009097547415314006, 'batch_size': 256, 'epochs': 82}. Best is trial 0 with value: 1.0675384004910786.\n",
      "[I 2024-06-12 23:57:50,563] Trial 1 finished with value: 1.5620194673538208 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'lr': 0.005881737243049447, 'batch_size': 432, 'epochs': 70}. Best is trial 0 with value: 1.0675384004910786.\n",
      "[I 2024-06-12 23:58:14,819] Trial 2 finished with value: 0.8973522086938223 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'lr': 0.005523293775951954, 'batch_size': 128, 'epochs': 70}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:01:45,885] Trial 3 finished with value: 1.4421514868736267 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'lr': 0.0073150922710988305, 'batch_size': 400, 'epochs': 92}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:03:01,253] Trial 4 finished with value: 1.092411955197652 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 620, 'lr': 0.0029902938708192745, 'batch_size': 352, 'epochs': 65}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:03:14,782] Trial 5 finished with value: 1.4102294445037842 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 256, 'lr': 0.0057150933814127105, 'batch_size': 416, 'epochs': 58}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:03:41,807] Trial 6 finished with value: 1.0767255425453186 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 256, 'lr': 0.0014823347691649174, 'batch_size': 432, 'epochs': 67}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:03:58,192] Trial 7 finished with value: 1.477042019367218 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 128, 'lr': 0.006032826477146079, 'batch_size': 496, 'epochs': 113}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:04:51,984] Trial 8 finished with value: 1.4797644813855488 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'lr': 0.005394791149332081, 'batch_size': 352, 'epochs': 109}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:05:12,765] Trial 9 finished with value: 1.7582439184188843 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'lr': 0.006988539660317762, 'batch_size': 400, 'epochs': 56}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:05:48,941] Trial 10 finished with value: 0.9133333497577243 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'lr': 0.0032002906009009676, 'batch_size': 80, 'epochs': 141}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:06:24,805] Trial 11 finished with value: 0.9194977333148321 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'lr': 0.003528655648935133, 'batch_size': 64, 'epochs': 150}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:06:47,324] Trial 12 finished with value: 1.1760573983192444 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'lr': 0.0039062012021385145, 'batch_size': 80, 'epochs': 137}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:07:53,821] Trial 13 finished with value: 0.9969597339630127 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 64, 'lr': 0.0019503469473199672, 'batch_size': 160, 'epochs': 128}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:09:29,575] Trial 14 finished with value: 1.065349781513214 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'lr': 0.004515759995780237, 'batch_size': 144, 'epochs': 87}. Best is trial 2 with value: 0.8973522086938223.\n",
      "[I 2024-06-13 00:12:36,375] Trial 15 finished with value: 0.8655959324704277 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'lr': 0.002468212708709788, 'batch_size': 16, 'epochs': 106}. Best is trial 15 with value: 0.8655959324704277.\n",
      "[I 2024-06-13 00:14:32,772] Trial 16 finished with value: 1.4984602954652575 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'lr': 0.009013564877626208, 'batch_size': 16, 'epochs': 106}. Best is trial 15 with value: 0.8655959324704277.\n",
      "[I 2024-06-13 00:16:13,413] Trial 17 finished with value: 1.030744269490242 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'lr': 0.007541928207170085, 'batch_size': 192, 'epochs': 117}. Best is trial 15 with value: 0.8655959324704277.\n",
      "[I 2024-06-13 00:26:29,989] Trial 18 finished with value: 0.7887704524728987 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 32, 'lr': 0.0022333137878054288, 'batch_size': 16, 'epochs': 98}. Best is trial 18 with value: 0.7887704524728987.\n",
      "[I 2024-06-13 00:31:01,718] Trial 19 finished with value: 0.8866104587264683 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 32, 'lr': 0.0023216018176398863, 'batch_size': 32, 'epochs': 122}. Best is trial 18 with value: 0.7887704524728987.\n",
      "[I 2024-06-13 00:35:47,443] Trial 20 finished with value: 0.791212777296702 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 16, 'lr': 0.0014431706040452224, 'batch_size': 272, 'epochs': 98}. Best is trial 18 with value: 0.7887704524728987.\n",
      "[I 2024-06-13 00:39:01,799] Trial 21 finished with value: 0.9173876245816549 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 16, 'lr': 0.0012430202176619534, 'batch_size': 256, 'epochs': 99}. Best is trial 18 with value: 0.7887704524728987.\n",
      "[W 2024-06-13 00:41:20,977] Trial 22 failed with parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 32, 'lr': 0.0010687425337386832, 'batch_size': 192, 'epochs': 99} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_2148\\2069080245.py\", line 34, in <lambda>\n",
      "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_2148\\920711575.py\", line 31, in KAN_full_optimize\n",
      "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_2148\\1050143317.py\", line 36, in train_KAN\n",
      "    loss.backward()\n",
      "  File \"d:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\torch\\_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"d:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-13 00:41:20,978] Trial 22 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[250], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# study.optimize(MLP_full_gridsearch, n_trials=2)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAN_full_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRIALS_KAN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m pretty_print_study(study)\n\u001b[0;32m     37\u001b[0m save_best_KAN(study, \u001b[38;5;241m620\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/KAN/full_KAN.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[250], line 34\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     32\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# study.optimize(MLP_full_gridsearch, n_trials=2)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mKAN_full_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], n_trials\u001b[38;5;241m=\u001b[39mTRIALS_KAN)\n\u001b[0;32m     36\u001b[0m pretty_print_study(study)\n\u001b[0;32m     37\u001b[0m save_best_KAN(study, \u001b[38;5;241m620\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/KAN/full_KAN.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[248], line 31\u001b[0m, in \u001b[0;36mKAN_full_optimize\u001b[1;34m(trial, optim)\u001b[0m\n\u001b[0;32m     28\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(t_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model, return validation loss\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m val_loss, trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_KAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss, trained_model\n",
      "Cell \u001b[1;32mIn[247], line 36\u001b[0m, in \u001b[0;36mtrain_KAN\u001b[1;34m(kan_model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m kan_model(inputs)\n\u001b[0;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# Accumulate the loss for this epoch\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 620, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_256:\n",
    "    print('Starting KAN reduced search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 256, f'./models/KAN/256_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "256\n",
      "512\n",
      "256\n",
      "128\n",
      "512\n",
      "256\n",
      "512\n",
      "256\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "perform_evaluation = False\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '256': './models/MLP/256_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '256': './models/KAN/256_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '256': ['./models/MLP/256_encoder_512.pth', './models/MLP/256_encoder_256.pth'],\n",
    "        '128': ['./models/MLP/128_encoder_512.pth', './models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '256': ['./models/KAN/256_encoder_512.pth', './models/KAN/256_encoder_256.pth'],\n",
    "        '128': ['./models/KAN/128_encoder_512.pth', './models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 128, the SAE will have 512 -> 256 -> 128\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 620\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 512 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [64, 16, 256, 256], 0.538091733959047, 620)\n",
    "    mlp_256 = load_MLP_model(model_paths['MLP']['256'], [32, 64], 0.30902776457290404, 256)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [128, 700, 128], 0.510021986710484, 128)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [256, 620, 128, 64], 620)\n",
    "    kan_256 = load_KAN_model(model_paths['KAN']['256'], [64, 256, 128], 256)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [512, 700, 128], 128)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_256 = load_SAE(SAE_paths['MLP']['256'], 256)\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    kan_SAE_256 = load_SAE(SAE_paths['KAN']['256'], 256)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n",
      "(21600,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy()\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1))\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_256 = stacked_encode_data(X_test_tensor, mlp_SAE_256)\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    kan_test_data_encoded_256 = stacked_encode_data(X_test_tensor, kan_SAE_256)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_256_distances = evaluate_model(mlp_256, mlp_test_data_encoded_256, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_256_distances = evaluate_model(kan_256, kan_test_data_encoded_256, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_256_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_256_distances.shape)\n",
    "    print(kan_128_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_256': mlp_256_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_256': kan_256_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_256'], data['KAN_256']], labels=['MLP_256', 'KAN_256'])\n",
    "    axes[1].set_title('Reduced models (256)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[2].set_title('Reduced models (128)')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
