{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (4320, 451)\n",
      "df_train_x: (3888, 448)\n",
      "df_train_y: (3888, 2)\n",
      "df_val_x: (432, 448)\n",
      "df_val_y: (432, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/V1.0/'\n",
    "training_months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 448\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test01-02-03_full: (12960, 451)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "sets = ['01', '02', '03'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test01-02-03_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=448):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Update input_dim for next layer\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [32, 16, 8, 4, 2]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [32, 16, 8, 4, 2]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "SEARCH_MLP_REDUCED_64 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-15 22:39:43,605] A new study created in memory with name: no-name-7470c12e-7295-4565-a3ac-55d1338e9644\n",
      "[I 2024-06-15 22:39:44,776] Trial 0 finished with value: 37.414798736572266 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 8, 'dropout_rate': 0.35466506621678645, 'lr': 0.0011206114014395293, 'batch_size': 512, 'epochs': 60}. Best is trial 0 with value: 37.414798736572266.\n",
      "[I 2024-06-15 22:39:47,187] Trial 1 finished with value: 5.724008083343506 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'dropout_rate': 0.34452058694178056, 'lr': 0.009613009595144755, 'batch_size': 176, 'epochs': 85}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:51,573] Trial 2 finished with value: 17.228859265645344 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 2, 'dropout_rate': 0.5910484233380484, 'lr': 0.009664074530571373, 'batch_size': 80, 'epochs': 65}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:54,353] Trial 3 finished with value: 184.98648579915366 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 4, 'dropout_rate': 0.5546561867471034, 'lr': 0.0010358790205906787, 'batch_size': 192, 'epochs': 68}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:55,282] Trial 4 finished with value: 20.1932110786438 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 32, 'dropout_rate': 0.24631624475467745, 'lr': 0.003309837691238275, 'batch_size': 128, 'epochs': 125}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:55,775] Trial 5 finished with value: 84.01505661010742 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 16, 'dropout_rate': 0.4501035408169794, 'lr': 0.001979762472222768, 'batch_size': 384, 'epochs': 92}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:56,984] Trial 6 finished with value: 10.42917251586914 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.39724961770740247, 'lr': 0.0011241479226879656, 'batch_size': 288, 'epochs': 79}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:39:59,391] Trial 7 finished with value: 19.806668281555176 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 8, 'dropout_rate': 0.39217628455373166, 'lr': 0.002999180041474151, 'batch_size': 256, 'epochs': 100}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:40:01,270] Trial 8 finished with value: 15.183991432189941 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 4, 'dropout_rate': 0.4139659729869747, 'lr': 0.009274985203897652, 'batch_size': 432, 'epochs': 56}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:40:02,883] Trial 9 finished with value: 12.627592086791992 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'dropout_rate': 0.2592694145377293, 'lr': 0.007029235306779735, 'batch_size': 256, 'epochs': 65}. Best is trial 1 with value: 5.724008083343506.\n",
      "[I 2024-06-15 22:40:09,300] Trial 10 finished with value: 3.4378366117124206 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3062107527664806, 'lr': 0.006600507781004648, 'batch_size': 16, 'epochs': 131}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:15,116] Trial 11 finished with value: 5.4897306671849 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3149760963384204, 'lr': 0.0070638125658616605, 'batch_size': 16, 'epochs': 145}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:23,573] Trial 12 finished with value: 7.962512351848461 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'dropout_rate': 0.30929735951302123, 'lr': 0.006407084533150961, 'batch_size': 16, 'epochs': 149}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:29,769] Trial 13 finished with value: 3.9191423345495155 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'dropout_rate': 0.2881943415510473, 'lr': 0.00761288604383308, 'batch_size': 16, 'epochs': 127}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:33,997] Trial 14 finished with value: 4.987898031870524 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'dropout_rate': 0.21876514651001203, 'lr': 0.005336176026197131, 'batch_size': 80, 'epochs': 121}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:37,089] Trial 15 finished with value: 12.49180825551351 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 2, 'dropout_rate': 0.29149403875469077, 'lr': 0.008316440852378624, 'batch_size': 80, 'epochs': 125}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:44,304] Trial 16 finished with value: 8.281798592320195 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'dropout_rate': 0.500031419783664, 'lr': 0.005008864304746271, 'batch_size': 16, 'epochs': 115}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:46,865] Trial 17 finished with value: 3.556566834449768 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'dropout_rate': 0.2613557512165231, 'lr': 0.008263588592009569, 'batch_size': 128, 'epochs': 136}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:48,856] Trial 18 finished with value: 3.598872741063436 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'dropout_rate': 0.21305883774848478, 'lr': 0.008407031035370247, 'batch_size': 160, 'epochs': 138}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:40:49,582] Trial 19 finished with value: 12.008795261383057 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'dropout_rate': 0.34787215580477227, 'lr': 0.004584828632074413, 'batch_size': 128, 'epochs': 108}. Best is trial 10 with value: 3.4378366117124206.\n",
      "[I 2024-06-15 22:41:01,375] A new study created in memory with name: no-name-7f64e2ff-bacb-4fde-bf6e-14c6cc936056\n",
      "[I 2024-06-15 22:41:05,222] Trial 0 finished with value: 20.513306427001954 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 2, 'dropout_rate': 0.3132599618258491, 'lr': 0.006668502142825391, 'batch_size': 96, 'epochs': 68}. Best is trial 0 with value: 20.513306427001954.\n",
      "[I 2024-06-15 22:41:07,338] Trial 1 finished with value: 32.57380390167236 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 8, 'dropout_rate': 0.556942216934319, 'lr': 0.005242647620654383, 'batch_size': 336, 'epochs': 62}. Best is trial 0 with value: 20.513306427001954.\n",
      "[I 2024-06-15 22:41:07,853] Trial 2 finished with value: 45.09322738647461 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 16, 'dropout_rate': 0.3682318133442898, 'lr': 0.0020983718394304712, 'batch_size': 144, 'epochs': 115}. Best is trial 0 with value: 20.513306427001954.\n",
      "[I 2024-06-15 22:41:13,579] Trial 3 finished with value: 2.473806619644165 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'dropout_rate': 0.22843747499759406, 'lr': 0.006865189268587252, 'batch_size': 32, 'epochs': 131}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:15,528] Trial 4 finished with value: 17.249738693237305 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'dropout_rate': 0.5166376843797196, 'lr': 0.001130133632981486, 'batch_size': 432, 'epochs': 125}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:18,124] Trial 5 finished with value: 27.32569408416748 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 4, 'dropout_rate': 0.32259785065086866, 'lr': 0.005936672230118136, 'batch_size': 304, 'epochs': 150}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:20,318] Trial 6 finished with value: 9.75166552407401 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 16, 'dropout_rate': 0.33773070603903754, 'lr': 0.00489724693699989, 'batch_size': 64, 'epochs': 64}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:22,871] Trial 7 finished with value: 8.675775170326233 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'dropout_rate': 0.5212025941634076, 'lr': 0.004236522651986017, 'batch_size': 128, 'epochs': 128}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:23,829] Trial 8 finished with value: 13.884011268615723 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 32, 'dropout_rate': 0.32643814836316887, 'lr': 0.009480929163092948, 'batch_size': 368, 'epochs': 127}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:24,985] Trial 9 finished with value: 13.061432838439941 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 32, 'dropout_rate': 0.44016218984358646, 'lr': 0.008142468776794694, 'batch_size': 464, 'epochs': 144}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:27,813] Trial 10 finished with value: 7.9648754596710205 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'dropout_rate': 0.22875633080321248, 'lr': 0.007892439848704743, 'batch_size': 224, 'epochs': 92}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:28,235] Trial 11 finished with value: 16.24225902557373 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'dropout_rate': 0.20554648643119755, 'lr': 0.00769124104528628, 'batch_size': 224, 'epochs': 94}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:34,038] Trial 12 finished with value: 4.786618285708958 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'dropout_rate': 0.2017554454389357, 'lr': 0.009531397149138636, 'batch_size': 16, 'epochs': 87}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:38,306] Trial 13 finished with value: 4.377970269748142 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'dropout_rate': 0.2590105152940902, 'lr': 0.009668194665552507, 'batch_size': 32, 'epochs': 78}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:46,252] Trial 14 finished with value: 4.9178066960087525 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'dropout_rate': 0.26923566930238607, 'lr': 0.0036204374806901177, 'batch_size': 16, 'epochs': 80}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:46,728] Trial 15 finished with value: 37.55113983154297 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'dropout_rate': 0.42905571821155397, 'lr': 0.008915619158536503, 'batch_size': 176, 'epochs': 107}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:47,791] Trial 16 finished with value: 8.84144041273329 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'dropout_rate': 0.2695346196120557, 'lr': 0.006667278171561768, 'batch_size': 48, 'epochs': 77}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:51,385] Trial 17 finished with value: 6.40696496963501 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.2693484842928743, 'lr': 0.009926563556212594, 'batch_size': 96, 'epochs': 105}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:53,120] Trial 18 finished with value: 7.090392589569092 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'dropout_rate': 0.40127920897508984, 'lr': 0.006415881723487719, 'batch_size': 192, 'epochs': 53}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:41:53,765] Trial 19 finished with value: 19.87533187866211 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 2, 'dropout_rate': 0.25029796566833623, 'lr': 0.007427212103579916, 'batch_size': 256, 'epochs': 137}. Best is trial 3 with value: 2.473806619644165.\n",
      "[I 2024-06-15 22:42:04,375] A new study created in memory with name: no-name-e5db320c-6f77-42ca-802a-537c61726a61\n",
      "[I 2024-06-15 22:42:05,561] Trial 0 finished with value: 8.299327850341797 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'dropout_rate': 0.5902915429095468, 'lr': 0.00919954444980962, 'batch_size': 240, 'epochs': 130}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:08,347] Trial 1 finished with value: 13.094978094100952 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 2, 'dropout_rate': 0.30350046878113324, 'lr': 0.003384263078683025, 'batch_size': 112, 'epochs': 76}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:10,712] Trial 2 finished with value: 28.39019857134138 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'dropout_rate': 0.5779012677032227, 'lr': 0.001402037058773198, 'batch_size': 64, 'epochs': 89}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:12,389] Trial 3 finished with value: 10.665233612060547 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 8, 'dropout_rate': 0.2241450779226109, 'lr': 0.008618122257049811, 'batch_size': 464, 'epochs': 56}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:14,923] Trial 4 finished with value: 17.72175359725952 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 2, 'dropout_rate': 0.32709440063028483, 'lr': 0.009888401389106485, 'batch_size': 416, 'epochs': 78}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:16,520] Trial 5 finished with value: 12.524508476257324 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 32, 'dropout_rate': 0.266596841385783, 'lr': 0.0065069656267553126, 'batch_size': 352, 'epochs': 76}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:19,514] Trial 6 finished with value: 20.484703063964844 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 4, 'dropout_rate': 0.43360626190152096, 'lr': 0.005904767448486295, 'batch_size': 480, 'epochs': 103}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:20,911] Trial 7 finished with value: 19.079074541727703 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 8, 'dropout_rate': 0.37016557024525, 'lr': 0.007646101332448185, 'batch_size': 208, 'epochs': 76}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:28,108] Trial 8 finished with value: 11.664181518554688 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 2, 'dropout_rate': 0.20809097971657936, 'lr': 0.004406414968173632, 'batch_size': 96, 'epochs': 127}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:32,686] Trial 9 finished with value: 15.495885848999023 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 4, 'dropout_rate': 0.4627526818327557, 'lr': 0.008768771494227182, 'batch_size': 224, 'epochs': 140}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:33,770] Trial 10 finished with value: 10.611281871795654 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.5943183797461846, 'lr': 0.009787058938155895, 'batch_size': 320, 'epochs': 117}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:34,896] Trial 11 finished with value: 10.648294925689697 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.5989005002301167, 'lr': 0.009821677419332634, 'batch_size': 304, 'epochs': 119}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:36,501] Trial 12 finished with value: 9.754173755645752 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.5257351029263065, 'lr': 0.0074679161526001216, 'batch_size': 288, 'epochs': 146}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:37,999] Trial 13 finished with value: 10.703173637390137 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'dropout_rate': 0.5120016715753211, 'lr': 0.007359814739446984, 'batch_size': 192, 'epochs': 150}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:39,012] Trial 14 finished with value: 12.657783508300781 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'dropout_rate': 0.5171027430966968, 'lr': 0.007917299314389998, 'batch_size': 384, 'epochs': 136}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:40,368] Trial 15 finished with value: 18.480990409851074 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 16, 'dropout_rate': 0.5264306383266333, 'lr': 0.004808651073219412, 'batch_size': 272, 'epochs': 150}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:41,267] Trial 16 finished with value: 11.119135538736979 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'dropout_rate': 0.4627022477212559, 'lr': 0.006613283456992852, 'batch_size': 160, 'epochs': 105}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:46,195] Trial 17 finished with value: 9.793187777201334 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 2, 'dropout_rate': 0.5487641339351123, 'lr': 0.008657134563481727, 'batch_size': 16, 'epochs': 133}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:47,573] Trial 18 finished with value: 33.91616439819336 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 2, 'dropout_rate': 0.4687135099051818, 'lr': 0.006771557272439781, 'batch_size': 272, 'epochs': 120}. Best is trial 0 with value: 8.299327850341797.\n",
      "[I 2024-06-15 22:42:48,167] Trial 19 finished with value: 26.591049830118816 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'dropout_rate': 0.385136403078529, 'lr': 0.0029333206206691572, 'batch_size': 160, 'epochs': 141}. Best is trial 0 with value: 8.299327850341797.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=448):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_64:\n",
    "    print('Starting MLP reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/64_MLP.pth', encoders, 64)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization 448AP - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [32, 16, 8, 4, 2]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [448] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [32, 16, 8, 4, 2]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_128 = True\n",
    "SEARCH_KAN_REDUCED_64 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-15 22:42:48,727] A new study created in memory with name: no-name-fe008b7f-f8be-4bab-99b5-4f4f35863b7f\n",
      "[I 2024-06-15 22:42:56,136] Trial 0 finished with value: 1.6804826259613037 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 8, 'lr': 0.0029974385320055232, 'batch_size': 336, 'epochs': 122}. Best is trial 0 with value: 1.6804826259613037.\n",
      "[I 2024-06-15 22:42:58,608] Trial 1 finished with value: 13.584057331085205 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 8, 'lr': 0.008653567797090464, 'batch_size': 352, 'epochs': 109}. Best is trial 0 with value: 1.6804826259613037.\n",
      "[I 2024-06-15 22:43:08,458] Trial 2 finished with value: 6.466972282954624 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 2, 'lr': 0.008182622943197545, 'batch_size': 64, 'epochs': 101}. Best is trial 0 with value: 1.6804826259613037.\n",
      "[I 2024-06-15 22:43:21,161] Trial 3 finished with value: 0.8365708738565445 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'lr': 0.006187726176074789, 'batch_size': 80, 'epochs': 116}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:43:29,113] Trial 4 finished with value: 6.3921332359313965 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 32, 'lr': 0.0069207015678382135, 'batch_size': 480, 'epochs': 75}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:43:39,443] Trial 5 finished with value: 6.5682932535807295 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 2, 'lr': 0.007476491611991572, 'batch_size': 176, 'epochs': 114}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:43:42,133] Trial 6 finished with value: 14.412921905517578 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 4, 'lr': 0.0037782401316839584, 'batch_size': 512, 'epochs': 85}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:43:48,194] Trial 7 finished with value: 1.8793349862098694 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 4, 'lr': 0.0070048008872899995, 'batch_size': 336, 'epochs': 150}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:43:53,412] Trial 8 finished with value: 2.2306360602378845 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 16, 'lr': 0.003259005980639461, 'batch_size': 384, 'epochs': 55}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:44:08,676] Trial 9 finished with value: 1.491854075874601 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'lr': 0.007190380790542903, 'batch_size': 32, 'epochs': 135}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:44:18,891] Trial 10 finished with value: 1.459343433380127 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.0011907195303509357, 'batch_size': 160, 'epochs': 84}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:44:27,808] Trial 11 finished with value: 1.1714340448379517 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.005128399939750571, 'batch_size': 160, 'epochs': 85}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:44:36,224] Trial 12 finished with value: 1.4084491332372029 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.005102049552243921, 'batch_size': 160, 'epochs': 69}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:44:48,588] Trial 13 finished with value: 1.0999597430229187 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.0053866547860211885, 'batch_size': 96, 'epochs': 94}. Best is trial 3 with value: 0.8365708738565445.\n",
      "[I 2024-06-15 22:45:07,006] Trial 14 finished with value: 0.812299425403277 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'lr': 0.0058088643408263346, 'batch_size': 80, 'epochs': 129}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:45:12,048] Trial 15 finished with value: 1.867196261882782 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'lr': 0.009399527200392003, 'batch_size': 240, 'epochs': 131}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:45:24,816] Trial 16 finished with value: 0.873360788822174 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'lr': 0.005900526200258946, 'batch_size': 96, 'epochs': 149}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:45:41,605] Trial 17 finished with value: 1.1576205668626007 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'lr': 0.004297858479339803, 'batch_size': 16, 'epochs': 131}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:45:48,385] Trial 18 finished with value: 1.7510938048362732 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'lr': 0.00637878906464407, 'batch_size': 240, 'epochs': 116}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:46:05,012] Trial 19 finished with value: 0.9190530776977539 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 4, 'lr': 0.0020359851250521233, 'batch_size': 96, 'epochs': 139}. Best is trial 14 with value: 0.812299425403277.\n",
      "[I 2024-06-15 22:46:23,700] A new study created in memory with name: no-name-7e120370-8cc9-4522-8408-ec7736de70f9\n",
      "[I 2024-06-15 22:46:27,769] Trial 0 finished with value: 2.0446261167526245 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'lr': 0.0010768951673432714, 'batch_size': 288, 'epochs': 149}. Best is trial 0 with value: 2.0446261167526245.\n",
      "[I 2024-06-15 22:46:33,148] Trial 1 finished with value: 1.7879379590352376 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 16, 'lr': 0.00651797473836425, 'batch_size': 160, 'epochs': 116}. Best is trial 1 with value: 1.7879379590352376.\n",
      "[I 2024-06-15 22:46:37,641] Trial 2 finished with value: 1.65972505013148 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'lr': 0.002558212670470653, 'batch_size': 80, 'epochs': 130}. Best is trial 2 with value: 1.65972505013148.\n",
      "[I 2024-06-15 22:46:46,012] Trial 3 finished with value: 4.551405151685079 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 8, 'lr': 0.006920623202155617, 'batch_size': 80, 'epochs': 63}. Best is trial 2 with value: 1.65972505013148.\n",
      "[I 2024-06-15 22:46:49,420] Trial 4 finished with value: 1.6389735341072083 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 32, 'lr': 0.005954653912427824, 'batch_size': 416, 'epochs': 53}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:46:54,950] Trial 5 finished with value: 6.5990471839904785 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 4, 'lr': 0.003394180794582595, 'batch_size': 288, 'epochs': 80}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:46:58,721] Trial 6 finished with value: 2.479716181755066 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 4, 'lr': 0.0016135782192311376, 'batch_size': 304, 'epochs': 57}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:01,124] Trial 7 finished with value: 2.3197138706843057 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'lr': 0.00853188283187858, 'batch_size': 192, 'epochs': 140}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:14,434] Trial 8 finished with value: 1.7825130564825875 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'lr': 0.004717722418134235, 'batch_size': 64, 'epochs': 144}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:18,014] Trial 9 finished with value: 2.1645212173461914 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 4, 'lr': 0.007833600325046305, 'batch_size': 480, 'epochs': 128}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:23,859] Trial 10 finished with value: 1.8576066493988037 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 32, 'lr': 0.009459117586483674, 'batch_size': 480, 'epochs': 91}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:29,303] Trial 11 finished with value: 1.9888465404510498 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 32, 'lr': 0.004473367743183393, 'batch_size': 416, 'epochs': 107}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:31,695] Trial 12 finished with value: 7.09946608543396 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 4, 'lr': 0.003174836802614196, 'batch_size': 368, 'epochs': 82}. Best is trial 4 with value: 1.6389735341072083.\n",
      "[I 2024-06-15 22:47:45,326] Trial 13 finished with value: 1.626584483517541 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'lr': 0.005501319862227131, 'batch_size': 16, 'epochs': 115}. Best is trial 13 with value: 1.626584483517541.\n",
      "[I 2024-06-15 22:48:15,919] Trial 14 finished with value: 1.3288611317122425 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 8, 'lr': 0.005712550240630544, 'batch_size': 16, 'epochs': 106}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:48:29,073] Trial 15 finished with value: 1.758258748937536 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'lr': 0.004969960915061907, 'batch_size': 16, 'epochs': 108}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:48:36,572] Trial 16 finished with value: 1.7204876740773518 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 8, 'lr': 0.007369275341976604, 'batch_size': 176, 'epochs': 118}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:48:54,296] Trial 17 finished with value: 2.165675648936519 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 8, 'lr': 0.005680468269519729, 'batch_size': 16, 'epochs': 96}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:48:58,297] Trial 18 finished with value: 1.8271227180957794 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'lr': 0.004107112179466677, 'batch_size': 128, 'epochs': 122}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:49:07,037] Trial 19 finished with value: 13.88857889175415 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 2, 'lr': 0.0059600703328278, 'batch_size': 240, 'epochs': 105}. Best is trial 14 with value: 1.3288611317122425.\n",
      "[I 2024-06-15 22:49:33,885] A new study created in memory with name: no-name-eb29cddf-3372-407e-95f7-f3e003097a65\n",
      "[I 2024-06-15 22:49:35,482] Trial 0 finished with value: 6.447315692901611 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 16, 'lr': 0.0039927319721854305, 'batch_size': 304, 'epochs': 61}. Best is trial 0 with value: 6.447315692901611.\n",
      "[I 2024-06-15 22:49:40,686] Trial 1 finished with value: 2.1932936906814575 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 8, 'lr': 0.004066369732068548, 'batch_size': 320, 'epochs': 127}. Best is trial 1 with value: 2.1932936906814575.\n",
      "[I 2024-06-15 22:49:46,633] Trial 2 finished with value: 2.1073739528656006 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 8, 'lr': 0.006460773933819305, 'batch_size': 448, 'epochs': 132}. Best is trial 2 with value: 2.1073739528656006.\n",
      "[I 2024-06-15 22:49:52,226] Trial 3 finished with value: 6.502181053161621 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 8, 'hidden_layer_size_2': 2, 'lr': 0.009902913892837643, 'batch_size': 240, 'epochs': 83}. Best is trial 2 with value: 2.1073739528656006.\n",
      "[I 2024-06-15 22:49:56,245] Trial 4 finished with value: 2.2758355140686035 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 8, 'lr': 0.004703977651662098, 'batch_size': 240, 'epochs': 70}. Best is trial 2 with value: 2.1073739528656006.\n",
      "[I 2024-06-15 22:49:58,976] Trial 5 finished with value: 11.071978569030762 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 4, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 8, 'lr': 0.009004014889643606, 'batch_size': 432, 'epochs': 128}. Best is trial 2 with value: 2.1073739528656006.\n",
      "[I 2024-06-15 22:50:02,093] Trial 6 finished with value: 1.9567725658416748 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'lr': 0.006114300718431978, 'batch_size': 512, 'epochs': 100}. Best is trial 6 with value: 1.9567725658416748.\n",
      "[I 2024-06-15 22:50:24,723] Trial 7 finished with value: 1.657778466189349 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'lr': 0.00261890399076408, 'batch_size': 16, 'epochs': 54}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:50:27,793] Trial 8 finished with value: 1.6731413006782532 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 32, 'lr': 0.00477125398068089, 'batch_size': 368, 'epochs': 97}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:50:32,357] Trial 9 finished with value: 1.9485237002372742 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 4, 'hidden_layer_size_2': 2, 'hidden_layer_size_3': 32, 'lr': 0.006577261045261097, 'batch_size': 128, 'epochs': 148}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:50:40,723] Trial 10 finished with value: 1.8775694029671806 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 4, 'lr': 0.0013285790169377193, 'batch_size': 64, 'epochs': 50}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:50:55,484] Trial 11 finished with value: 1.6659356620576646 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'lr': 0.001852226961916338, 'batch_size': 48, 'epochs': 97}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:51:08,957] Trial 12 finished with value: 1.7400200622422355 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 16, 'lr': 0.001411089763341949, 'batch_size': 32, 'epochs': 79}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:51:12,989] Trial 13 finished with value: 2.0029991070429483 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'lr': 0.0028814067990760297, 'batch_size': 144, 'epochs': 111}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:51:17,944] Trial 14 finished with value: 2.4225286841392517 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 2, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 32, 'lr': 0.002567626590786178, 'batch_size': 128, 'epochs': 50}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:51:42,154] Trial 15 finished with value: 2.0443154838350086 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 2, 'hidden_layer_size_2': 8, 'lr': 0.0024093791057327217, 'batch_size': 16, 'epochs': 87}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:51:55,135] Trial 16 finished with value: 1.769961178302765 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 4, 'hidden_layer_size_3': 4, 'lr': 0.0010205646784058175, 'batch_size': 80, 'epochs': 112}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:52:00,101] Trial 17 finished with value: 1.8971312443415325 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.0030682956959283774, 'batch_size': 192, 'epochs': 66}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:52:06,265] Trial 18 finished with value: 1.7040223081906636 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'lr': 0.008189200667493103, 'batch_size': 80, 'epochs': 96}. Best is trial 7 with value: 1.657778466189349.\n",
      "[I 2024-06-15 22:52:13,859] Trial 19 finished with value: 2.4547027746836343 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 8, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 8, 'hidden_layer_size_3': 2, 'lr': 0.00358091800868648, 'batch_size': 176, 'epochs': 110}. Best is trial 7 with value: 1.657778466189349.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 448, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_64:\n",
    "    print('Starting KAN reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 64, f'./models/KAN/64_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization 448AP - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth',\n",
    "        '64': './models/MLP/64_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth',\n",
    "        '64': './models/KAN/64_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '128': ['./models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth'],\n",
    "        '64': ['./models/MLP/64_encoder_256.pth', './models/MLP/64_encoder_128.pth', './models/MLP/64_encoder_64.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '128': ['./models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth'],\n",
    "        '64': ['./models/KAN/64_encoder_256.pth', './models/KAN/64_encoder_128.pth', './models/KAN/64_encoder_64.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 64, the SAE will have 256 -> 128 -> 64\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 448\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 256 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [8, 32], 0.3062107527664806, 448)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [32, 16], 0.22843747499759406, 128)\n",
    "    mlp_64 = load_MLP_model(model_paths['MLP']['64'], [16, 16], 0.5902915429095468, 64)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [32, 16, 32], 448)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [16, 8, 32, 8], 128)\n",
    "    kan_64 = load_KAN_model(model_paths['KAN']['64'], [16, 16, 32, 32], 64)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    mlp_SAE_64 = load_SAE(SAE_paths['MLP']['64'], 64)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    kan_SAE_64 = load_SAE(SAE_paths['KAN']['64'], 64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy() # Predictions from the model (on CPU) - we need to move them to CPU to use numpy\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1)) # Euclidean distance between predictions and ground-truth\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    mlp_test_data_encoded_64 = stacked_encode_data(X_test_tensor, mlp_SAE_64)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    kan_test_data_encoded_64 = stacked_encode_data(X_test_tensor, kan_SAE_64)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    mlp_64_distances = evaluate_model(mlp_64, mlp_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    kan_64_distances = evaluate_model(kan_64, kan_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    print(mlp_64_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_128_distances.shape)\n",
    "    print(kan_64_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Statistics for  MLP_64\n",
      "MLP_64: 25th percentile: 5.25 - 50th percentile: 7.65 - 75th percentile: 9.53 - Outliers: 3 (0.02%)\n",
      "Max distance: 16.67\n",
      "====================================\n",
      "Statistics for  MLP_128\n",
      "MLP_128: 25th percentile: 1.41 - 50th percentile: 2.14 - 75th percentile: 2.99 - Outliers: 174 (1.34%)\n",
      "Max distance: 7.67\n",
      "====================================\n",
      "Statistics for  MLP_full\n",
      "MLP_full: 25th percentile: 1.72 - 50th percentile: 2.39 - 75th percentile: 3.11 - Outliers: 388 (2.99%)\n",
      "Max distance: 7.57\n",
      "====================================\n",
      "Statistics for  KAN_64\n",
      "KAN_64: 25th percentile: 0.99 - 50th percentile: 1.55 - 75th percentile: 2.51 - Outliers: 608 (4.69%)\n",
      "Max distance: 9.67\n",
      "====================================\n",
      "Statistics for  KAN_128\n",
      "KAN_128: 25th percentile: 0.97 - 50th percentile: 1.48 - 75th percentile: 2.53 - Outliers: 665 (5.13%)\n",
      "Max distance: 9.90\n",
      "====================================\n",
      "Statistics for  KAN_full\n",
      "KAN_full: 25th percentile: 0.82 - 50th percentile: 1.31 - 75th percentile: 2.00 - Outliers: 903 (6.97%)\n",
      "Max distance: 10.21\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_64': mlp_64_distances,\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_64': kan_64_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[1].set_title('Reduced models (128)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_64'], data['KAN_64']], labels=['MLP_64', 'KAN_64'])\n",
    "    axes[2].set_title('Reduced models (64)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print model statistics: 25th, 50th (median), 75th percentiles, and the amount or percentage of outliers\n",
    "    for key in data:\n",
    "        print(\"====================================\")\n",
    "        print(\"Statistics for \", key)\n",
    "        distances = data[key]\n",
    "        q25, q50, q75 = np.percentile(distances, [25, 50, 75])\n",
    "        iqr = q75 - q25\n",
    "        lower_bound = q25 - 1.5 * iqr\n",
    "        upper_bound = q75 + 1.5 * iqr\n",
    "        outliers = np.sum((distances < lower_bound) | (distances > upper_bound))\n",
    "        print(f'{key}: 25th percentile: {q25:.2f} - 50th percentile: {q50:.2f} - 75th percentile: {q75:.2f} - Outliers: {outliers} ({outliers/len(distances)*100:.2f}%)')\n",
    "        \n",
    "        # Print max predicted distance\n",
    "        max_distance = np.max(distances)\n",
    "        print(f'Max distance: {max_distance:.2f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
