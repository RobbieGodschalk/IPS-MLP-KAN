{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "@DeprecationWarning\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# *Load and pre-process the data\\\n",
    "@DeprecationWarning\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    pass\n",
    "    # # Since the csv files do not have column names, we define these first.\n",
    "    # list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # # Load the test data from all specified test sets.  \n",
    "    # df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    # df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # # Get all x,y,floor labels\n",
    "    # df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    # df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # # Add the labels to the pre-processed data\n",
    "    # df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # # Filter the data to only include the specified floor\n",
    "    # df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # # Pre-processing of the training data\n",
    "    # df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    # return df_test\n",
    "\n",
    "\n",
    "# Let's do it differently, have two separate functions\n",
    "# 1. Load a dataset based on a list of full paths, and return a df_{type}_x and df_{type}_y\n",
    "# 2. Preprocess the dataset based on the df_{type}_x and df_{type}_y\n",
    "\n",
    "def load_dataset(paths: list[str], num_APs: int, floor: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "    \n",
    "    # Load the data from all specified paths\n",
    "    df_x = pd.concat([pd.read_csv(path + 'rss.csv', names=list_of_APs) for path in paths])\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_y = pd.concat([pd.read_csv(path + 'crd.csv', names=['x', 'y', 'floor']) for path in paths])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    # Get indexes of the specified floor\n",
    "    floor_indexes = df_y[df_y['floor'] == floor].index\n",
    "    \n",
    "    # Keep only the rows with the specified floor for both x and y, and reset the indexes\n",
    "    df_x = df_x.loc[floor_indexes]\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # For df_y, also remove the floor column\n",
    "    df_y = df_y.loc[floor_indexes]\n",
    "    df_y = df_y.drop(columns=['floor'])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    return df_x, df_y\n",
    "\n",
    "def preprocess_rssi(df_rssi: pd.DataFrame, scaling_strategy: Scaling):\n",
    "    # Flattened dataset for easy searching\n",
    "    flattened = df_rssi.values.flatten()\n",
    "    \n",
    "    # Minimum rssi found\n",
    "    min_rssi = np.min(flattened)\n",
    "    print(\"Minimum RSSI: \", min_rssi)\n",
    "    \n",
    "    # Find biggest multiple of 10 smaller than min_rssi\n",
    "    replacement_rssi = np.floor((min_rssi - 1) / 10) * 10\n",
    "    print(\"Replacement value\", replacement_rssi)\n",
    "    \n",
    "    # Replace all 100 values with replacement_rssi\n",
    "    df_rssi = df_rssi.replace(100, replacement_rssi)\n",
    "    flattened = df_rssi.values.flatten() # Update flattened since we changed the dataframe\n",
    "    \n",
    "    # Standardization part\n",
    "    if scaling_strategy == Scaling.INDEPENDENT: # Might not work\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(df_rssi)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=df_rssi.columns)\n",
    "        df_rssi = df_scaled_rss\n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        df_rssi = (df_rssi - global_mean) / global_std\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df_rssi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x: (4320, 448)\n",
      "df_y: (4320, 2)\n",
      "df_train_x: (3888, 448)\n",
      "df_train_y: (3888, 2)\n",
      "df_val_x: (432, 448)\n",
      "df_val_y: (432, 2)\n",
      "      AP0  AP1  AP2  AP3  AP4  AP5  AP6  AP7  AP8  AP9  ...  AP438  AP439  \\\n",
      "4053  100  100  100  100  100  -85  -61  -86  -60  100  ...    100    100   \n",
      "963   100  -78  -68  100  100  100  -66  100  -80  -93  ...    100    100   \n",
      "4039  100  100  100  100  100  100  -47  100  -43  -80  ...    100    100   \n",
      "3523  100  100  100  100  100  100  -81  100  -85  100  ...    100    100   \n",
      "1766  100  -77  -84  100  100  100  -81  100  -75  -86  ...    100    100   \n",
      "\n",
      "      AP440  AP441  AP442  AP443  AP444  AP445  AP446  AP447  \n",
      "4053    100    100    100    100    100    100    100    100  \n",
      "963     100    100    100    100    100    100    100    100  \n",
      "4039    100    100    100    100    100    100    100    100  \n",
      "3523    100    100    100    100    100    100    100    100  \n",
      "1766    100    100    100    100    100    100    100    100  \n",
      "\n",
      "[5 rows x 448 columns]\n",
      "              x          y\n",
      "4053  12.904458  27.419428\n",
      "963    8.514918  20.267011\n",
      "4039   8.514918  29.207532\n",
      "3523   4.125378  23.843220\n",
      "1766  12.904458  25.631324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_paths = [\n",
    "    './data/V1.0/01/trn01',\n",
    "    './data/V1.0/02/trn01',\n",
    "    './data/V1.0/03/trn01', \n",
    "    './data/V1.0/04/trn01',\n",
    "    './data/V1.0/05/trn01',\n",
    "    './data/V1.0/06/trn01',\n",
    "    './data/V1.0/07/trn01',\n",
    "    './data/V1.0/08/trn01',\n",
    "    './data/V1.0/09/trn01',\n",
    "    './data/V1.0/10/trn01',\n",
    "    './data/V1.0/11/trn01',\n",
    "    './data/V1.0/12/trn01',\n",
    "    './data/V1.0/13/trn01',\n",
    "    './data/V1.0/14/trn01',\n",
    "    './data/V1.0/15/trn01',\n",
    "]\n",
    "num_APs = 448\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "# Split training data into 10% validation data\n",
    "df_x, df_y = load_dataset(data_paths, num_APs, floor)\n",
    "\n",
    "if DEBUG: print('df_x:', df_x.shape)\n",
    "if DEBUG: print('df_y:', df_y.shape)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n",
    "if DEBUG: print(df_train_x.head())\n",
    "if DEBUG: print(df_train_y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RSSI:  -98\n",
      "Replacement value -100.0\n",
      "Minimum RSSI:  -97\n",
      "Replacement value -100.0\n",
      "6.978871070431797e-17\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train_x = preprocess_rssi(df_train_x, scaling_strategy)\n",
    "df_val_x = preprocess_rssi(df_val_x, scaling_strategy)\n",
    "\n",
    "if DEBUG: print(np.mean(df_train_x.values.flatten()))\n",
    "if DEBUG: print(np.std(df_train_x.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RSSI:  -100\n",
      "Replacement value -110.0\n",
      "df_test_x: (12960, 448)\n",
      "df_test_y: (12960, 2)\n",
      "-6.698835098229902e-16\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    './data/V1.0/01/tst01', './data/V1.0/01/tst02', './data/V1.0/01/tst03',\n",
    "    './data/V1.0/02/tst01', './data/V1.0/02/tst02', './data/V1.0/02/tst03',\n",
    "    './data/V1.0/03/tst01', './data/V1.0/03/tst02', './data/V1.0/03/tst03',\n",
    "    './data/V1.0/04/tst01', './data/V1.0/04/tst02', './data/V1.0/04/tst03',\n",
    "    './data/V1.0/05/tst01', './data/V1.0/05/tst02', './data/V1.0/05/tst03',\n",
    "    './data/V1.0/06/tst01', './data/V1.0/06/tst02', './data/V1.0/06/tst03',\n",
    "    './data/V1.0/07/tst01', './data/V1.0/07/tst02', './data/V1.0/07/tst03',\n",
    "    './data/V1.0/08/tst01', './data/V1.0/08/tst02', './data/V1.0/08/tst03',\n",
    "    './data/V1.0/09/tst01', './data/V1.0/09/tst02', './data/V1.0/09/tst03',\n",
    "    './data/V1.0/10/tst01', './data/V1.0/10/tst02', './data/V1.0/10/tst03',\n",
    "    './data/V1.0/11/tst01', './data/V1.0/11/tst02', './data/V1.0/11/tst03',\n",
    "    './data/V1.0/12/tst01', './data/V1.0/12/tst02', './data/V1.0/12/tst03',\n",
    "    './data/V1.0/13/tst01', './data/V1.0/13/tst02', './data/V1.0/13/tst03',\n",
    "    './data/V1.0/14/tst01', './data/V1.0/14/tst02', './data/V1.0/14/tst03',\n",
    "    './data/V1.0/15/tst01', './data/V1.0/15/tst02', './data/V1.0/15/tst03',\n",
    "]\n",
    "\n",
    "df_test_x, df_test_y = load_dataset(data_paths, num_APs, floor)\n",
    "\n",
    "# Standardize the test data\n",
    "df_test_x = preprocess_rssi(df_test_x, scaling_strategy)\n",
    "\n",
    "if DEBUG: print('df_test_x:', df_test_x.shape)\n",
    "if DEBUG: print('df_test_y:', df_test_y.shape)\n",
    "\n",
    "if DEBUG: print(np.mean(df_test_x.values.flatten()))\n",
    "if DEBUG: print(np.std(df_test_x.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=448):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Update input_dim for next layer\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\IPS-MLP-KAN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "SEARCH_MLP_REDUCED_64 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-16 18:15:50,534] A new study created in memory with name: no-name-0682de5c-6ab6-4f9e-9b3b-c244300ad63c\n",
      "[I 2024-06-16 18:15:59,766] Trial 0 finished with value: 2.174485445022583 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 640, 'dropout_rate': 0.30848093222675216, 'lr': 0.0032402683889786142, 'batch_size': 480, 'epochs': 96}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:07,079] Trial 1 finished with value: 2.9070494174957275 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 576, 'dropout_rate': 0.3413382089889892, 'lr': 0.006422556850056005, 'batch_size': 384, 'epochs': 122}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:09,000] Trial 2 finished with value: 5.358824968338013 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 16, 'dropout_rate': 0.34106213503434424, 'lr': 0.0021924218378869707, 'batch_size': 256, 'epochs': 105}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:09,992] Trial 3 finished with value: 4.680583715438843 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'dropout_rate': 0.3911808094973789, 'lr': 0.0029676472524915163, 'batch_size': 272, 'epochs': 101}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:10,536] Trial 4 finished with value: 51.430973052978516 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 448, 'dropout_rate': 0.5821927755431093, 'lr': 0.001908638970522845, 'batch_size': 144, 'epochs': 77}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:12,152] Trial 5 finished with value: 4.06929349899292 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 448, 'dropout_rate': 0.2821710523620181, 'lr': 0.0020178999473633866, 'batch_size': 160, 'epochs': 116}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:16,044] Trial 6 finished with value: 3.801441788673401 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 448, 'dropout_rate': 0.3306659608765753, 'lr': 0.005024657697474284, 'batch_size': 336, 'epochs': 99}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:17,298] Trial 7 finished with value: 43.0877742767334 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 576, 'dropout_rate': 0.5561564523409511, 'lr': 0.0016362178211021643, 'batch_size': 240, 'epochs': 122}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:19,974] Trial 8 finished with value: 8.112934006585014 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 640, 'dropout_rate': 0.47816622492479743, 'lr': 0.0018453360632800257, 'batch_size': 48, 'epochs': 90}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:20,902] Trial 9 finished with value: 23.598045349121094 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 640, 'dropout_rate': 0.264552221379473, 'lr': 0.0028421093345842195, 'batch_size': 288, 'epochs': 101}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:21,990] Trial 10 finished with value: 24.98554801940918 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 64, 'dropout_rate': 0.2005932602649027, 'lr': 0.00937597904591593, 'batch_size': 496, 'epochs': 150}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:24,421] Trial 11 finished with value: 10.98106575012207 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 576, 'dropout_rate': 0.44899827854901975, 'lr': 0.006964249040359341, 'batch_size': 464, 'epochs': 64}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:26,142] Trial 12 finished with value: 15.832841873168945 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 640, 'dropout_rate': 0.37049430691812707, 'lr': 0.0050436422811671, 'batch_size': 416, 'epochs': 137}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:29,534] Trial 13 finished with value: 4.427920460700989 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 256, 'dropout_rate': 0.2757629330248794, 'lr': 0.006817926614409636, 'batch_size': 384, 'epochs': 124}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:30,401] Trial 14 finished with value: 20.67503547668457 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 32, 'dropout_rate': 0.4786193139018383, 'lr': 0.004404552758939343, 'batch_size': 512, 'epochs': 82}. Best is trial 0 with value: 2.174485445022583.\n",
      "[I 2024-06-16 18:16:37,530] Trial 15 finished with value: 1.184235155582428 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 576, 'dropout_rate': 0.2070868635955763, 'lr': 0.006614806812724865, 'batch_size': 416, 'epochs': 55}. Best is trial 15 with value: 1.184235155582428.\n",
      "[I 2024-06-16 18:16:43,188] Trial 16 finished with value: 1.4507113695144653 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'dropout_rate': 0.2043226416729979, 'lr': 0.008347759500095071, 'batch_size': 448, 'epochs': 59}. Best is trial 15 with value: 1.184235155582428.\n",
      "[I 2024-06-16 18:16:44,731] Trial 17 finished with value: 8.6957426071167 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'dropout_rate': 0.21076096085393264, 'lr': 0.00861077382732741, 'batch_size': 432, 'epochs': 52}. Best is trial 15 with value: 1.184235155582428.\n",
      "[I 2024-06-16 18:16:46,348] Trial 18 finished with value: 6.205065011978149 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 512, 'dropout_rate': 0.23758209841852732, 'lr': 0.008190209907379932, 'batch_size': 352, 'epochs': 56}. Best is trial 15 with value: 1.184235155582428.\n",
      "[I 2024-06-16 18:16:47,455] Trial 19 finished with value: 4.365190744400024 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 448, 'dropout_rate': 0.24126985229969822, 'lr': 0.008296224549705333, 'batch_size': 320, 'epochs': 69}. Best is trial 15 with value: 1.184235155582428.\n",
      "[I 2024-06-16 18:16:59,575] A new study created in memory with name: no-name-724c3504-0d2e-490e-91cf-b3243a7e7fe9\n",
      "[I 2024-06-16 18:17:00,462] Trial 0 finished with value: 14.9361941019694 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 64, 'dropout_rate': 0.3037486705550504, 'lr': 0.0029104789132647895, 'batch_size': 208, 'epochs': 149}. Best is trial 0 with value: 14.9361941019694.\n",
      "[I 2024-06-16 18:17:03,192] Trial 1 finished with value: 13.473776499430338 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'dropout_rate': 0.5433670897318293, 'lr': 0.007847799044929868, 'batch_size': 176, 'epochs': 109}. Best is trial 1 with value: 13.473776499430338.\n",
      "[I 2024-06-16 18:17:05,113] Trial 2 finished with value: 5.779810905456543 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'dropout_rate': 0.46654346225148857, 'lr': 0.0011043288132111689, 'batch_size': 176, 'epochs': 64}. Best is trial 2 with value: 5.779810905456543.\n",
      "[I 2024-06-16 18:17:12,339] Trial 3 finished with value: 2.1954160928726196 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 576, 'dropout_rate': 0.5246554165652277, 'lr': 0.0036829086417717446, 'batch_size': 224, 'epochs': 108}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:14,814] Trial 4 finished with value: 32.47937870025635 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 16, 'dropout_rate': 0.501813890667109, 'lr': 0.005070050577375753, 'batch_size': 320, 'epochs': 86}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:23,991] Trial 5 finished with value: 7.914528012275696 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 640, 'dropout_rate': 0.4856978911523879, 'lr': 0.005373022121659815, 'batch_size': 112, 'epochs': 108}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:24,617] Trial 6 finished with value: 15.296650886535645 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 128, 'dropout_rate': 0.3957356213066478, 'lr': 0.00358869338117386, 'batch_size': 224, 'epochs': 71}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:32,773] Trial 7 finished with value: 17.012049674987793 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'dropout_rate': 0.29279283239072906, 'lr': 0.008682722328613779, 'batch_size': 384, 'epochs': 105}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:37,720] Trial 8 finished with value: 4.303770895357485 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'dropout_rate': 0.2891693303442308, 'lr': 0.005034104199400773, 'batch_size': 16, 'epochs': 100}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:38,724] Trial 9 finished with value: 31.886905670166016 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 32, 'dropout_rate': 0.5237558725977912, 'lr': 0.007140308031303234, 'batch_size': 432, 'epochs': 136}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:40,829] Trial 10 finished with value: 3.5368380546569824 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.5975365693915089, 'lr': 0.0013096249940136744, 'batch_size': 512, 'epochs': 127}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:44,202] Trial 11 finished with value: 3.2494750022888184 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.5898868196251552, 'lr': 0.0010238911649497564, 'batch_size': 496, 'epochs': 128}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:47,495] Trial 12 finished with value: 3.293672561645508 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'dropout_rate': 0.5982135622159248, 'lr': 0.0029375229162705027, 'batch_size': 304, 'epochs': 126}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:48,597] Trial 13 finished with value: 10.148911476135254 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 16, 'dropout_rate': 0.40910006794200693, 'lr': 0.0021686674008791745, 'batch_size': 512, 'epochs': 50}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:50,159] Trial 14 finished with value: 3.7537373304367065 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'dropout_rate': 0.42631900659661465, 'lr': 0.003791929880725011, 'batch_size': 384, 'epochs': 119}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:53,273] Trial 15 finished with value: 3.5586153984069826 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 128, 'dropout_rate': 0.22000663172273063, 'lr': 0.006466724709605749, 'batch_size': 96, 'epochs': 91}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:57,248] Trial 16 finished with value: 3.0180095434188843 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 576, 'dropout_rate': 0.5649631955052673, 'lr': 0.009630385202336158, 'batch_size': 272, 'epochs': 145}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:17:58,405] Trial 17 finished with value: 16.178603172302246 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 576, 'dropout_rate': 0.548390343907549, 'lr': 0.009605618125091918, 'batch_size': 272, 'epochs': 150}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:18:01,030] Trial 18 finished with value: 24.64302158355713 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 16, 'dropout_rate': 0.4501796886709439, 'lr': 0.006068429539783163, 'batch_size': 336, 'epochs': 139}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:18:02,794] Trial 19 finished with value: 6.178981184959412 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 576, 'dropout_rate': 0.3540459162230022, 'lr': 0.009818305943292704, 'batch_size': 112, 'epochs': 86}. Best is trial 3 with value: 2.1954160928726196.\n",
      "[I 2024-06-16 18:18:11,449] A new study created in memory with name: no-name-ec47e53c-f7e0-46b9-bf28-3cfb6511b80b\n",
      "[I 2024-06-16 18:18:12,162] Trial 0 finished with value: 8.128515720367432 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'dropout_rate': 0.2099198501723009, 'lr': 0.007983690656949528, 'batch_size': 352, 'epochs': 92}. Best is trial 0 with value: 8.128515720367432.\n",
      "[I 2024-06-16 18:18:12,732] Trial 1 finished with value: 27.662002563476562 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'dropout_rate': 0.5463836585494524, 'lr': 0.004667634035183761, 'batch_size': 240, 'epochs': 131}. Best is trial 0 with value: 8.128515720367432.\n",
      "[I 2024-06-16 18:18:18,792] Trial 2 finished with value: 16.37103407723563 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 32, 'dropout_rate': 0.5336554639517728, 'lr': 0.008271938663909337, 'batch_size': 64, 'epochs': 119}. Best is trial 0 with value: 8.128515720367432.\n",
      "[I 2024-06-16 18:18:20,039] Trial 3 finished with value: 4.440332492192586 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'dropout_rate': 0.41907641941255036, 'lr': 0.0046176074474823855, 'batch_size': 160, 'epochs': 52}. Best is trial 3 with value: 4.440332492192586.\n",
      "[I 2024-06-16 18:18:21,906] Trial 4 finished with value: 3.986717104911804 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'dropout_rate': 0.5335403638412346, 'lr': 0.007925898336993312, 'batch_size': 128, 'epochs': 56}. Best is trial 4 with value: 3.986717104911804.\n",
      "[I 2024-06-16 18:18:24,894] Trial 5 finished with value: 3.1718111832936606 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'dropout_rate': 0.5150773646033733, 'lr': 0.008232434072541503, 'batch_size': 176, 'epochs': 89}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:30,466] Trial 6 finished with value: 5.426696300506592 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 256, 'dropout_rate': 0.4235464827978257, 'lr': 0.008923073653094967, 'batch_size': 208, 'epochs': 148}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:33,116] Trial 7 finished with value: 7.849365658230251 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'dropout_rate': 0.38395708044789767, 'lr': 0.004178206823040777, 'batch_size': 48, 'epochs': 78}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:35,048] Trial 8 finished with value: 16.542871475219727 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 256, 'dropout_rate': 0.45066190354564145, 'lr': 0.00896993370346575, 'batch_size': 352, 'epochs': 56}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:35,406] Trial 9 finished with value: 19.029335021972656 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'dropout_rate': 0.25304165740508877, 'lr': 0.008095717338838855, 'batch_size': 480, 'epochs': 125}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:36,653] Trial 10 finished with value: 3.5615053176879883 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'dropout_rate': 0.29419986123426806, 'lr': 0.0016582493093605533, 'batch_size': 368, 'epochs': 97}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:38,383] Trial 11 finished with value: 3.5738638639450073 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'dropout_rate': 0.3170074371029927, 'lr': 0.002820416467415999, 'batch_size': 320, 'epochs': 96}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:40,836] Trial 12 finished with value: 3.2274885177612305 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'dropout_rate': 0.5995339604586052, 'lr': 0.001412813997630412, 'batch_size': 464, 'epochs': 78}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:42,661] Trial 13 finished with value: 3.552096128463745 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'dropout_rate': 0.5934607267874351, 'lr': 0.006257084332424239, 'batch_size': 496, 'epochs': 77}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:44,202] Trial 14 finished with value: 7.177785396575928 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'dropout_rate': 0.5966991647322488, 'lr': 0.006181199791997645, 'batch_size': 448, 'epochs': 78}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:44,749] Trial 15 finished with value: 29.479001998901367 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 16, 'dropout_rate': 0.48441046474394656, 'lr': 0.0010159593255830391, 'batch_size': 288, 'epochs': 110}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:45,340] Trial 16 finished with value: 13.607033252716064 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 576, 'dropout_rate': 0.4927942190683867, 'lr': 0.009802706331080113, 'batch_size': 416, 'epochs': 70}. Best is trial 5 with value: 3.1718111832936606.\n",
      "[I 2024-06-16 18:18:56,377] Trial 17 finished with value: 3.16702401638031 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 64, 'dropout_rate': 0.37044336268225275, 'lr': 0.0029727549033427662, 'batch_size': 128, 'epochs': 88}. Best is trial 17 with value: 3.16702401638031.\n",
      "[I 2024-06-16 18:19:04,602] Trial 18 finished with value: 3.200053036212921 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 64, 'dropout_rate': 0.35226413565616027, 'lr': 0.003120878082538545, 'batch_size': 112, 'epochs': 108}. Best is trial 17 with value: 3.16702401638031.\n",
      "[I 2024-06-16 18:19:15,120] Trial 19 finished with value: 5.876731634140015 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 640, 'dropout_rate': 0.357302274745303, 'lr': 0.0068363757191278645, 'batch_size': 16, 'epochs': 88}. Best is trial 17 with value: 3.16702401638031.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=448):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_64:\n",
    "    print('Starting MLP reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/64_MLP.pth', encoders, 64)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization 448AP fixed standardization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [448] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_128 = True\n",
    "SEARCH_KAN_REDUCED_64 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-16 18:19:25,872] A new study created in memory with name: no-name-5c0d8a30-ca1d-4663-8c3f-e8bf5a05a1c9\n",
      "[I 2024-06-16 18:20:20,247] Trial 0 finished with value: 1.8104567527770996 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 640, 'lr': 0.009654011384132813, 'batch_size': 192, 'epochs': 133}. Best is trial 0 with value: 1.8104567527770996.\n",
      "[I 2024-06-16 18:20:28,963] Trial 1 finished with value: 1.5299582481384277 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 64, 'lr': 0.0024191479211101226, 'batch_size': 128, 'epochs': 126}. Best is trial 1 with value: 1.5299582481384277.\n",
      "[I 2024-06-16 18:20:38,997] Trial 2 finished with value: 1.103310775756836 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 640, 'lr': 0.0026536691005005345, 'batch_size': 96, 'epochs': 132}. Best is trial 2 with value: 1.103310775756836.\n",
      "[I 2024-06-16 18:21:31,286] Trial 3 finished with value: 0.7081619799137115 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 448, 'lr': 0.0010723562104427505, 'batch_size': 368, 'epochs': 147}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:22:20,689] Trial 4 finished with value: 1.756373405456543 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 640, 'lr': 0.001707362642087162, 'batch_size': 480, 'epochs': 90}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:22:50,494] Trial 5 finished with value: 1.1134046614170074 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'lr': 0.009889422144853565, 'batch_size': 336, 'epochs': 136}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:24:24,576] Trial 6 finished with value: 0.9926500618457794 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 256, 'lr': 0.006693772378031352, 'batch_size': 320, 'epochs': 83}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:24:37,307] Trial 7 finished with value: 1.6412978172302246 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 640, 'lr': 0.00867391754897619, 'batch_size': 512, 'epochs': 76}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:25:13,562] Trial 8 finished with value: 1.752303123474121 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 512, 'lr': 0.007285737386840552, 'batch_size': 512, 'epochs': 86}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:25:24,616] Trial 9 finished with value: 0.9431795179843903 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 128, 'lr': 0.007646244720954341, 'batch_size': 240, 'epochs': 69}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:25:56,354] Trial 10 finished with value: 1.4814549088478088 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 448, 'lr': 0.004330870614630191, 'batch_size': 384, 'epochs': 113}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:26:25,015] Trial 11 finished with value: 1.2808354496955872 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 128, 'lr': 0.004925386221637261, 'batch_size': 240, 'epochs': 51}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:27:13,926] Trial 12 finished with value: 1.4796091318130493 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 448, 'lr': 0.006400159519157896, 'batch_size': 400, 'epochs': 150}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:27:38,647] Trial 13 finished with value: 1.5502728819847107 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'lr': 0.0038584447979695758, 'batch_size': 256, 'epochs': 56}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:28:10,226] Trial 14 finished with value: 0.9745950500170389 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 576, 'hidden_layer_size_3': 128, 'lr': 0.007828659287118414, 'batch_size': 160, 'epochs': 104}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:28:14,394] Trial 15 finished with value: 1.667453646659851 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 128, 'lr': 0.001053198831641607, 'batch_size': 416, 'epochs': 65}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:28:56,156] Trial 16 finished with value: 0.9637993403843471 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'lr': 0.0060654075510114865, 'batch_size': 32, 'epochs': 115}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:29:42,079] Trial 17 finished with value: 0.9337918162345886 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 64, 'lr': 0.008298387272678846, 'batch_size': 272, 'epochs': 99}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:30:16,370] Trial 18 finished with value: 1.6347548961639404 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 64, 'lr': 0.0033070742831515084, 'batch_size': 320, 'epochs': 102}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:30:44,197] Trial 19 finished with value: 1.7569893598556519 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 448, 'lr': 0.0054633587814698365, 'batch_size': 448, 'epochs': 150}. Best is trial 3 with value: 0.7081619799137115.\n",
      "[I 2024-06-16 18:31:36,672] A new study created in memory with name: no-name-7c4d778d-695d-4e50-aef0-01ec137d7efb\n",
      "[I 2024-06-16 18:31:41,361] Trial 0 finished with value: 2.203926205635071 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 576, 'lr': 0.008766462463605289, 'batch_size': 336, 'epochs': 114}. Best is trial 0 with value: 2.203926205635071.\n",
      "[I 2024-06-16 18:31:57,587] Trial 1 finished with value: 1.1333348453044891 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 32, 'lr': 0.0011883161866214449, 'batch_size': 384, 'epochs': 140}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:32:12,204] Trial 2 finished with value: 1.8681009411811829 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 512, 'lr': 0.005349148422409402, 'batch_size': 336, 'epochs': 67}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:32:19,244] Trial 3 finished with value: 1.8444992303848267 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'lr': 0.006859311991437056, 'batch_size': 336, 'epochs': 149}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:33:07,912] Trial 4 finished with value: 1.1664912913526808 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 576, 'lr': 0.001667967927599912, 'batch_size': 32, 'epochs': 107}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:33:10,183] Trial 5 finished with value: 2.4062072038650513 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'lr': 0.008351582164335389, 'batch_size': 352, 'epochs': 132}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:33:38,325] Trial 6 finished with value: 1.51430082321167 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'lr': 0.003491499314123799, 'batch_size': 448, 'epochs': 75}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:34:30,606] Trial 7 finished with value: 1.5163503885269165 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'lr': 0.006217577780577689, 'batch_size': 480, 'epochs': 97}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:34:51,226] Trial 8 finished with value: 1.6501215696334839 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 576, 'lr': 0.0011489139916763887, 'batch_size': 368, 'epochs': 147}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:35:10,628] Trial 9 finished with value: 1.4012335538864136 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 448, 'hidden_layer_size_3': 128, 'lr': 0.009188670852776307, 'batch_size': 368, 'epochs': 99}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:35:25,347] Trial 10 finished with value: 1.3233977556228638 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 32, 'lr': 0.003476225765133348, 'batch_size': 176, 'epochs': 127}. Best is trial 1 with value: 1.1333348453044891.\n",
      "[I 2024-06-16 18:36:14,476] Trial 11 finished with value: 1.1137675378057692 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 128, 'lr': 0.0010207785755847534, 'batch_size': 48, 'epochs': 85}. Best is trial 11 with value: 1.1137675378057692.\n",
      "[I 2024-06-16 18:37:10,503] Trial 12 finished with value: 0.9072818160057068 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'lr': 0.002707536889781467, 'batch_size': 176, 'epochs': 82}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:37:47,844] Trial 13 finished with value: 0.9332822561264038 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.003043582190653463, 'batch_size': 96, 'epochs': 51}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:38:24,131] Trial 14 finished with value: 1.133917212486267 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.0033599226288509453, 'batch_size': 176, 'epochs': 50}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:38:59,385] Trial 15 finished with value: 1.2027978897094727 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 16, 'lr': 0.004811982169048048, 'batch_size': 128, 'epochs': 51}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:39:15,329] Trial 16 finished with value: 1.3945105075836182 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 256, 'lr': 0.002521339656671461, 'batch_size': 112, 'epochs': 67}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:39:30,251] Trial 17 finished with value: 1.8406261603037517 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 64, 'lr': 0.004428920152481976, 'batch_size': 192, 'epochs': 85}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:40:36,386] Trial 18 finished with value: 1.0656580924987793 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 64, 'lr': 0.0024238963957672758, 'batch_size': 256, 'epochs': 62}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:41:14,759] Trial 19 finished with value: 0.9401890337467194 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'lr': 0.002376283364348193, 'batch_size': 256, 'epochs': 83}. Best is trial 12 with value: 0.9072818160057068.\n",
      "[I 2024-06-16 18:42:00,170] A new study created in memory with name: no-name-5bbddd3a-2ed1-4995-a2f3-032c5c392d3c\n",
      "[I 2024-06-16 18:42:32,440] Trial 0 finished with value: 1.7681940793991089 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 576, 'lr': 0.004750475212491478, 'batch_size': 256, 'epochs': 75}. Best is trial 0 with value: 1.7681940793991089.\n",
      "[I 2024-06-16 18:42:39,321] Trial 1 finished with value: 1.2887278199195862 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'lr': 0.0075937376263498415, 'batch_size': 256, 'epochs': 85}. Best is trial 1 with value: 1.2887278199195862.\n",
      "[I 2024-06-16 18:42:45,090] Trial 2 finished with value: 1.6707653105258942 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 256, 'lr': 0.009298253489364406, 'batch_size': 128, 'epochs': 67}. Best is trial 1 with value: 1.2887278199195862.\n",
      "[I 2024-06-16 18:44:25,947] Trial 3 finished with value: 1.5622050762176514 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 576, 'lr': 0.00854152757498277, 'batch_size': 400, 'epochs': 102}. Best is trial 1 with value: 1.2887278199195862.\n",
      "[I 2024-06-16 18:44:49,967] Trial 4 finished with value: 1.2473695993423461 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'lr': 0.009124710840138661, 'batch_size': 96, 'epochs': 117}. Best is trial 4 with value: 1.2473695993423461.\n",
      "[I 2024-06-16 18:45:26,284] Trial 5 finished with value: 1.0565543174743652 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 448, 'lr': 0.0014557192659044077, 'batch_size': 448, 'epochs': 100}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:46:54,146] Trial 6 finished with value: 1.4625605344772339 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 448, 'hidden_layer_size_2': 640, 'hidden_layer_size_3': 256, 'lr': 0.00674929559996789, 'batch_size': 432, 'epochs': 127}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:47:12,750] Trial 7 finished with value: 1.1816564003626506 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'lr': 0.0051899321849524305, 'batch_size': 80, 'epochs': 97}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:48:15,732] Trial 8 finished with value: 1.1026530265808105 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 640, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 256, 'lr': 0.0027294910341981275, 'batch_size': 432, 'epochs': 118}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:48:47,927] Trial 9 finished with value: 1.644210091659001 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 576, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 256, 'lr': 0.007088703655438465, 'batch_size': 32, 'epochs': 70}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:49:25,489] Trial 10 finished with value: 1.4119873046875 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 448, 'lr': 0.001069680639527226, 'batch_size': 512, 'epochs': 141}. Best is trial 5 with value: 1.0565543174743652.\n",
      "[I 2024-06-16 18:50:06,173] Trial 11 finished with value: 0.8779779970645905 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 256, 'lr': 0.001638216791444897, 'batch_size': 384, 'epochs': 111}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:50:32,376] Trial 12 finished with value: 1.2400495409965515 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 16, 'lr': 0.0010023432712301203, 'batch_size': 336, 'epochs': 50}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:51:23,584] Trial 13 finished with value: 1.186016321182251 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 448, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 448, 'lr': 0.0029393422487440967, 'batch_size': 512, 'epochs': 102}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:51:44,236] Trial 14 finished with value: 1.6047250032424927 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 576, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 128, 'lr': 0.0028308378779719813, 'batch_size': 352, 'epochs': 140}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:52:06,089] Trial 15 finished with value: 1.016448199748993 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 640, 'hidden_layer_size_2': 128, 'lr': 0.0036297718193212545, 'batch_size': 192, 'epochs': 114}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:52:14,914] Trial 16 finished with value: 1.2004175384839375 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 128, 'lr': 0.0040841664318225, 'batch_size': 192, 'epochs': 117}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:52:20,986] Trial 17 finished with value: 1.4705769419670105 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'lr': 0.003655501936895643, 'batch_size': 304, 'epochs': 128}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:52:32,838] Trial 18 finished with value: 1.110602617263794 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'lr': 0.00209718815639791, 'batch_size': 192, 'epochs': 128}. Best is trial 11 with value: 0.8779779970645905.\n",
      "[I 2024-06-16 18:52:56,280] Trial 19 finished with value: 1.1189398964246113 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 448, 'lr': 0.005725597514438655, 'batch_size': 192, 'epochs': 150}. Best is trial 11 with value: 0.8779779970645905.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 448, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_64:\n",
    "    print('Starting KAN reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 64, f'./models/KAN/64_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization 448AP fixed standardization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth',\n",
    "        '64': './models/MLP/64_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth',\n",
    "        '64': './models/KAN/64_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '128': ['./models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth'],\n",
    "        '64': ['./models/MLP/64_encoder_256.pth', './models/MLP/64_encoder_128.pth', './models/MLP/64_encoder_64.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '128': ['./models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth'],\n",
    "        '64': ['./models/KAN/64_encoder_256.pth', './models/KAN/64_encoder_128.pth', './models/KAN/64_encoder_64.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 64, the SAE will have 256 -> 128 -> 64\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 448\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 256 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [576, 512, 576], 0.2070868635955763, 448)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [512, 576], 0.5246554165652277, 128)\n",
    "    mlp_64 = load_MLP_model(model_paths['MLP']['64'], [512, 640, 448, 64], 0.37044336268225275, 64)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [512, 128, 448], 448)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [512, 448, 128], 128)\n",
    "    kan_64 = load_KAN_model(model_paths['KAN']['64'], [448, 576, 64, 256], 64)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    mlp_SAE_64 = load_SAE(SAE_paths['MLP']['64'], 64)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    kan_SAE_64 = load_SAE(SAE_paths['KAN']['64'], 64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n",
      "(12960,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy() # Predictions from the model (on CPU) - we need to move them to CPU to use numpy\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1)) # Euclidean distance between predictions and ground-truth\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    mlp_test_data_encoded_64 = stacked_encode_data(X_test_tensor, mlp_SAE_64)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    kan_test_data_encoded_64 = stacked_encode_data(X_test_tensor, kan_SAE_64)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    mlp_64_distances = evaluate_model(mlp_64, mlp_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    kan_64_distances = evaluate_model(kan_64, kan_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    print(mlp_64_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_128_distances.shape)\n",
    "    print(kan_64_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Statistics for  MLP_64\n",
      "MLP_64: 25th percentile: 1.70 - 50th percentile: 2.43 - 75th percentile: 3.25 - Outliers: 179 (1.38%)\n",
      "Max distance: 7.89\n",
      "====================================\n",
      "Statistics for  MLP_128\n",
      "MLP_128: 25th percentile: 2.02 - 50th percentile: 2.92 - 75th percentile: 4.02 - Outliers: 219 (1.69%)\n",
      "Max distance: 10.36\n",
      "====================================\n",
      "Statistics for  MLP_full\n",
      "MLP_full: 25th percentile: 1.56 - 50th percentile: 2.31 - 75th percentile: 3.19 - Outliers: 334 (2.58%)\n",
      "Max distance: 10.39\n",
      "====================================\n",
      "Statistics for  KAN_64\n",
      "KAN_64: 25th percentile: 1.00 - 50th percentile: 1.60 - 75th percentile: 2.56 - Outliers: 640 (4.94%)\n",
      "Max distance: 10.68\n",
      "====================================\n",
      "Statistics for  KAN_128\n",
      "KAN_128: 25th percentile: 1.02 - 50th percentile: 1.58 - 75th percentile: 2.53 - Outliers: 662 (5.11%)\n",
      "Max distance: 10.02\n",
      "====================================\n",
      "Statistics for  KAN_full\n",
      "KAN_full: 25th percentile: 1.00 - 50th percentile: 1.59 - 75th percentile: 2.39 - Outliers: 543 (4.19%)\n",
      "Max distance: 9.99\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_64': mlp_64_distances,\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_64': kan_64_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[1].set_title('Reduced models (128)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_64'], data['KAN_64']], labels=['MLP_64', 'KAN_64'])\n",
    "    axes[2].set_title('Reduced models (64)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print model statistics: 25th, 50th (median), 75th percentiles, and the amount or percentage of outliers\n",
    "    for key in data:\n",
    "        print(\"====================================\")\n",
    "        print(\"Statistics for \", key)\n",
    "        distances = data[key]\n",
    "        q25, q50, q75 = np.percentile(distances, [25, 50, 75])\n",
    "        iqr = q75 - q25\n",
    "        lower_bound = q25 - 1.5 * iqr\n",
    "        upper_bound = q75 + 1.5 * iqr\n",
    "        outliers = np.sum((distances < lower_bound) | (distances > upper_bound))\n",
    "        print(f'{key}: 25th percentile: {q25:.2f} - 50th percentile: {q50:.2f} - 75th percentile: {q75:.2f} - Outliers: {outliers} ({outliers/len(distances)*100:.2f}%)')\n",
    "        \n",
    "        # Print max predicted distance\n",
    "        max_distance = np.max(distances)\n",
    "        print(f'Max distance: {max_distance:.2f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
