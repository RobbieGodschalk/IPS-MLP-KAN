{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "@DeprecationWarning\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# *Load and pre-process the data\\\n",
    "@DeprecationWarning\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    pass\n",
    "    # # Since the csv files do not have column names, we define these first.\n",
    "    # list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # # Load the test data from all specified test sets.  \n",
    "    # df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    # df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # # Get all x,y,floor labels\n",
    "    # df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    # df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # # Add the labels to the pre-processed data\n",
    "    # df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # # Filter the data to only include the specified floor\n",
    "    # df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # # Pre-processing of the training data\n",
    "    # df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    # return df_test\n",
    "\n",
    "\n",
    "# Let's do it differently, have two separate functions\n",
    "# 1. Load a dataset based on a list of full paths, and return a df_{type}_x and df_{type}_y\n",
    "# 2. Preprocess the dataset based on the df_{type}_x and df_{type}_y\n",
    "\n",
    "def load_dataset(paths: list[str], num_APs: int, floor: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "    \n",
    "    # Load the data from all specified paths\n",
    "    df_x = pd.concat([pd.read_csv(path + 'rss.csv', names=list_of_APs) for path in paths])\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_y = pd.concat([pd.read_csv(path + 'crd.csv', names=['x', 'y', 'floor']) for path in paths])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    # Get indexes of the specified floor\n",
    "    floor_indexes = df_y[df_y['floor'] == floor].index\n",
    "    \n",
    "    # Keep only the rows with the specified floor for both x and y, and reset the indexes\n",
    "    df_x = df_x.loc[floor_indexes]\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # For df_y, also remove the floor column\n",
    "    df_y = df_y.loc[floor_indexes]\n",
    "    df_y = df_y.drop(columns=['floor'])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    return df_x, df_y\n",
    "\n",
    "def preprocess_rssi(df_rssi: pd.DataFrame, scaling_strategy: Scaling):\n",
    "    # Flattened dataset for easy searching\n",
    "    flattened = df_rssi.values.flatten()\n",
    "    \n",
    "    # Minimum rssi found\n",
    "    min_rssi = np.min(flattened)\n",
    "    print(\"Minimum RSSI: \", min_rssi)\n",
    "    \n",
    "    # Find biggest multiple of 10 smaller than min_rssi\n",
    "    replacement_rssi = np.floor((min_rssi - 1) / 10) * 10\n",
    "    print(\"Replacement value\", replacement_rssi)\n",
    "    \n",
    "    # Replace all 100 values with replacement_rssi\n",
    "    df_rssi = df_rssi.replace(100, replacement_rssi)\n",
    "    flattened = df_rssi.values.flatten() # Update flattened since we changed the dataframe\n",
    "    \n",
    "    # Standardization part\n",
    "    if scaling_strategy == Scaling.INDEPENDENT: # Might not work\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(df_rssi)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=df_rssi.columns)\n",
    "        df_rssi = df_scaled_rss\n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        df_rssi = (df_rssi - global_mean) / global_std\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df_rssi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x: (4320, 448)\n",
      "df_y: (4320, 2)\n",
      "df_train_x: (3888, 448)\n",
      "df_train_y: (3888, 2)\n",
      "df_val_x: (432, 448)\n",
      "df_val_y: (432, 2)\n",
      "      AP0  AP1  AP2  AP3  AP4  AP5  AP6  AP7  AP8  AP9  ...  AP438  AP439  \\\n",
      "4053  100  100  100  100  100  -85  -61  -86  -60  100  ...    100    100   \n",
      "963   100  -78  -68  100  100  100  -66  100  -80  -93  ...    100    100   \n",
      "4039  100  100  100  100  100  100  -47  100  -43  -80  ...    100    100   \n",
      "3523  100  100  100  100  100  100  -81  100  -85  100  ...    100    100   \n",
      "1766  100  -77  -84  100  100  100  -81  100  -75  -86  ...    100    100   \n",
      "\n",
      "      AP440  AP441  AP442  AP443  AP444  AP445  AP446  AP447  \n",
      "4053    100    100    100    100    100    100    100    100  \n",
      "963     100    100    100    100    100    100    100    100  \n",
      "4039    100    100    100    100    100    100    100    100  \n",
      "3523    100    100    100    100    100    100    100    100  \n",
      "1766    100    100    100    100    100    100    100    100  \n",
      "\n",
      "[5 rows x 448 columns]\n",
      "              x          y\n",
      "4053  12.904458  27.419428\n",
      "963    8.514918  20.267011\n",
      "4039   8.514918  29.207532\n",
      "3523   4.125378  23.843220\n",
      "1766  12.904458  25.631324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_paths = [\n",
    "    './data/V1.0/01/trn01',\n",
    "    './data/V1.0/02/trn01',\n",
    "    './data/V1.0/03/trn01', \n",
    "    './data/V1.0/04/trn01',\n",
    "    './data/V1.0/05/trn01',\n",
    "    './data/V1.0/06/trn01',\n",
    "    './data/V1.0/07/trn01',\n",
    "    './data/V1.0/08/trn01',\n",
    "    './data/V1.0/09/trn01',\n",
    "    './data/V1.0/10/trn01',\n",
    "    './data/V1.0/11/trn01',\n",
    "    './data/V1.0/12/trn01',\n",
    "    './data/V1.0/13/trn01',\n",
    "    './data/V1.0/14/trn01',\n",
    "    './data/V1.0/15/trn01',\n",
    "]\n",
    "num_APs = 448\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "# Split training data into 10% validation data\n",
    "df_x, df_y = load_dataset(data_paths, num_APs, floor)\n",
    "\n",
    "if DEBUG: print('df_x:', df_x.shape)\n",
    "if DEBUG: print('df_y:', df_y.shape)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n",
    "if DEBUG: print(df_train_x.head())\n",
    "if DEBUG: print(df_train_y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RSSI:  -98\n",
      "Replacement value -100.0\n",
      "Minimum RSSI:  -97\n",
      "Replacement value -100.0\n",
      "6.978871070431797e-17\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train_x = preprocess_rssi(df_train_x, scaling_strategy)\n",
    "df_val_x = preprocess_rssi(df_val_x, scaling_strategy)\n",
    "\n",
    "if DEBUG: print(np.mean(df_train_x.values.flatten()))\n",
    "if DEBUG: print(np.std(df_train_x.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RSSI:  -100\n",
      "Replacement value -110.0\n",
      "df_test_x: (12960, 448)\n",
      "df_test_y: (12960, 2)\n",
      "-6.698835098229902e-16\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    './data/V1.0/01/tst01', './data/V1.0/01/tst02', './data/V1.0/01/tst03',\n",
    "    './data/V1.0/02/tst01', './data/V1.0/02/tst02', './data/V1.0/02/tst03',\n",
    "    './data/V1.0/03/tst01', './data/V1.0/03/tst02', './data/V1.0/03/tst03',\n",
    "    './data/V1.0/04/tst01', './data/V1.0/04/tst02', './data/V1.0/04/tst03',\n",
    "    './data/V1.0/05/tst01', './data/V1.0/05/tst02', './data/V1.0/05/tst03',\n",
    "    './data/V1.0/06/tst01', './data/V1.0/06/tst02', './data/V1.0/06/tst03',\n",
    "    './data/V1.0/07/tst01', './data/V1.0/07/tst02', './data/V1.0/07/tst03',\n",
    "    './data/V1.0/08/tst01', './data/V1.0/08/tst02', './data/V1.0/08/tst03',\n",
    "    './data/V1.0/09/tst01', './data/V1.0/09/tst02', './data/V1.0/09/tst03',\n",
    "    './data/V1.0/10/tst01', './data/V1.0/10/tst02', './data/V1.0/10/tst03',\n",
    "    './data/V1.0/11/tst01', './data/V1.0/11/tst02', './data/V1.0/11/tst03',\n",
    "    './data/V1.0/12/tst01', './data/V1.0/12/tst02', './data/V1.0/12/tst03',\n",
    "    './data/V1.0/13/tst01', './data/V1.0/13/tst02', './data/V1.0/13/tst03',\n",
    "    './data/V1.0/14/tst01', './data/V1.0/14/tst02', './data/V1.0/14/tst03',\n",
    "    './data/V1.0/15/tst01', './data/V1.0/15/tst02', './data/V1.0/15/tst03',\n",
    "]\n",
    "\n",
    "df_test_x, df_test_y = load_dataset(data_paths, num_APs, floor)\n",
    "\n",
    "# Standardize the test data\n",
    "df_test_x = preprocess_rssi(df_test_x, scaling_strategy)\n",
    "\n",
    "if DEBUG: print('df_test_x:', df_test_x.shape)\n",
    "if DEBUG: print('df_test_y:', df_test_y.shape)\n",
    "\n",
    "if DEBUG: print(np.mean(df_test_x.values.flatten()))\n",
    "if DEBUG: print(np.std(df_test_x.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=448):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Update input_dim for next layer\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0         \n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    \n",
    "    # hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [64, 48, 48]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = False\n",
    "SEARCH_MLP_REDUCED_128 = False\n",
    "SEARCH_MLP_REDUCED_64 = False\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-16 23:27:45,286] A new study created in memory with name: no-name-ed8c7af1-fe64-4056-9d7c-058bd0c076a1\n",
      "[I 2024-06-16 23:27:46,920] Trial 0 finished with value: 6.627836465835571 and parameters: {'dropout_rate': 0.4722451348187576, 'lr': 0.0013442876966560987, 'batch_size': 272, 'epochs': 58}. Best is trial 0 with value: 6.627836465835571.\n",
      "[I 2024-06-16 23:27:48,986] Trial 1 finished with value: 2.803048610687256 and parameters: {'dropout_rate': 0.26491172086727766, 'lr': 0.002111559280411616, 'batch_size': 496, 'epochs': 99}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:27:50,252] Trial 2 finished with value: 7.431334495544434 and parameters: {'dropout_rate': 0.42340197867290985, 'lr': 0.002547588579292908, 'batch_size': 448, 'epochs': 87}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:27:51,949] Trial 3 finished with value: 3.821970224380493 and parameters: {'dropout_rate': 0.3251394309457778, 'lr': 0.004639912739374085, 'batch_size': 496, 'epochs': 77}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:27:54,509] Trial 4 finished with value: 2.979691743850708 and parameters: {'dropout_rate': 0.4928123985160818, 'lr': 0.008642977114479991, 'batch_size': 304, 'epochs': 122}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:27:57,139] Trial 5 finished with value: 4.0990214347839355 and parameters: {'dropout_rate': 0.4315903595797554, 'lr': 0.0011912574537061083, 'batch_size': 304, 'epochs': 115}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:28:00,652] Trial 6 finished with value: 3.8634532519749234 and parameters: {'dropout_rate': 0.4635864888872727, 'lr': 0.007495970839997908, 'batch_size': 32, 'epochs': 70}. Best is trial 1 with value: 2.803048610687256.\n",
      "[I 2024-06-16 23:28:02,337] Trial 7 finished with value: 2.6665966510772705 and parameters: {'dropout_rate': 0.2096898237120256, 'lr': 0.008994311201827521, 'batch_size': 448, 'epochs': 65}. Best is trial 7 with value: 2.6665966510772705.\n",
      "[I 2024-06-16 23:28:03,494] Trial 8 finished with value: 4.918931007385254 and parameters: {'dropout_rate': 0.3647028295278705, 'lr': 0.008017482955347693, 'batch_size': 496, 'epochs': 114}. Best is trial 7 with value: 2.6665966510772705.\n",
      "[I 2024-06-16 23:28:05,423] Trial 9 finished with value: 8.129998842875162 and parameters: {'dropout_rate': 0.5700350603471613, 'lr': 0.0038910284607000497, 'batch_size': 144, 'epochs': 81}. Best is trial 7 with value: 2.6665966510772705.\n",
      "[I 2024-06-16 23:28:06,852] Trial 10 finished with value: 2.9371893405914307 and parameters: {'dropout_rate': 0.20119182714456904, 'lr': 0.006417832236419082, 'batch_size': 384, 'epochs': 54}. Best is trial 7 with value: 2.6665966510772705.\n",
      "[I 2024-06-16 23:28:09,010] Trial 11 finished with value: 2.0989518761634827 and parameters: {'dropout_rate': 0.22702635125317247, 'lr': 0.009783234367078926, 'batch_size': 400, 'epochs': 98}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:10,509] Trial 12 finished with value: 2.596722960472107 and parameters: {'dropout_rate': 0.21267108789128245, 'lr': 0.00984957661807241, 'batch_size': 384, 'epochs': 100}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:11,258] Trial 13 finished with value: 5.222286939620972 and parameters: {'dropout_rate': 0.28843183665358546, 'lr': 0.009652847771420548, 'batch_size': 368, 'epochs': 147}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:12,373] Trial 14 finished with value: 3.872983455657959 and parameters: {'dropout_rate': 0.2548744736115667, 'lr': 0.00997280538478142, 'batch_size': 192, 'epochs': 103}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:13,896] Trial 15 finished with value: 4.607221841812134 and parameters: {'dropout_rate': 0.3296705361577021, 'lr': 0.006563174843523734, 'batch_size': 368, 'epochs': 132}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:15,187] Trial 16 finished with value: 3.348198175430298 and parameters: {'dropout_rate': 0.23962221347114793, 'lr': 0.00736653135088773, 'batch_size': 192, 'epochs': 95}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:16,515] Trial 17 finished with value: 3.2241923809051514 and parameters: {'dropout_rate': 0.3022041667347207, 'lr': 0.005671279961831257, 'batch_size': 416, 'epochs': 107}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:19,008] Trial 18 finished with value: 2.10809326171875 and parameters: {'dropout_rate': 0.37472261446606925, 'lr': 0.008983722083780114, 'batch_size': 336, 'epochs': 93}. Best is trial 11 with value: 2.0989518761634827.\n",
      "[I 2024-06-16 23:28:22,195] Trial 19 finished with value: 1.9713761806488037 and parameters: {'dropout_rate': 0.37017412737151817, 'lr': 0.008606023151463369, 'batch_size': 224, 'epochs': 86}. Best is trial 19 with value: 1.9713761806488037.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=448):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [64, 48, 48]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_64:\n",
    "    print('Starting MLP reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/64_MLP.pth', encoders, 64)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# filename = now.strftime(\"./optimization/MLP/MLP optimization 448AP timing full - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "# with open(filename, 'w') as f:\n",
    "#     f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 448 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [8, 8]\n",
    "    kan_layers = [448] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [640, 576, 512, 448, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = False \n",
    "SEARCH_KAN_REDUCED_128 = False\n",
    "SEARCH_KAN_REDUCED_64 = False \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-16 23:42:11,677] A new study created in memory with name: no-name-969f128c-c8a0-410c-8cb8-6d7219c6e5f6\n",
      "[I 2024-06-16 23:42:18,091] Trial 0 finished with value: 1.9822933077812195 and parameters: {'lr': 0.002808517011406305, 'batch_size': 256, 'epochs': 71}. Best is trial 0 with value: 1.9822933077812195.\n",
      "[I 2024-06-16 23:42:22,729] Trial 1 finished with value: 1.9613197445869446 and parameters: {'lr': 0.009791518119072366, 'batch_size': 240, 'epochs': 88}. Best is trial 1 with value: 1.9613197445869446.\n",
      "[I 2024-06-16 23:42:27,777] Trial 2 finished with value: 1.9724478125572205 and parameters: {'lr': 0.005657611601347645, 'batch_size': 320, 'epochs': 126}. Best is trial 1 with value: 1.9613197445869446.\n",
      "[I 2024-06-16 23:42:35,664] Trial 3 finished with value: 1.3489575684070587 and parameters: {'lr': 0.0047618708784224854, 'batch_size': 112, 'epochs': 66}. Best is trial 3 with value: 1.3489575684070587.\n",
      "[I 2024-06-16 23:42:40,152] Trial 4 finished with value: 1.9039395451545715 and parameters: {'lr': 0.006156047937185428, 'batch_size': 240, 'epochs': 51}. Best is trial 3 with value: 1.3489575684070587.\n",
      "[I 2024-06-16 23:42:47,329] Trial 5 finished with value: 1.934863805770874 and parameters: {'lr': 0.007644240004441629, 'batch_size': 240, 'epochs': 96}. Best is trial 3 with value: 1.3489575684070587.\n",
      "[I 2024-06-16 23:42:52,338] Trial 6 finished with value: 1.801107664903005 and parameters: {'lr': 0.008064191963162534, 'batch_size': 80, 'epochs': 140}. Best is trial 3 with value: 1.3489575684070587.\n",
      "[I 2024-06-16 23:42:57,172] Trial 7 finished with value: 1.8161021868387859 and parameters: {'lr': 0.0072592146777494355, 'batch_size': 192, 'epochs': 90}. Best is trial 3 with value: 1.3489575684070587.\n",
      "[I 2024-06-16 23:43:19,568] Trial 8 finished with value: 1.1663448093114075 and parameters: {'lr': 0.0029234675088545825, 'batch_size': 16, 'epochs': 120}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:43:23,975] Trial 9 finished with value: 1.8694796562194824 and parameters: {'lr': 0.004173863247070113, 'batch_size': 400, 'epochs': 73}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:43:32,854] Trial 10 finished with value: 1.8753738403320312 and parameters: {'lr': 0.0022325182591065515, 'batch_size': 512, 'epochs': 119}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:44:04,960] Trial 11 finished with value: 1.1696030652081524 and parameters: {'lr': 0.0010050746677084839, 'batch_size': 16, 'epochs': 116}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:44:28,787] Trial 12 finished with value: 1.21717346566064 and parameters: {'lr': 0.0011897990879142116, 'batch_size': 32, 'epochs': 115}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:44:41,985] Trial 13 finished with value: 1.513147575514657 and parameters: {'lr': 0.0010129915676457722, 'batch_size': 32, 'epochs': 143}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:44:51,921] Trial 14 finished with value: 1.8945340712865193 and parameters: {'lr': 0.0030053893065349366, 'batch_size': 144, 'epochs': 107}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:45:20,570] Trial 15 finished with value: 1.2152165902985468 and parameters: {'lr': 0.0035577733698790234, 'batch_size': 16, 'epochs': 130}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:45:28,224] Trial 16 finished with value: 1.6945606867472331 and parameters: {'lr': 0.001967380135284391, 'batch_size': 160, 'epochs': 110}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:45:33,655] Trial 17 finished with value: 1.7525462905565898 and parameters: {'lr': 0.0018496624188283051, 'batch_size': 80, 'epochs': 149}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:45:39,575] Trial 18 finished with value: 1.4795987606048584 and parameters: {'lr': 0.004248315751438979, 'batch_size': 336, 'epochs': 130}. Best is trial 8 with value: 1.1663448093114075.\n",
      "[I 2024-06-16 23:45:43,857] Trial 19 finished with value: 1.869301398595174 and parameters: {'lr': 0.0028639130597524756, 'batch_size': 80, 'epochs': 103}. Best is trial 8 with value: 1.1663448093114075.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [448, 8,8, 2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 448, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced search for 256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_64 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_64:\n",
    "    print('Starting KAN reduced grid search for 256-128-64 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 448, [256, 128, 64], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/64_encoder_{int(256 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 64, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 64, f'./models/KAN/64_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# filename = now.strftime(\"./optimization/KAN/KAN optimization 448AP fixed standardization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "# with open(filename, 'w') as f:\n",
    "#     f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth',\n",
    "        '64': './models/MLP/64_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth',\n",
    "        '64': './models/KAN/64_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '128': ['./models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth'],\n",
    "        '64': ['./models/MLP/64_encoder_256.pth', './models/MLP/64_encoder_128.pth', './models/MLP/64_encoder_64.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '128': ['./models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth'],\n",
    "        '64': ['./models/KAN/64_encoder_256.pth', './models/KAN/64_encoder_128.pth', './models/KAN/64_encoder_64.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 64, the SAE will have 256 -> 128 -> 64\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 448\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 256 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [64, 48, 48], 0.37017412737151817, 448)\n",
    "    # mlp_128 = load_MLP_model(model_paths['MLP']['128'], [512, 576], 0.5246554165652277, 128)\n",
    "    # mlp_64 = load_MLP_model(model_paths['MLP']['64'], [512, 640, 448, 64], 0.37044336268225275, 64)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [8,9], 448)\n",
    "    # kan_128 = load_KAN_model(model_paths['KAN']['128'], [512, 448, 128], 128)\n",
    "    # kan_64 = load_KAN_model(model_paths['KAN']['64'], [448, 576, 64, 256], 64)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    # mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    # mlp_SAE_64 = load_SAE(SAE_paths['MLP']['64'], 64)\n",
    "    # kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    # kan_SAE_64 = load_SAE(SAE_paths['KAN']['64'], 64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation is disabled\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy() # Predictions from the model (on CPU) - we need to move them to CPU to use numpy\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1)) # Euclidean distance between predictions and ground-truth\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    # mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    # mlp_test_data_encoded_64 = stacked_encode_data(X_test_tensor, mlp_SAE_64)\n",
    "    # kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    # kan_test_data_encoded_64 = stacked_encode_data(X_test_tensor, kan_SAE_64)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    # mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    # mlp_64_distances = evaluate_model(mlp_64, mlp_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    # kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    # kan_64_distances = evaluate_model(kan_64, kan_test_data_encoded_64, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    # print(mlp_128_distances.shape)\n",
    "    # print(mlp_64_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    # print(kan_128_distances.shape)\n",
    "    # print(kan_64_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation is disabled\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        # 'MLP_64': mlp_64_distances,\n",
    "        # 'MLP_128': mlp_128_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        # 'KAN_64': kan_64_distances,\n",
    "        # 'KAN_128': kan_128_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    fig, axes = plt.subplots()\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    # axes[1].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    # axes[1].set_title('Reduced models (128)')\n",
    "    \n",
    "    # axes[2].boxplot([data['MLP_64'], data['KAN_64']], labels=['MLP_64', 'KAN_64'])\n",
    "    # axes[2].set_title('Reduced models (64)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print model statistics: 25th, 50th (median), 75th percentiles, and the amount or percentage of outliers\n",
    "    for key in data:\n",
    "        print(\"====================================\")\n",
    "        print(\"Statistics for \", key)\n",
    "        distances = data[key]\n",
    "        q25, q50, q75 = np.percentile(distances, [25, 50, 75])\n",
    "        iqr = q75 - q25\n",
    "        lower_bound = q25 - 1.5 * iqr\n",
    "        upper_bound = q75 + 1.5 * iqr\n",
    "        outliers = np.sum((distances < lower_bound) | (distances > upper_bound))\n",
    "        print(f'{key}: 25th percentile: {q25:.2f} - 50th percentile: {q50:.2f} - 75th percentile: {q75:.2f} - Outliers: {outliers} ({outliers/len(distances)*100:.2f}%)')\n",
    "        \n",
    "        # Print max predicted distance\n",
    "        max_distance = np.max(distances)\n",
    "        print(f'Max distance: {max_distance:.2f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
