{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7777)\n",
    "np.random.seed(7777)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# print('Training will be done on the ' + 'GPU' if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full: (4320, 623)\n",
      "df_train_x: (3888, 620)\n",
      "df_train_y: (3888, 2)\n",
      "df_val_x: (432, 620)\n",
      "df_val_y: (432, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data/'\n",
    "training_months = ['01']\n",
    "sets = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "\n",
    "df_x = df_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_y = df_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(df_x, df_y, test_size=0.1, random_state=7777)\n",
    "\n",
    "if DEBUG: print('df_full:', df_full.shape)\n",
    "if DEBUG: print('df_train_x:', df_train_x.shape)\n",
    "if DEBUG: print('df_train_y:', df_train_y.shape)\n",
    "if DEBUG: print('df_val_x:', df_val_x.shape)\n",
    "if DEBUG: print('df_val_y:', df_val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test: (288, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01']\n",
    "sets = ['01'] # 01 Corresponds to the same locations as the training set - All with same direction\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = ['01']\n",
    "# sets = ['02', '03', '04']\n",
    "# type = DatasetType.VALIDATION\n",
    "\n",
    "# df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "# df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "# df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "# if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=620):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training/Validation loops\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH \n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "    \n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train() # Enable training mode\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data = data.to(device)\n",
    "            data, _ = encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    # hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = True\n",
    "SEARCH_MLP_REDUCED_256 = True\n",
    "SEARCH_MLP_REDUCED_128 = True\n",
    "\n",
    "TRIALS_MLP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 23:05:15,332] A new study created in memory with name: no-name-504946cc-ad68-4a9b-8644-b87e9560a764\n",
      "[I 2024-06-13 23:05:18,781] Trial 0 finished with value: 11.534497737884521 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'dropout_rate': 0.5155105088626305, 'lr': 0.003393888938323266, 'batch_size': 336, 'epochs': 120}. Best is trial 0 with value: 11.534497737884521.\n",
      "[I 2024-06-13 23:05:21,536] Trial 1 finished with value: 5.4646992683410645 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 256, 'dropout_rate': 0.45067922353741907, 'lr': 0.006096627170644639, 'batch_size': 496, 'epochs': 105}. Best is trial 1 with value: 5.4646992683410645.\n",
      "[I 2024-06-13 23:05:25,885] Trial 2 finished with value: 8.858087857564291 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 64, 'dropout_rate': 0.46282720552286, 'lr': 0.007799685202204747, 'batch_size': 192, 'epochs': 100}. Best is trial 1 with value: 5.4646992683410645.\n",
      "[I 2024-06-13 23:05:29,325] Trial 3 finished with value: 2.4721198081970215 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'dropout_rate': 0.4185923900334064, 'lr': 0.002940700590626965, 'batch_size': 384, 'epochs': 144}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:31,944] Trial 4 finished with value: 7.570296764373779 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 32, 'dropout_rate': 0.5140996060025862, 'lr': 0.005902454301105334, 'batch_size': 416, 'epochs': 90}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:41,491] Trial 5 finished with value: 4.5476250648498535 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 620, 'dropout_rate': 0.4872330078052703, 'lr': 0.006895612837911031, 'batch_size': 288, 'epochs': 116}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:42,448] Trial 6 finished with value: 20.325806617736816 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'dropout_rate': 0.5545397302519703, 'lr': 0.005080038067702322, 'batch_size': 416, 'epochs': 105}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:48,602] Trial 7 finished with value: 4.6161192655563354 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 620, 'dropout_rate': 0.47968778010789376, 'lr': 0.007094533482119063, 'batch_size': 128, 'epochs': 118}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:49,778] Trial 8 finished with value: 15.283542315165201 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 620, 'dropout_rate': 0.3873522841022127, 'lr': 0.0012597615549217575, 'batch_size': 208, 'epochs': 69}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:05:51,261] Trial 9 finished with value: 6.165332555770874 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'dropout_rate': 0.3539764704508211, 'lr': 0.008872210793555376, 'batch_size': 272, 'epochs': 79}. Best is trial 3 with value: 2.4721198081970215.\n",
      "[I 2024-06-13 23:06:01,830] Trial 10 finished with value: 2.122578497286196 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.2208843371779567, 'lr': 0.0010885060389407757, 'batch_size': 16, 'epochs': 150}. Best is trial 10 with value: 2.122578497286196.\n",
      "[I 2024-06-13 23:06:16,480] Trial 11 finished with value: 1.761126377752849 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.2282902744800481, 'lr': 0.0012696329215743322, 'batch_size': 32, 'epochs': 150}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:06:36,017] Trial 12 finished with value: 1.803977694776323 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.200126653170006, 'lr': 0.0011604114440357746, 'batch_size': 16, 'epochs': 150}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:06:48,519] Trial 13 finished with value: 2.0222574604882135 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.21777277438309522, 'lr': 0.0027591040154694574, 'batch_size': 16, 'epochs': 136}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:06:51,378] Trial 14 finished with value: 2.4095847606658936 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.2852372659485962, 'lr': 0.004369382996186435, 'batch_size': 96, 'epochs': 133}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:06:52,166] Trial 15 finished with value: 26.241798400878906 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 64, 'dropout_rate': 0.28239407664685234, 'lr': 0.0018844207313685243, 'batch_size': 96, 'epochs': 132}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:06:57,495] Trial 16 finished with value: 2.1369649853025163 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.28172302043989383, 'lr': 0.004094421295051294, 'batch_size': 64, 'epochs': 51}. Best is trial 11 with value: 1.761126377752849.\n",
      "[I 2024-06-13 23:07:04,828] Trial 17 finished with value: 1.7487763563791912 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 64, 'dropout_rate': 0.20163246481690708, 'lr': 0.0021621838352839846, 'batch_size': 176, 'epochs': 144}. Best is trial 17 with value: 1.7487763563791912.\n",
      "[I 2024-06-13 23:07:09,789] Trial 18 finished with value: 2.5828715165456138 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 32, 'dropout_rate': 0.3288373887549719, 'lr': 0.0025151813923045903, 'batch_size': 176, 'epochs': 129}. Best is trial 17 with value: 1.7487763563791912.\n",
      "[I 2024-06-13 23:07:12,855] Trial 19 finished with value: 12.024195035298666 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'dropout_rate': 0.24413908946403987, 'lr': 0.0020037740777521743, 'batch_size': 144, 'epochs': 142}. Best is trial 17 with value: 1.7487763563791912.\n",
      "[I 2024-06-13 23:07:28,304] A new study created in memory with name: no-name-5e786a6d-daf6-4116-b4e0-4a3cbeebefdc\n",
      "[I 2024-06-13 23:07:28,877] Trial 0 finished with value: 68.19110488891602 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 32, 'dropout_rate': 0.518880307195958, 'lr': 0.005796708880648027, 'batch_size': 336, 'epochs': 78}. Best is trial 0 with value: 68.19110488891602.\n",
      "[I 2024-06-13 23:07:31,247] Trial 1 finished with value: 19.50319480895996 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 128, 'dropout_rate': 0.4739421430989776, 'lr': 0.007989800483545254, 'batch_size': 336, 'epochs': 82}. Best is trial 1 with value: 19.50319480895996.\n",
      "[I 2024-06-13 23:07:34,931] Trial 2 finished with value: 3.1423546075820923 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'dropout_rate': 0.42970722204463674, 'lr': 0.002875560344493324, 'batch_size': 320, 'epochs': 101}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:35,828] Trial 3 finished with value: 24.98765754699707 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.5625052651331346, 'lr': 0.006566293858264163, 'batch_size': 416, 'epochs': 58}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:37,648] Trial 4 finished with value: 102.62246856689453 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 700, 'dropout_rate': 0.5240664668236785, 'lr': 0.002602228437480145, 'batch_size': 96, 'epochs': 88}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:39,253] Trial 5 finished with value: 5.504850387573242 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 32, 'dropout_rate': 0.5207145373618347, 'lr': 0.007773365430577982, 'batch_size': 144, 'epochs': 84}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:41,102] Trial 6 finished with value: 5.153893232345581 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 16, 'dropout_rate': 0.2568618912335841, 'lr': 0.003059937097348527, 'batch_size': 128, 'epochs': 100}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:44,576] Trial 7 finished with value: 3.3892961740493774 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 700, 'dropout_rate': 0.24386311862943577, 'lr': 0.006283469786728547, 'batch_size': 128, 'epochs': 86}. Best is trial 2 with value: 3.1423546075820923.\n",
      "[I 2024-06-13 23:07:59,719] Trial 8 finished with value: 2.8623881254877364 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 128, 'dropout_rate': 0.27822867289553976, 'lr': 0.0017393432973224197, 'batch_size': 32, 'epochs': 105}. Best is trial 8 with value: 2.8623881254877364.\n",
      "[I 2024-06-13 23:08:01,425] Trial 9 finished with value: 14.226170063018799 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 128, 'dropout_rate': 0.5191565423180209, 'lr': 0.005663407300937136, 'batch_size': 336, 'epochs': 115}. Best is trial 8 with value: 2.8623881254877364.\n",
      "[I 2024-06-13 23:08:43,276] Trial 10 finished with value: 2.054494467046526 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'dropout_rate': 0.3254543738426199, 'lr': 0.0010847498238103878, 'batch_size': 16, 'epochs': 139}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:07,119] Trial 11 finished with value: 2.6047460149835655 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'dropout_rate': 0.3301068988422208, 'lr': 0.0010237276105579227, 'batch_size': 16, 'epochs': 150}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:22,376] Trial 12 finished with value: 2.543483700071062 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'dropout_rate': 0.3503567690868493, 'lr': 0.0013152620693340712, 'batch_size': 32, 'epochs': 150}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:23,600] Trial 13 finished with value: 16.464848518371582 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'dropout_rate': 0.3522972834541665, 'lr': 0.004221122228526819, 'batch_size': 224, 'epochs': 149}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:29,453] Trial 14 finished with value: 6.825025796890259 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 620, 'dropout_rate': 0.37127578727443333, 'lr': 0.009841386716166196, 'batch_size': 224, 'epochs': 132}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:35,798] Trial 15 finished with value: 2.888209649494716 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 64, 'dropout_rate': 0.3062403095976376, 'lr': 0.003860220813249409, 'batch_size': 64, 'epochs': 132}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:40,284] Trial 16 finished with value: 2.718216896057129 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'dropout_rate': 0.20614541291714306, 'lr': 0.0012237745892978431, 'batch_size': 496, 'epochs': 134}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:09:42,485] Trial 17 finished with value: 18.254902521769207 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 512, 'dropout_rate': 0.41437583537595984, 'lr': 0.0043745857030794534, 'batch_size': 192, 'epochs': 120}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:10:02,446] Trial 18 finished with value: 3.5164035514548972 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 620, 'dropout_rate': 0.38382897227385454, 'lr': 0.0019846371921660327, 'batch_size': 16, 'epochs': 140}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:10:14,773] Trial 19 finished with value: 3.4125714699427285 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'dropout_rate': 0.3054463294202424, 'lr': 0.003536941200054, 'batch_size': 80, 'epochs': 121}. Best is trial 10 with value: 2.054494467046526.\n",
      "[I 2024-06-13 23:11:04,094] A new study created in memory with name: no-name-3b57e887-d1b2-4e3f-bfdb-15ba4906e331\n",
      "[I 2024-06-13 23:11:06,827] Trial 0 finished with value: 13.49194367726644 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 700, 'dropout_rate': 0.4937349174373655, 'lr': 0.004870386012839288, 'batch_size': 208, 'epochs': 52}. Best is trial 0 with value: 13.49194367726644.\n",
      "[I 2024-06-13 23:11:08,717] Trial 1 finished with value: 2.954624652862549 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 700, 'dropout_rate': 0.24611086204261443, 'lr': 0.005568828240602486, 'batch_size': 480, 'epochs': 136}. Best is trial 1 with value: 2.954624652862549.\n",
      "[I 2024-06-13 23:11:10,015] Trial 2 finished with value: 2.860386689503988 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 32, 'dropout_rate': 0.20204588036886828, 'lr': 0.009437494347297147, 'batch_size': 208, 'epochs': 120}. Best is trial 2 with value: 2.860386689503988.\n",
      "[I 2024-06-13 23:11:10,735] Trial 3 finished with value: 27.702626546223957 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'dropout_rate': 0.37490388472027525, 'lr': 0.0023749148304943873, 'batch_size': 208, 'epochs': 73}. Best is trial 2 with value: 2.860386689503988.\n",
      "[I 2024-06-13 23:11:11,250] Trial 4 finished with value: 26.39377212524414 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 128, 'dropout_rate': 0.5768002370029631, 'lr': 0.0096237665159896, 'batch_size': 272, 'epochs': 141}. Best is trial 2 with value: 2.860386689503988.\n",
      "[I 2024-06-13 23:11:21,610] Trial 5 finished with value: 2.187407612800598 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.57174503364085, 'lr': 0.0011345819050349543, 'batch_size': 272, 'epochs': 112}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:23,694] Trial 6 finished with value: 15.398860795157296 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'dropout_rate': 0.44392263420170563, 'lr': 0.009395739338758011, 'batch_size': 64, 'epochs': 104}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:35,940] Trial 7 finished with value: 9.25059713636126 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 700, 'dropout_rate': 0.32835986194537603, 'lr': 0.0010356903969630624, 'batch_size': 64, 'epochs': 141}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:36,934] Trial 8 finished with value: 38.508811950683594 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 16, 'dropout_rate': 0.4018214853614863, 'lr': 0.009650714563781287, 'batch_size': 224, 'epochs': 117}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:49,687] Trial 9 finished with value: 2.651179075241089 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 128, 'dropout_rate': 0.3481497226665174, 'lr': 0.005413704737438063, 'batch_size': 224, 'epochs': 55}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:51,887] Trial 10 finished with value: 4.011552453041077 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 64, 'dropout_rate': 0.5926636921522616, 'lr': 0.0034308767600784865, 'batch_size': 400, 'epochs': 77}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:56,282] Trial 11 finished with value: 9.934659004211426 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 128, 'dropout_rate': 0.3052541545787498, 'lr': 0.00709143082065061, 'batch_size': 352, 'epochs': 90}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:11:59,715] Trial 12 finished with value: 8.212569952011108 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 128, 'dropout_rate': 0.5083730515722468, 'lr': 0.0071047537520746015, 'batch_size': 320, 'epochs': 99}. Best is trial 5 with value: 2.187407612800598.\n",
      "[I 2024-06-13 23:12:07,881] Trial 13 finished with value: 2.0428555011749268 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.315733992718169, 'lr': 0.0036185607103207656, 'batch_size': 144, 'epochs': 57}. Best is trial 13 with value: 2.0428555011749268.\n",
      "[I 2024-06-13 23:12:18,039] Trial 14 finished with value: 1.6536128222942352 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'dropout_rate': 0.27699133072484455, 'lr': 0.0013393296521469157, 'batch_size': 128, 'epochs': 73}. Best is trial 14 with value: 1.6536128222942352.\n",
      "[I 2024-06-13 23:12:18,778] Trial 15 finished with value: 16.90466833114624 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'dropout_rate': 0.28420796731384523, 'lr': 0.0031030571580085145, 'batch_size': 112, 'epochs': 69}. Best is trial 14 with value: 1.6536128222942352.\n",
      "[I 2024-06-13 23:12:24,526] Trial 16 finished with value: 1.9532756209373474 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.25570351281332043, 'lr': 0.0023320097916141305, 'batch_size': 128, 'epochs': 62}. Best is trial 14 with value: 1.6536128222942352.\n",
      "[I 2024-06-13 23:12:27,705] Trial 17 finished with value: 2.3312868773937225 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'dropout_rate': 0.2469342119113387, 'lr': 0.0021534900368665073, 'batch_size': 128, 'epochs': 84}. Best is trial 14 with value: 1.6536128222942352.\n",
      "[I 2024-06-13 23:12:55,692] Trial 18 finished with value: 2.0850479823571666 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 16, 'dropout_rate': 0.20523433430848712, 'lr': 0.002022185513381305, 'batch_size': 16, 'epochs': 63}. Best is trial 14 with value: 1.6536128222942352.\n",
      "[I 2024-06-13 23:12:58,464] Trial 19 finished with value: 2.548980156580607 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 620, 'dropout_rate': 0.26431990374898423, 'lr': 0.004317690702166173, 'batch_size': 160, 'epochs': 86}. Best is trial 14 with value: 1.6536128222942352.\n"
     ]
    }
   ],
   "source": [
    "%%capture MLP_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Trial Number: ', trial.number)\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "def save_best_MLP(study, path, SAE=None, input_size=620):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    hidden_layer_sizes = [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])]\n",
    "    model = MLP(hidden_layer_sizes, best_trial.params['dropout_rate'], input_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_MLP(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    # Save trained model from best trial\n",
    "    best_trial = study.best_trial\n",
    "    save_best_MLP(study, f'./models/MLP/full_MLP.pth')\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_256:\n",
    "    print('Starting MLP reduced grid search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/256_MLP.pth', encoders, 256)\n",
    "    \n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/MLP/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_MLP)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_MLP(study, f'./models/MLP/128_MLP.pth', encoders, 128)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/MLP/MLP optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(MLP_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov Arnold Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input KAN\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_KAN(kan_model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kan_model.to(device) # Move model to GPU if available\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_model_wts = None\n",
    "    best_val_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    epochs_to_use = 0\n",
    "    patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        kan_model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "                \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate the loss for this epoch\n",
    "        \n",
    "        kan_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = kan_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(kan_model.state_dict())\n",
    "            epochs_to_use = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement. Use model from epoch {epochs_to_use}')\n",
    "            break\n",
    "        \n",
    "    if best_model_wts is not None:\n",
    "        kan_model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return best_val_loss, kan_model # Return the besaverage validation loss for final epoch (taking early stopping into account) AND the model itself (for evaluation use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "import optuna\n",
    "\n",
    "def KAN_full_optimize(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [620] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "\n",
    "    print(kan_layers)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers) # We use the FastKAN implementation.\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss() # As we are doing regression\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model\n",
    "\n",
    "\n",
    "\n",
    "def KAN_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = [trial.suggest_categorical(f'hidden_layer_size_{i}', [700, 620, 512, 256, 128, 64, 32, 16]) for i in range(hidden_layer_count)]\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2] # Ensure correct input/output size\n",
    "    \n",
    "    print(kan_layers)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KAN(kan_layers)\n",
    "    \n",
    "    if optim.lower() == 'adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim.lower() == 'lbfgs': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_KAN_FULL = True \n",
    "SEARCH_KAN_REDUCED_256 = True\n",
    "SEARCH_KAN_REDUCED_128 = True \n",
    "\n",
    "TRIALS_KAN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 23:13:09,721] A new study created in memory with name: no-name-7211c8cf-72d2-4d7f-b0bf-1f844463d6b7\n",
      "[I 2024-06-13 23:13:48,307] Trial 0 finished with value: 1.1326236724853516 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'lr': 0.004600289864777528, 'batch_size': 144, 'epochs': 98}. Best is trial 0 with value: 1.1326236724853516.\n",
      "[I 2024-06-13 23:15:03,092] Trial 1 finished with value: 1.0659255882104237 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 128, 'lr': 0.00895913834198326, 'batch_size': 80, 'epochs': 130}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:15:08,115] Trial 2 finished with value: 3.069628119468689 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'lr': 0.005370547155749667, 'batch_size': 240, 'epochs': 80}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:15:20,523] Trial 3 finished with value: 1.7874387502670288 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 256, 'lr': 0.005517924722621563, 'batch_size': 384, 'epochs': 121}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:15:41,823] Trial 4 finished with value: 1.4552923440933228 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 128, 'lr': 0.0016116150258308017, 'batch_size': 496, 'epochs': 110}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:15:54,090] Trial 5 finished with value: 2.5860304832458496 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 700, 'lr': 0.008528090186291661, 'batch_size': 480, 'epochs': 57}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:16:34,251] Trial 6 finished with value: 2.1395577788352966 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 620, 'lr': 0.0021681085734650066, 'batch_size': 368, 'epochs': 75}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:16:48,952] Trial 7 finished with value: 1.2719100713729858 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 32, 'lr': 0.009263129814658464, 'batch_size': 112, 'epochs': 150}. Best is trial 1 with value: 1.0659255882104237.\n",
      "[I 2024-06-13 23:18:57,747] Trial 8 finished with value: 0.9787469208240509 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 32, 'lr': 0.002730160692048019, 'batch_size': 80, 'epochs': 60}. Best is trial 8 with value: 0.9787469208240509.\n",
      "[I 2024-06-13 23:20:17,176] Trial 9 finished with value: 1.6920117139816284 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'lr': 0.005228769369262201, 'batch_size': 448, 'epochs': 106}. Best is trial 8 with value: 0.9787469208240509.\n",
      "[I 2024-06-13 23:23:12,720] Trial 10 finished with value: 0.945140041410923 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'lr': 0.003257112451054223, 'batch_size': 32, 'epochs': 54}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:25:19,324] Trial 11 finished with value: 1.4256768281813021 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'lr': 0.0033564236399174236, 'batch_size': 16, 'epochs': 54}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:26:27,067] Trial 12 finished with value: 1.2405983010927837 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 16, 'lr': 0.0035580142329245837, 'batch_size': 208, 'epochs': 71}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:29:00,083] Trial 13 finished with value: 1.2978696447831612 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 32, 'lr': 0.007190759191132107, 'batch_size': 16, 'epochs': 50}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:30:07,041] Trial 14 finished with value: 1.0327550768852234 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 32, 'lr': 0.0028978669269227736, 'batch_size': 176, 'epochs': 86}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:32:15,024] Trial 15 finished with value: 1.7152894139289856 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 700, 'lr': 0.0012896944501873367, 'batch_size': 304, 'epochs': 65}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:32:49,040] Trial 16 finished with value: 1.3481444478034974 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 700, 'lr': 0.0037404540493194242, 'batch_size': 96, 'epochs': 61}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:32:55,933] Trial 17 finished with value: 1.3847390753882272 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 64, 'lr': 0.006584565303398328, 'batch_size': 64, 'epochs': 90}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:33:37,511] Trial 18 finished with value: 1.2265392541885376 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 512, 'hidden_layer_size_3': 16, 'lr': 0.004260049254934069, 'batch_size': 288, 'epochs': 73}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:34:02,147] Trial 19 finished with value: 1.0810866554578145 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 64, 'lr': 0.002507620055626995, 'batch_size': 160, 'epochs': 65}. Best is trial 10 with value: 0.945140041410923.\n",
      "[I 2024-06-13 23:37:04,453] A new study created in memory with name: no-name-ed290837-1410-49ba-8d2b-8f56be6cd1cc\n",
      "[I 2024-06-13 23:37:40,774] Trial 0 finished with value: 2.337564468383789 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 700, 'lr': 0.009770567896053618, 'batch_size': 464, 'epochs': 142}. Best is trial 0 with value: 2.337564468383789.\n",
      "[I 2024-06-13 23:38:36,204] Trial 1 finished with value: 1.2760212062685579 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 64, 'hidden_layer_size_3': 620, 'lr': 0.002138082786079681, 'batch_size': 16, 'epochs': 63}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:38:41,075] Trial 2 finished with value: 1.9477161169052124 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 256, 'lr': 0.005234904344914384, 'batch_size': 304, 'epochs': 102}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:38:45,507] Trial 3 finished with value: 2.183885872364044 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 16, 'lr': 0.009485623549780466, 'batch_size': 320, 'epochs': 65}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:39:30,388] Trial 4 finished with value: 1.41629296541214 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 620, 'lr': 0.0023884814455000553, 'batch_size': 224, 'epochs': 57}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:40:17,312] Trial 5 finished with value: 1.5017223954200745 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 700, 'lr': 0.007381013509151971, 'batch_size': 112, 'epochs': 121}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:40:22,296] Trial 6 finished with value: 1.97141432762146 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 16, 'lr': 0.002220439891178556, 'batch_size': 368, 'epochs': 70}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:40:40,100] Trial 7 finished with value: 1.6304612358411152 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 620, 'lr': 0.009130986631568475, 'batch_size': 80, 'epochs': 55}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:41:01,695] Trial 8 finished with value: 1.350717544555664 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'lr': 0.004573630274010849, 'batch_size': 368, 'epochs': 61}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:41:14,988] Trial 9 finished with value: 1.4016779065132141 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 700, 'lr': 0.002333606309348107, 'batch_size': 112, 'epochs': 64}. Best is trial 1 with value: 1.2760212062685579.\n",
      "[I 2024-06-13 23:41:46,598] Trial 10 finished with value: 1.2148535019821591 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'lr': 0.0011089891973441821, 'batch_size': 16, 'epochs': 86}. Best is trial 10 with value: 1.2148535019821591.\n",
      "[I 2024-06-13 23:42:42,927] Trial 11 finished with value: 1.0946753897048809 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'lr': 0.0013075652855029842, 'batch_size': 16, 'epochs': 83}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:43:12,111] Trial 12 finished with value: 1.2425731321175892 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'lr': 0.001014109295161887, 'batch_size': 16, 'epochs': 89}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:43:27,567] Trial 13 finished with value: 1.1474737524986267 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 64, 'lr': 0.0037166544612099108, 'batch_size': 192, 'epochs': 87}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:43:49,860] Trial 14 finished with value: 2.1327006816864014 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 620, 'lr': 0.0038738348393770887, 'batch_size': 208, 'epochs': 102}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:45:21,384] Trial 15 finished with value: 1.482132077217102 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 700, 'lr': 0.0036607467985906106, 'batch_size': 176, 'epochs': 82}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:45:49,603] Trial 16 finished with value: 1.6693748235702515 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 128, 'lr': 0.006602138878433035, 'batch_size': 160, 'epochs': 116}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:46:01,059] Trial 17 finished with value: 1.640201449394226 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 620, 'lr': 0.0034929920747854915, 'batch_size': 496, 'epochs': 79}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:46:14,472] Trial 18 finished with value: 1.2988834977149963 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 512, 'lr': 0.006253107768090106, 'batch_size': 256, 'epochs': 115}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:47:34,435] Trial 19 finished with value: 1.1669792036215465 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 700, 'lr': 0.0030950194445732854, 'batch_size': 80, 'epochs': 96}. Best is trial 11 with value: 1.0946753897048809.\n",
      "[I 2024-06-13 23:48:27,927] A new study created in memory with name: no-name-e15a3c3c-bbd5-4a80-8022-fe9ca94fa625\n",
      "[I 2024-06-13 23:48:38,411] Trial 0 finished with value: 1.5895192197390966 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 32, 'lr': 0.0022758653636475333, 'batch_size': 64, 'epochs': 61}. Best is trial 0 with value: 1.5895192197390966.\n",
      "[I 2024-06-13 23:48:46,853] Trial 1 finished with value: 1.5698094367980957 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 32, 'lr': 0.0021397052570841417, 'batch_size': 464, 'epochs': 110}. Best is trial 1 with value: 1.5698094367980957.\n",
      "[I 2024-06-13 23:49:07,238] Trial 2 finished with value: 1.6911185724394662 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 32, 'hidden_layer_size_3': 256, 'lr': 0.006003469561554266, 'batch_size': 32, 'epochs': 52}. Best is trial 1 with value: 1.5698094367980957.\n",
      "[I 2024-06-13 23:49:13,202] Trial 3 finished with value: 1.85597425699234 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 16, 'hidden_layer_size_2': 64, 'lr': 0.0031615336082395323, 'batch_size': 352, 'epochs': 132}. Best is trial 1 with value: 1.5698094367980957.\n",
      "[I 2024-06-13 23:49:17,156] Trial 4 finished with value: 1.613641619682312 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 32, 'hidden_layer_size_1': 64, 'lr': 0.007831207935560865, 'batch_size': 288, 'epochs': 72}. Best is trial 1 with value: 1.5698094367980957.\n",
      "[I 2024-06-13 23:49:39,973] Trial 5 finished with value: 1.3577189445495605 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'lr': 0.004328857784282327, 'batch_size': 96, 'epochs': 83}. Best is trial 5 with value: 1.3577189445495605.\n",
      "[I 2024-06-13 23:49:49,580] Trial 6 finished with value: 2.0105188290278115 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 620, 'lr': 0.0073887004735518685, 'batch_size': 208, 'epochs': 73}. Best is trial 5 with value: 1.3577189445495605.\n",
      "[I 2024-06-13 23:50:15,479] Trial 7 finished with value: 1.4777761101722717 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 64, 'hidden_layer_size_1': 512, 'hidden_layer_size_2': 512, 'lr': 0.0024005450953902075, 'batch_size': 256, 'epochs': 57}. Best is trial 5 with value: 1.3577189445495605.\n",
      "[I 2024-06-13 23:51:16,797] Trial 8 finished with value: 1.283397614955902 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 700, 'lr': 0.0024059565554771143, 'batch_size': 272, 'epochs': 107}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:51:23,623] Trial 9 finished with value: 2.017784684896469 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 16, 'hidden_layer_size_1': 128, 'hidden_layer_size_2': 620, 'lr': 0.00912337359682032, 'batch_size': 128, 'epochs': 139}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:51:36,588] Trial 10 finished with value: 1.7401093244552612 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 16, 'hidden_layer_size_3': 700, 'lr': 0.0010837207004682744, 'batch_size': 512, 'epochs': 107}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:51:56,741] Trial 11 finished with value: 1.5489333073298137 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 256, 'lr': 0.0044144988530603994, 'batch_size': 160, 'epochs': 90}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:53:17,270] Trial 12 finished with value: 2.0513899326324463 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 700, 'hidden_layer_size_1': 700, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 700, 'lr': 0.004499391080310571, 'batch_size': 368, 'epochs': 89}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:53:40,329] Trial 13 finished with value: 1.447839468717575 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 256, 'hidden_layer_size_2': 128, 'hidden_layer_size_3': 32, 'lr': 0.004189224270533041, 'batch_size': 128, 'epochs': 121}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:53:49,782] Trial 14 finished with value: 2.032575309276581 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 256, 'lr': 0.005937526789576429, 'batch_size': 352, 'epochs': 92}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:54:06,322] Trial 15 finished with value: 1.4616177678108215 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 512, 'hidden_layer_size_1': 64, 'hidden_layer_size_2': 256, 'hidden_layer_size_3': 128, 'lr': 0.003357762700359343, 'batch_size': 256, 'epochs': 78}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:54:16,800] Trial 16 finished with value: 1.7455259958902996 and parameters: {'hidden_layer_count': 2, 'hidden_layer_size_0': 256, 'hidden_layer_size_1': 700, 'lr': 0.0012605613739830705, 'batch_size': 192, 'epochs': 120}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:54:35,296] Trial 17 finished with value: 1.5830463369687398 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 620, 'hidden_layer_size_1': 32, 'hidden_layer_size_2': 700, 'lr': 0.005168015803007723, 'batch_size': 80, 'epochs': 102}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:55:26,190] Trial 18 finished with value: 1.3368130326271057 and parameters: {'hidden_layer_count': 3, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'lr': 0.0034000735500639995, 'batch_size': 304, 'epochs': 149}. Best is trial 8 with value: 1.283397614955902.\n",
      "[I 2024-06-13 23:55:54,618] Trial 19 finished with value: 2.2181830406188965 and parameters: {'hidden_layer_count': 4, 'hidden_layer_size_0': 128, 'hidden_layer_size_1': 620, 'hidden_layer_size_2': 620, 'hidden_layer_size_3': 512, 'lr': 0.003403272014705186, 'batch_size': 400, 'epochs': 147}. Best is trial 8 with value: 1.283397614955902.\n"
     ]
    }
   ],
   "source": [
    "%%capture KAN_opt_out\n",
    "\n",
    "import optuna\n",
    "\n",
    "def save_best_KAN(study, input_size, path, SAE=None):\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    kan_layers = [input_size] + [best_trial.params[f'hidden_layer_size_{i}'] for i in range(best_trial.params['hidden_layer_count'])] + [2]\n",
    "    \n",
    "    model = KAN(kan_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if SAE is not None: # Encode training and validation data using the stacked autoencoders in SAE\n",
    "        train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "        val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    else :\n",
    "        train_loader = DataLoader(t_training, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(t_val, batch_size=best_trial.params['batch_size'], shuffle=True)\n",
    "    \n",
    "    val_loss, trained_model = train_KAN(model, train_loader, val_loader, criterion, optimizer, best_trial.params['epochs'])\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "if SEARCH_KAN_FULL:\n",
    "    print('Starting KAN full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: KAN_full_optimize(trial, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 620, f'./models/KAN/full_KAN.pth')\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_FULL')\n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_256 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_256:\n",
    "    print('Starting KAN reduced search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/256_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 256, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 256, f'./models/KAN/256_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_256') \n",
    "\n",
    "print('\\n====================================')\n",
    "print('V results for SEARCH_KAN_REDUCED_128 V')\n",
    "print('====================================\\n')\n",
    "\n",
    "if SEARCH_KAN_REDUCED_128:\n",
    "    print('Starting KAN reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    # Save the encoders for later use\n",
    "    for i, enc in enumerate(encoders):\n",
    "        torch.save(enc.state_dict(), f'./models/KAN/128_encoder_{int(512 * 0.5**i)}.pth')\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: KAN_SAE_optimize(trial, encoders, 128, 'Adam')[0], n_trials=TRIALS_KAN)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "    save_best_KAN(study, 128, f'./models/KAN/128_KAN.pth', encoders)\n",
    "\n",
    "else: print('Skipping SEARCH_KAN_REDUCED_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"./optimization/KAN/KAN optimization - \"\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(KAN_opt_out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation = True\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'MLP': {\n",
    "        'full': './models/MLP/full_MLP.pth',\n",
    "        '256': './models/MLP/256_MLP.pth',\n",
    "        '128': './models/MLP/128_MLP.pth'\n",
    "    },\n",
    "    'KAN': {\n",
    "        'full': './models/KAN/full_KAN.pth',\n",
    "        '256': './models/KAN/256_KAN.pth',\n",
    "        '128': './models/KAN/128_KAN.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "SAE_paths = {\n",
    "    'MLP': {\n",
    "        '256': ['./models/MLP/256_encoder_512.pth', './models/MLP/256_encoder_256.pth'],\n",
    "        '128': ['./models/MLP/128_encoder_512.pth', './models/MLP/128_encoder_256.pth', './models/MLP/128_encoder_128.pth']\n",
    "    },\n",
    "    'KAN': {\n",
    "        '256': ['./models/KAN/256_encoder_512.pth', './models/KAN/256_encoder_256.pth'],\n",
    "        '128': ['./models/KAN/128_encoder_512.pth', './models/KAN/128_encoder_256.pth', './models/KAN/128_encoder_128.pth']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the saved MLP models\n",
    "def load_MLP_model(path, hidden_layer_sizes, dropout_rate, input_size):\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load the saved KAN models\n",
    "def load_KAN_model(path, hidden_layer_sizes, input_size):\n",
    "    kan_layers = [input_size] + hidden_layer_sizes + [2]\n",
    "    model = KAN(kan_layers)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example, if final_size is 128, the SAE will have 512 -> 256 -> 128\n",
    "# So we need to instantiate and load three autoencoders in the correct order\n",
    "# and then load them from the path\n",
    "def load_SAE(paths, final_size):\n",
    "    encoders = []\n",
    "    input_dim = 620\n",
    "    for i, path in enumerate(paths):\n",
    "        current_dim = 512 * 0.5**i\n",
    "        current_dim = int(current_dim)\n",
    "        if current_dim < final_size: # Should not happen, but just in case\n",
    "            break\n",
    "        encoder = Autoencoder(input_dim, current_dim)\n",
    "        encoder.load_state_dict(torch.load(path))\n",
    "        encoder.eval() # Set model to evaluation mode\n",
    "        encoders.append(encoder)\n",
    "\n",
    "        input_dim = current_dim\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "    \n",
    "else:\n",
    "    # TODO: Find a way to get the hyperparameters from before ==> LAYERS AND DROPOUT RATE MUST BE ADDED BY HAND FOR NOW\n",
    "    \n",
    "    # Load the MLP models\n",
    "    mlp_full = load_MLP_model(model_paths['MLP']['full'], [700, 64], 0.20163246481690708, 620)\n",
    "    mlp_256 = load_MLP_model(model_paths['MLP']['256'], [700, 128, 512, 512], 0.3254543738426199, 256)\n",
    "    mlp_128 = load_MLP_model(model_paths['MLP']['128'], [620, 700], 0.27699133072484455, 128)\n",
    "\n",
    "    # Load the KAN models\n",
    "    kan_full = load_KAN_model(model_paths['KAN']['full'], [620, 700, 64, 32], 620)\n",
    "    kan_256 = load_KAN_model(model_paths['KAN']['256'], [32, 620, 64], 256)\n",
    "    kan_128 = load_KAN_model(model_paths['KAN']['128'], [512, 64, 620, 700], 128)\n",
    "    \n",
    "    # Load the SAE models\n",
    "    mlp_SAE_256 = load_SAE(SAE_paths['MLP']['256'], 256)\n",
    "    mlp_SAE_128 = load_SAE(SAE_paths['MLP']['128'], 128)\n",
    "    kan_SAE_256 = load_SAE(SAE_paths['KAN']['256'], 256)\n",
    "    kan_SAE_128 = load_SAE(SAE_paths['KAN']['128'], 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288,)\n",
      "(288,)\n",
      "(288,)\n",
      "(288,)\n",
      "(288,)\n",
      "(288,)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features).cpu().numpy()\n",
    "        euc_distances = np.sqrt(np.sum((predictions - test_labels.cpu().numpy())**2, axis=1))\n",
    "        \n",
    "    return euc_distances\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    # Test tensors are defined previously as X_test_tensor and y_test_tensor\n",
    "    # and we have def stacked_encode_data(data, encoders): to encode the data\n",
    "    \n",
    "    # Encode the test data using the stacked autoencoders\n",
    "    mlp_test_data_encoded_256 = stacked_encode_data(X_test_tensor, mlp_SAE_256)\n",
    "    mlp_test_data_encoded_128 = stacked_encode_data(X_test_tensor, mlp_SAE_128)\n",
    "    kan_test_data_encoded_256 = stacked_encode_data(X_test_tensor, kan_SAE_256)\n",
    "    kan_test_data_encoded_128 = stacked_encode_data(X_test_tensor, kan_SAE_128)\n",
    "    \n",
    "    # Convert the encoded data to PyTorch Tensors\n",
    "    # mlp_test_data_encoded_256 = torch.tensor(mlp_test_data_encoded_256, dtype=torch.float32)\n",
    "    # mlp_test_data_encoded_128 = torch.tensor(mlp_test_data_encoded_128, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_256 = torch.tensor(kan_test_data_encoded_256, dtype=torch.float32)\n",
    "    # kan_test_data_encoded_128 = torch.tensor(kan_test_data_encoded_128, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    mlp_full_distances = evaluate_model(mlp_full, X_test_tensor, y_test_tensor)\n",
    "    mlp_256_distances = evaluate_model(mlp_256, mlp_test_data_encoded_256, y_test_tensor)\n",
    "    mlp_128_distances = evaluate_model(mlp_128, mlp_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    kan_full_distances = evaluate_model(kan_full, X_test_tensor, y_test_tensor)\n",
    "    kan_256_distances = evaluate_model(kan_256, kan_test_data_encoded_256, y_test_tensor)\n",
    "    kan_128_distances = evaluate_model(kan_128, kan_test_data_encoded_128, y_test_tensor)\n",
    "    \n",
    "    # Print the shapes\n",
    "    print(mlp_full_distances.shape)\n",
    "    print(mlp_256_distances.shape)\n",
    "    print(mlp_128_distances.shape)\n",
    "    \n",
    "    print(kan_full_distances.shape)\n",
    "    print(kan_256_distances.shape)\n",
    "    print(kan_128_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_128 - Median: 1.496880054473877 - 25th percentile: 0.9084416776895523 - 75th percentile: 2.4593050479888916 - Outliers: 7\n",
      "MLP_256 - Median: 1.6154813766479492 - 25th percentile: 1.0314720273017883 - 75th percentile: 2.55817848443985 - Outliers: 7\n",
      "MLP_full - Median: 1.7346258163452148 - 25th percentile: 1.1176683008670807 - 75th percentile: 2.4072445034980774 - Outliers: 12\n",
      "KAN_128 - Median: 1.1787176132202148 - 25th percentile: 0.6539593487977982 - 75th percentile: 2.0730645656585693 - Outliers: 18\n",
      "KAN_256 - Median: 0.6822882890701294 - 25th percentile: 0.39600562304258347 - 75th percentile: 1.2484205961227417 - Outliers: 40\n",
      "KAN_full - Median: 0.60174560546875 - 25th percentile: 0.3249804228544235 - 75th percentile: 1.1206862330436707 - Outliers: 34\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not perform_evaluation:\n",
    "    print('Evaluation is disabled')\n",
    "else:\n",
    "    data = {\n",
    "        'MLP_128': mlp_128_distances,\n",
    "        'MLP_256': mlp_256_distances,\n",
    "        'MLP_full': mlp_full_distances,\n",
    "        'KAN_128': kan_128_distances,\n",
    "        'KAN_256': kan_256_distances,\n",
    "        'KAN_full': kan_full_distances\n",
    "    }\n",
    "    \n",
    "    # Create boxplots for the results\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].boxplot([data['MLP_full'], data['KAN_full']], labels=['MLP_full', 'KAN_full'])\n",
    "    axes[0].set_title('Full models')\n",
    "    \n",
    "    axes[1].boxplot([data['MLP_256'], data['KAN_256']], labels=['MLP_256', 'KAN_256'])\n",
    "    axes[1].set_title('Reduced models (256)')\n",
    "    \n",
    "    axes[2].boxplot([data['MLP_128'], data['KAN_128']], labels=['MLP_128', 'KAN_128'])\n",
    "    axes[2].set_title('Reduced models (128)')\n",
    "    \n",
    "    axes[0].set_ylabel('Euclidean Distance (m)')\n",
    "    \n",
    "    # Enable light grid\n",
    "    # for ax in axes:\n",
    "    #     ax.grid(True)\n",
    "\n",
    "    # Print the median, 25ht and 75th percentiles, and number of outliers for each model\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        print(f'{key} - Median: {np.median(value)} - 25th percentile: {np.percentile(value, 25)} - 75th percentile: {np.percentile(value, 75)} - Outliers: {np.sum(value > np.percentile(value, 75) + 1.5*(np.percentile(value, 75) - np.percentile(value, 25)))}')\n",
    "        \n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
