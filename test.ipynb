{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "filename = now.strftime(\"%Y-%m-%d-%H-%M-%S\") + '.txt'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def load_dataset(paths: list[str], num_APs: int, floor: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "    \n",
    "    # Load the data from all specified paths\n",
    "    df_x = pd.concat([pd.read_csv(path + 'rss.csv', names=list_of_APs) for path in paths])\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_y = pd.concat([pd.read_csv(path + 'crd.csv', names=['x', 'y', 'floor']) for path in paths])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    # Get indexes of the specified floor\n",
    "    floor_indexes = df_y[df_y['floor'] == floor].index\n",
    "    \n",
    "    # Keep only the rows with the specified floor for both x and y, and reset the indexes\n",
    "    df_x = df_x.loc[floor_indexes]\n",
    "    df_x = df_x.reset_index(drop=True)\n",
    "    \n",
    "    # For df_y, also remove the floor column\n",
    "    df_y = df_y.loc[floor_indexes]\n",
    "    df_y = df_y.drop(columns=['floor'])\n",
    "    df_y = df_y.reset_index(drop=True)\n",
    "    \n",
    "    return df_x, df_y\n",
    "\n",
    "def preprocess_rssi(df_rssi: pd.DataFrame, scaling_strategy: Scaling):\n",
    "    # Flattened dataset for easy searching\n",
    "    flattened = df_rssi.values.flatten()\n",
    "    \n",
    "    # Minimum rssi found\n",
    "    min_rssi = np.min(flattened)\n",
    "    print(\"Minimum RSSI: \", min_rssi)\n",
    "    \n",
    "    # Find biggest multiple of 10 smaller than min_rssi\n",
    "    replacement_rssi = np.floor((min_rssi - 1) / 10) * 10\n",
    "    print(\"Replacement value\", replacement_rssi)\n",
    "    \n",
    "    # Replace all 100 values with replacement_rssi\n",
    "    df_rssi = df_rssi.replace(100, replacement_rssi)\n",
    "    flattened = df_rssi.values.flatten() # Update flattened since we changed the dataframe\n",
    "    \n",
    "    # Standardization part\n",
    "    if scaling_strategy == Scaling.INDEPENDENT: # Might not work\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(df_rssi)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=df_rssi.columns)\n",
    "        df_rssi = df_scaled_rss\n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        df_rssi = (df_rssi - global_mean) / global_std\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df_rssi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RSSI:  -96\n",
      "Replacement value -100.0\n",
      "Explained variance by 128 components: 1.00\n",
      "Explained variance by 64 components: 1.00\n",
      "Shape of train_x_pca_128: (288, 128)\n",
      "Shape of train_x_pca_64: (288, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_paths = [\n",
    "    './data/V1.0/01/trn01'\n",
    "]\n",
    "num_APs = 448\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "df_x, df_y = load_dataset(data_paths, num_APs, floor)\n",
    "\n",
    "train_x_scaled = preprocess_rssi(df_x, scaling_strategy)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 128 components\n",
    "pca_128 = PCA(n_components=128)\n",
    "train_x_pca_128 = pca_128.fit_transform(train_x_scaled)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 64 components\n",
    "pca_64 = PCA(n_components=64)\n",
    "train_x_pca_64 = pca_64.fit_transform(train_x_scaled)\n",
    "\n",
    "# Inspect the explained variance ratio\n",
    "explained_variance_128 = np.sum(pca_128.explained_variance_ratio_)\n",
    "explained_variance_64 = np.sum(pca_64.explained_variance_ratio_)\n",
    "\n",
    "print(f'Explained variance by 128 components: {explained_variance_128:.2f}')\n",
    "print(f'Explained variance by 64 components: {explained_variance_64:.2f}')\n",
    "\n",
    "# Outputs the transformed datasets\n",
    "print('Shape of train_x_pca_128:', train_x_pca_128.shape)\n",
    "print('Shape of train_x_pca_64:', train_x_pca_64.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
