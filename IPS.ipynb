{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "\n",
    "df_train_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_train_x = df_train_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_train_y = df_train_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_train_full:', df_train_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01'] # 01 Corresponds to the same locations as the training set\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_val_full: (864, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01']\n",
    "sets = ['02', '03', '04']\n",
    "type = DatasetType.VALIDATION\n",
    "\n",
    "df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-Input Networks\n",
    "These networks take the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
