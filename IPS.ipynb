{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "\n",
    "df_train_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_train_x = df_train_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_train_y = df_train_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_train_full:', df_train_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01'] # 01 Corresponds to the same locations as the training set\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_val_full: (864, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01']\n",
    "sets = ['02', '03', '04']\n",
    "type = DatasetType.VALIDATION\n",
    "\n",
    "df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-Input Networks\n",
    "These networks take the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_MLP_FULL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPfull(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate):\n",
    "        super(MLPfull, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = 620\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_full(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "            loss.backward() # Perform backpropagation\n",
    "            optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return val_loss/len(val_loader) # Return the average validation loss for final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:43:24,394] A new study created in memory with name: no-name-19746cf3-d649-4b1f-b404-e13673723da2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP full grid search\n",
      "Epoch 1/141 - Avg Training Loss: 461.39050242106117 - Avg Validation Loss: 22.01729170481364\n",
      "Epoch 2/141 - Avg Training Loss: 45.4310119205051 - Avg Validation Loss: 33.11333988330983\n",
      "Epoch 3/141 - Avg Training Loss: 35.277532725863985 - Avg Validation Loss: 90.47611377857349\n",
      "Epoch 4/141 - Avg Training Loss: 25.320383008321127 - Avg Validation Loss: 66.91950988769531\n",
      "Epoch 5/141 - Avg Training Loss: 17.478486442565917 - Avg Validation Loss: 28.29524591233995\n",
      "Epoch 6/141 - Avg Training Loss: 11.964862166510688 - Avg Validation Loss: 13.938885653460467\n",
      "Epoch 7/141 - Avg Training Loss: 10.634931786855061 - Avg Validation Loss: 10.84793703644364\n",
      "Epoch 8/141 - Avg Training Loss: 9.95594023598565 - Avg Validation Loss: 10.150906280235008\n",
      "Epoch 9/141 - Avg Training Loss: 9.162545511457655 - Avg Validation Loss: 7.224841541714138\n",
      "Epoch 10/141 - Avg Training Loss: 9.530191527472603 - Avg Validation Loss: 8.00630467026322\n",
      "Epoch 11/141 - Avg Training Loss: 8.969494893815781 - Avg Validation Loss: 9.249808117195412\n",
      "Epoch 12/141 - Avg Training Loss: 8.998698870340982 - Avg Validation Loss: 7.697783576117621\n",
      "Epoch 13/141 - Avg Training Loss: 8.49772311316596 - Avg Validation Loss: 7.606666935814752\n",
      "Epoch 14/141 - Avg Training Loss: 8.834777386983236 - Avg Validation Loss: 8.27228809286047\n",
      "Epoch 15/141 - Avg Training Loss: 9.083920680152046 - Avg Validation Loss: 6.312954761363842\n",
      "Epoch 16/141 - Avg Training Loss: 9.008573065863715 - Avg Validation Loss: 7.903126540007414\n",
      "Epoch 17/141 - Avg Training Loss: 8.699773608313667 - Avg Validation Loss: 5.893770164913601\n",
      "Epoch 18/141 - Avg Training Loss: 9.512334272596572 - Avg Validation Loss: 8.733187163317645\n",
      "Epoch 19/141 - Avg Training Loss: 8.740808614095052 - Avg Validation Loss: 7.268710524947555\n",
      "Epoch 20/141 - Avg Training Loss: 8.521281592051189 - Avg Validation Loss: 6.399726373177987\n",
      "Epoch 21/141 - Avg Training Loss: 8.414245075649685 - Avg Validation Loss: 5.442710505591498\n",
      "Epoch 22/141 - Avg Training Loss: 9.010212951236301 - Avg Validation Loss: 5.689495263276277\n",
      "Epoch 23/141 - Avg Training Loss: 9.232370747460259 - Avg Validation Loss: 6.231055983790645\n",
      "Epoch 24/141 - Avg Training Loss: 8.834876039293077 - Avg Validation Loss: 6.506743201503047\n",
      "Epoch 25/141 - Avg Training Loss: 8.824314339955647 - Avg Validation Loss: 7.250549899207221\n",
      "Epoch 26/141 - Avg Training Loss: 8.860836590660943 - Avg Validation Loss: 5.796574363002071\n",
      "Epoch 27/141 - Avg Training Loss: 8.45700905058119 - Avg Validation Loss: 6.822800389042607\n",
      "Epoch 28/141 - Avg Training Loss: 10.151175753275554 - Avg Validation Loss: 5.9774278711389615\n",
      "Epoch 29/141 - Avg Training Loss: 9.508420499165853 - Avg Validation Loss: 6.457368656441018\n",
      "Epoch 30/141 - Avg Training Loss: 9.892850250667996 - Avg Validation Loss: 6.803281448505543\n",
      "Epoch 31/141 - Avg Training Loss: 9.93136117723253 - Avg Validation Loss: 7.2021890569616245\n",
      "Epoch 32/141 - Avg Training Loss: 10.35328090455797 - Avg Validation Loss: 6.588228896812156\n",
      "Epoch 33/141 - Avg Training Loss: 10.370739852057563 - Avg Validation Loss: 7.706584382940222\n",
      "Epoch 34/141 - Avg Training Loss: 10.773633681403266 - Avg Validation Loss: 7.5466413497924805\n",
      "Epoch 35/141 - Avg Training Loss: 10.206958474053277 - Avg Validation Loss: 6.3599048720465765\n",
      "Epoch 36/141 - Avg Training Loss: 9.532432704501682 - Avg Validation Loss: 7.159538728219491\n",
      "Epoch 37/141 - Avg Training Loss: 9.585836166805692 - Avg Validation Loss: 7.039145469665527\n",
      "Epoch 38/141 - Avg Training Loss: 9.611831092834473 - Avg Validation Loss: 7.859600490993923\n",
      "Epoch 39/141 - Avg Training Loss: 9.191712845696344 - Avg Validation Loss: 7.172150082058376\n",
      "Epoch 40/141 - Avg Training Loss: 10.028015963236491 - Avg Validation Loss: 7.408480343995271\n",
      "Epoch 41/141 - Avg Training Loss: 10.622384431627061 - Avg Validation Loss: 7.650487581888835\n",
      "Epoch 42/141 - Avg Training Loss: 10.670795885721843 - Avg Validation Loss: 7.143801441899052\n",
      "Epoch 43/141 - Avg Training Loss: 11.578996679517958 - Avg Validation Loss: 10.74472932462339\n",
      "Epoch 44/141 - Avg Training Loss: 12.489799732632108 - Avg Validation Loss: 9.315802026678014\n",
      "Epoch 45/141 - Avg Training Loss: 11.347645452287463 - Avg Validation Loss: 9.187664367534497\n",
      "Epoch 46/141 - Avg Training Loss: 12.354514164394802 - Avg Validation Loss: 8.974855140403465\n",
      "Epoch 47/141 - Avg Training Loss: 11.972223832872178 - Avg Validation Loss: 9.320257204550284\n",
      "Epoch 48/141 - Avg Training Loss: 11.453100395202636 - Avg Validation Loss: 8.757009965402109\n",
      "Epoch 49/141 - Avg Training Loss: 10.705886766645644 - Avg Validation Loss: 8.526003148820665\n",
      "Epoch 50/141 - Avg Training Loss: 10.934345605638292 - Avg Validation Loss: 8.879129286165591\n",
      "Epoch 51/141 - Avg Training Loss: 10.95909506479899 - Avg Validation Loss: 8.561605700740108\n",
      "Epoch 52/141 - Avg Training Loss: 11.071477911207412 - Avg Validation Loss: 8.150770240359837\n",
      "Epoch 53/141 - Avg Training Loss: 10.4079286787245 - Avg Validation Loss: 8.69083547592163\n",
      "Epoch 54/141 - Avg Training Loss: 10.418724505106608 - Avg Validation Loss: 8.192414884214047\n",
      "Epoch 55/141 - Avg Training Loss: 10.972734430101182 - Avg Validation Loss: 9.141030965027985\n",
      "Epoch 56/141 - Avg Training Loss: 11.513010745578342 - Avg Validation Loss: 9.137046813964844\n",
      "Epoch 57/141 - Avg Training Loss: 11.090730328030057 - Avg Validation Loss: 9.241035408443874\n",
      "Epoch 58/141 - Avg Training Loss: 10.7369828859965 - Avg Validation Loss: 8.84009693287037\n",
      "Epoch 59/141 - Avg Training Loss: 10.906002680460611 - Avg Validation Loss: 9.195830451117622\n",
      "Epoch 60/141 - Avg Training Loss: 11.029039425320095 - Avg Validation Loss: 8.738416971983733\n",
      "Epoch 61/141 - Avg Training Loss: 12.764971023135715 - Avg Validation Loss: 9.12029234568278\n",
      "Epoch 62/141 - Avg Training Loss: 11.40282359653049 - Avg Validation Loss: 8.702761208569562\n",
      "Epoch 63/141 - Avg Training Loss: 11.541997061835396 - Avg Validation Loss: 9.042271984948052\n",
      "Epoch 64/141 - Avg Training Loss: 11.004105801052518 - Avg Validation Loss: 8.846129488061976\n",
      "Epoch 65/141 - Avg Training Loss: 10.990232181549072 - Avg Validation Loss: 9.080593744913736\n",
      "Epoch 66/141 - Avg Training Loss: 11.429631487528484 - Avg Validation Loss: 9.502812102988914\n",
      "Epoch 67/141 - Avg Training Loss: 10.99582912656996 - Avg Validation Loss: 8.955061541663277\n",
      "Epoch 68/141 - Avg Training Loss: 11.032500902811686 - Avg Validation Loss: 9.241362236164234\n",
      "Epoch 69/141 - Avg Training Loss: 11.49659056133694 - Avg Validation Loss: 9.39853231995194\n",
      "Epoch 70/141 - Avg Training Loss: 10.585474162631565 - Avg Validation Loss: 8.779107394041839\n",
      "Epoch 71/141 - Avg Training Loss: 10.896902063157823 - Avg Validation Loss: 8.913522490748653\n",
      "Epoch 72/141 - Avg Training Loss: 10.614735137091742 - Avg Validation Loss: 9.338380636992278\n",
      "Epoch 73/141 - Avg Training Loss: 10.979333029852974 - Avg Validation Loss: 8.949973230008725\n",
      "Epoch 74/141 - Avg Training Loss: 11.458162053426106 - Avg Validation Loss: 9.108508675186723\n",
      "Epoch 75/141 - Avg Training Loss: 10.783356666564941 - Avg Validation Loss: 9.11740020469383\n",
      "Epoch 76/141 - Avg Training Loss: 15.103494580586752 - Avg Validation Loss: 9.568144233138472\n",
      "Epoch 77/141 - Avg Training Loss: 12.148313585917155 - Avg Validation Loss: 9.737323160524722\n",
      "Epoch 78/141 - Avg Training Loss: 12.441063584221734 - Avg Validation Loss: 9.605372058020698\n",
      "Epoch 79/141 - Avg Training Loss: 11.979480489095051 - Avg Validation Loss: 11.087159050835503\n",
      "Epoch 80/141 - Avg Training Loss: 12.005207061767578 - Avg Validation Loss: 9.815129968855116\n",
      "Epoch 81/141 - Avg Training Loss: 11.793385124206543 - Avg Validation Loss: 9.747920354207357\n",
      "Epoch 82/141 - Avg Training Loss: 11.543357955084907 - Avg Validation Loss: 9.8272044217145\n",
      "Epoch 83/141 - Avg Training Loss: 11.053248638576932 - Avg Validation Loss: 9.699092352831805\n",
      "Epoch 84/141 - Avg Training Loss: 11.744421662224664 - Avg Validation Loss: 9.182317839728462\n",
      "Epoch 85/141 - Avg Training Loss: 10.913346417744954 - Avg Validation Loss: 9.098776287502712\n",
      "Epoch 86/141 - Avg Training Loss: 10.894399706522623 - Avg Validation Loss: 8.965451929304335\n",
      "Epoch 87/141 - Avg Training Loss: 11.290614636739095 - Avg Validation Loss: 9.784775892893473\n",
      "Epoch 88/141 - Avg Training Loss: 11.298243437872992 - Avg Validation Loss: 9.096727865713614\n",
      "Epoch 89/141 - Avg Training Loss: 11.06303440729777 - Avg Validation Loss: 8.903338962131077\n",
      "Epoch 90/141 - Avg Training Loss: 11.196093983120388 - Avg Validation Loss: 9.237682395511204\n",
      "Epoch 91/141 - Avg Training Loss: 10.975028663211399 - Avg Validation Loss: 9.14314130500511\n",
      "Epoch 92/141 - Avg Training Loss: 10.941001828511556 - Avg Validation Loss: 8.876025606084752\n",
      "Epoch 93/141 - Avg Training Loss: 10.962189780341255 - Avg Validation Loss: 9.344086187857169\n",
      "Epoch 94/141 - Avg Training Loss: 11.223043823242188 - Avg Validation Loss: 9.472138457828098\n",
      "Epoch 95/141 - Avg Training Loss: 11.384517108069526 - Avg Validation Loss: 10.379720935115108\n",
      "Epoch 96/141 - Avg Training Loss: 14.10887991587321 - Avg Validation Loss: 11.125441374602142\n",
      "Epoch 97/141 - Avg Training Loss: 12.600962850782606 - Avg Validation Loss: 9.65396859910753\n",
      "Epoch 98/141 - Avg Training Loss: 11.425177552964952 - Avg Validation Loss: 8.81403832965427\n",
      "Epoch 99/141 - Avg Training Loss: 11.605411550733779 - Avg Validation Loss: 8.955550635302508\n",
      "Epoch 100/141 - Avg Training Loss: 10.855558374192979 - Avg Validation Loss: 8.861418988969591\n",
      "Epoch 101/141 - Avg Training Loss: 10.739027214050292 - Avg Validation Loss: 8.931412308304399\n",
      "Epoch 102/141 - Avg Training Loss: 10.905431281195746 - Avg Validation Loss: 8.83515414485225\n",
      "Epoch 103/141 - Avg Training Loss: 11.018729125128852 - Avg Validation Loss: 8.832514497968885\n",
      "Epoch 104/141 - Avg Training Loss: 11.051120705074734 - Avg Validation Loss: 8.97745994285301\n",
      "Epoch 105/141 - Avg Training Loss: 11.063239924112956 - Avg Validation Loss: 9.24207971714161\n",
      "Epoch 106/141 - Avg Training Loss: 11.153360642327202 - Avg Validation Loss: 9.236822428526702\n",
      "Epoch 107/141 - Avg Training Loss: 11.359720590379503 - Avg Validation Loss: 10.681076173429135\n",
      "Epoch 108/141 - Avg Training Loss: 12.263619507683648 - Avg Validation Loss: 9.557934142925122\n",
      "Epoch 109/141 - Avg Training Loss: 29.698000865512423 - Avg Validation Loss: 17.531355928491664\n",
      "Epoch 110/141 - Avg Training Loss: 15.740498797098796 - Avg Validation Loss: 9.592344760894775\n",
      "Epoch 111/141 - Avg Training Loss: 13.999910990397135 - Avg Validation Loss: 8.739074971940783\n",
      "Epoch 112/141 - Avg Training Loss: 12.35279752943251 - Avg Validation Loss: 8.84015002074065\n",
      "Epoch 113/141 - Avg Training Loss: 12.374603271484375 - Avg Validation Loss: 8.92260111702813\n",
      "Epoch 114/141 - Avg Training Loss: 20.72795761956109 - Avg Validation Loss: 9.063616417072437\n",
      "Epoch 115/141 - Avg Training Loss: 13.240352143181696 - Avg Validation Loss: 9.132186395150644\n",
      "Epoch 116/141 - Avg Training Loss: 13.316912990146212 - Avg Validation Loss: 9.47499727319788\n",
      "Epoch 117/141 - Avg Training Loss: 13.261954138014051 - Avg Validation Loss: 10.52423565476029\n",
      "Epoch 118/141 - Avg Training Loss: 12.710835923088922 - Avg Validation Loss: 9.236968199412027\n",
      "Epoch 119/141 - Avg Training Loss: 12.14954007466634 - Avg Validation Loss: 9.063564636089184\n",
      "Epoch 120/141 - Avg Training Loss: 11.722106732262505 - Avg Validation Loss: 9.645536828924108\n",
      "Epoch 121/141 - Avg Training Loss: 11.845693100823297 - Avg Validation Loss: 9.374956183963352\n",
      "Epoch 122/141 - Avg Training Loss: 11.653673595852322 - Avg Validation Loss: 9.0594113667806\n",
      "Epoch 123/141 - Avg Training Loss: 11.665159013536242 - Avg Validation Loss: 9.01544678652728\n",
      "Epoch 124/141 - Avg Training Loss: 11.910725402832032 - Avg Validation Loss: 9.02567031648424\n",
      "Epoch 125/141 - Avg Training Loss: 11.848591889275445 - Avg Validation Loss: 9.056093215942383\n",
      "Epoch 126/141 - Avg Training Loss: 11.875388908386231 - Avg Validation Loss: 9.280664232042101\n",
      "Epoch 127/141 - Avg Training Loss: 23.12185548146566 - Avg Validation Loss: 14.871465435734502\n",
      "Epoch 128/141 - Avg Training Loss: 14.18532517751058 - Avg Validation Loss: 9.33943561271385\n",
      "Epoch 129/141 - Avg Training Loss: 12.63795666164822 - Avg Validation Loss: 8.912544038560656\n",
      "Epoch 130/141 - Avg Training Loss: 12.91351146697998 - Avg Validation Loss: 9.17261305561772\n",
      "Epoch 131/141 - Avg Training Loss: 12.491812494066027 - Avg Validation Loss: 9.301126409459997\n",
      "Epoch 132/141 - Avg Training Loss: 12.087975396050346 - Avg Validation Loss: 9.84043418036567\n",
      "Epoch 133/141 - Avg Training Loss: 13.293315739101834 - Avg Validation Loss: 11.35247619063766\n",
      "Epoch 134/141 - Avg Training Loss: 13.085794936286078 - Avg Validation Loss: 10.659898316418683\n",
      "Epoch 135/141 - Avg Training Loss: 12.936117574903701 - Avg Validation Loss: 10.311880712155942\n",
      "Epoch 136/141 - Avg Training Loss: 12.879969469706218 - Avg Validation Loss: 10.13386653970789\n",
      "Epoch 137/141 - Avg Training Loss: 12.85914012061225 - Avg Validation Loss: 10.363866558781377\n",
      "Epoch 138/141 - Avg Training Loss: 12.41591050889757 - Avg Validation Loss: 9.944106313917372\n",
      "Epoch 139/141 - Avg Training Loss: 12.032251167297364 - Avg Validation Loss: 9.151247642658374\n",
      "Epoch 140/141 - Avg Training Loss: 11.863972112867568 - Avg Validation Loss: 9.691090760407624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:10,884] Trial 0 finished with value: 9.527064535352919 and parameters: {'hidden_layer_size': 700, 'hidden_layer_count': 4, 'dropout_rate': 0.4049990911685759, 'lr': 0.009976921697034229, 'batch_size': 32, 'epochs': 141}. Best is trial 0 with value: 9.527064535352919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/141 - Avg Training Loss: 11.679001596238878 - Avg Validation Loss: 9.527064535352919\n",
      "Finished Training\n",
      "Epoch 1/87 - Avg Training Loss: 161.10828590393066 - Avg Validation Loss: 41.00327014923096\n",
      "Epoch 2/87 - Avg Training Loss: 40.67371400197347 - Avg Validation Loss: 40.88878345489502\n",
      "Epoch 3/87 - Avg Training Loss: 29.235767364501953 - Avg Validation Loss: 11.918127536773682\n",
      "Epoch 4/87 - Avg Training Loss: 25.303975741068523 - Avg Validation Loss: 24.144399642944336\n",
      "Epoch 5/87 - Avg Training Loss: 22.083481788635254 - Avg Validation Loss: 7.736772537231445\n",
      "Epoch 6/87 - Avg Training Loss: 19.364613850911457 - Avg Validation Loss: 12.721908330917358\n",
      "Epoch 7/87 - Avg Training Loss: 17.003052552541096 - Avg Validation Loss: 7.72745144367218\n",
      "Epoch 8/87 - Avg Training Loss: 15.926359494527182 - Avg Validation Loss: 9.806276559829712\n",
      "Epoch 9/87 - Avg Training Loss: 15.330939292907715 - Avg Validation Loss: 9.051262140274048\n",
      "Epoch 10/87 - Avg Training Loss: 13.962060133616129 - Avg Validation Loss: 6.29178261756897\n",
      "Epoch 11/87 - Avg Training Loss: 14.241145769755045 - Avg Validation Loss: 8.734429359436035\n",
      "Epoch 12/87 - Avg Training Loss: 12.894671440124512 - Avg Validation Loss: 8.52166485786438\n",
      "Epoch 13/87 - Avg Training Loss: 12.900772253672281 - Avg Validation Loss: 6.880040884017944\n",
      "Epoch 14/87 - Avg Training Loss: 12.338369051615397 - Avg Validation Loss: 7.740682244300842\n",
      "Epoch 15/87 - Avg Training Loss: 12.578183015187582 - Avg Validation Loss: 6.544524073600769\n",
      "Epoch 16/87 - Avg Training Loss: 12.064942200978598 - Avg Validation Loss: 6.525528311729431\n",
      "Epoch 17/87 - Avg Training Loss: 12.336485068003336 - Avg Validation Loss: 7.028916358947754\n",
      "Epoch 18/87 - Avg Training Loss: 11.948050816853842 - Avg Validation Loss: 5.94873583316803\n",
      "Epoch 19/87 - Avg Training Loss: 11.942427476247152 - Avg Validation Loss: 7.037220001220703\n",
      "Epoch 20/87 - Avg Training Loss: 11.925750414530436 - Avg Validation Loss: 9.232080936431885\n",
      "Epoch 21/87 - Avg Training Loss: 11.768396218617758 - Avg Validation Loss: 7.98872709274292\n",
      "Epoch 22/87 - Avg Training Loss: 11.54572327931722 - Avg Validation Loss: 8.615970849990845\n",
      "Epoch 23/87 - Avg Training Loss: 11.485260804494223 - Avg Validation Loss: 7.472346186637878\n",
      "Epoch 24/87 - Avg Training Loss: 11.737045447031656 - Avg Validation Loss: 6.321575164794922\n",
      "Epoch 25/87 - Avg Training Loss: 11.529430389404297 - Avg Validation Loss: 5.7634419202804565\n",
      "Epoch 26/87 - Avg Training Loss: 11.887738545735678 - Avg Validation Loss: 7.473192930221558\n",
      "Epoch 27/87 - Avg Training Loss: 11.827330271402994 - Avg Validation Loss: 5.632921576499939\n",
      "Epoch 28/87 - Avg Training Loss: 10.972234408060709 - Avg Validation Loss: 5.1579426527023315\n",
      "Epoch 29/87 - Avg Training Loss: 11.191260178883871 - Avg Validation Loss: 7.000688791275024\n",
      "Epoch 30/87 - Avg Training Loss: 11.638116677602133 - Avg Validation Loss: 10.296822786331177\n",
      "Epoch 31/87 - Avg Training Loss: 11.990010420481363 - Avg Validation Loss: 9.47137999534607\n",
      "Epoch 32/87 - Avg Training Loss: 11.031554222106934 - Avg Validation Loss: 6.8324466943740845\n",
      "Epoch 33/87 - Avg Training Loss: 10.885826110839844 - Avg Validation Loss: 7.45269513130188\n",
      "Epoch 34/87 - Avg Training Loss: 11.90969959894816 - Avg Validation Loss: 7.369663238525391\n",
      "Epoch 35/87 - Avg Training Loss: 11.119288285573324 - Avg Validation Loss: 7.045605897903442\n",
      "Epoch 36/87 - Avg Training Loss: 11.326023578643799 - Avg Validation Loss: 7.408229231834412\n",
      "Epoch 37/87 - Avg Training Loss: 11.411262194315592 - Avg Validation Loss: 5.340195059776306\n",
      "Epoch 38/87 - Avg Training Loss: 11.368732770284018 - Avg Validation Loss: 6.344305038452148\n",
      "Epoch 39/87 - Avg Training Loss: 10.732328255971273 - Avg Validation Loss: 7.238647222518921\n",
      "Epoch 40/87 - Avg Training Loss: 10.708248456319174 - Avg Validation Loss: 6.076942682266235\n",
      "Epoch 41/87 - Avg Training Loss: 10.823384443918863 - Avg Validation Loss: 6.696522355079651\n",
      "Epoch 42/87 - Avg Training Loss: 11.244880040486654 - Avg Validation Loss: 7.7472933530807495\n",
      "Epoch 43/87 - Avg Training Loss: 11.062408447265625 - Avg Validation Loss: 8.236491799354553\n",
      "Epoch 44/87 - Avg Training Loss: 10.663342793782553 - Avg Validation Loss: 6.431262016296387\n",
      "Epoch 45/87 - Avg Training Loss: 11.016315142313639 - Avg Validation Loss: 6.045208930969238\n",
      "Epoch 46/87 - Avg Training Loss: 10.769269307454428 - Avg Validation Loss: 8.59667420387268\n",
      "Epoch 47/87 - Avg Training Loss: 10.928675015767416 - Avg Validation Loss: 7.236082553863525\n",
      "Epoch 48/87 - Avg Training Loss: 11.232398192087809 - Avg Validation Loss: 9.542576551437378\n",
      "Epoch 49/87 - Avg Training Loss: 10.663769404093424 - Avg Validation Loss: 9.322057247161865\n",
      "Epoch 50/87 - Avg Training Loss: 11.252373854319254 - Avg Validation Loss: 10.035212278366089\n",
      "Epoch 51/87 - Avg Training Loss: 11.271573543548584 - Avg Validation Loss: 7.224396347999573\n",
      "Epoch 52/87 - Avg Training Loss: 10.92818514506022 - Avg Validation Loss: 6.666477560997009\n",
      "Epoch 53/87 - Avg Training Loss: 10.986420154571533 - Avg Validation Loss: 6.3528443574905396\n",
      "Epoch 54/87 - Avg Training Loss: 10.76599407196045 - Avg Validation Loss: 6.376055598258972\n",
      "Epoch 55/87 - Avg Training Loss: 10.934503396352133 - Avg Validation Loss: 7.945356488227844\n",
      "Epoch 56/87 - Avg Training Loss: 11.118353048960367 - Avg Validation Loss: 7.596054553985596\n",
      "Epoch 57/87 - Avg Training Loss: 10.856477578481039 - Avg Validation Loss: 7.2554484605789185\n",
      "Epoch 58/87 - Avg Training Loss: 11.146456241607666 - Avg Validation Loss: 7.070581316947937\n",
      "Epoch 59/87 - Avg Training Loss: 11.1287792523702 - Avg Validation Loss: 6.5537132024765015\n",
      "Epoch 60/87 - Avg Training Loss: 10.888317902882894 - Avg Validation Loss: 7.224281907081604\n",
      "Epoch 61/87 - Avg Training Loss: 10.401721636454264 - Avg Validation Loss: 6.4591392278671265\n",
      "Epoch 62/87 - Avg Training Loss: 10.803368409474691 - Avg Validation Loss: 7.3869324922561646\n",
      "Epoch 63/87 - Avg Training Loss: 10.642762978871664 - Avg Validation Loss: 7.614338755607605\n",
      "Epoch 64/87 - Avg Training Loss: 10.887562115987143 - Avg Validation Loss: 8.870713233947754\n",
      "Epoch 65/87 - Avg Training Loss: 10.395021756490072 - Avg Validation Loss: 9.323469400405884\n",
      "Epoch 66/87 - Avg Training Loss: 11.061386108398438 - Avg Validation Loss: 7.845272421836853\n",
      "Epoch 67/87 - Avg Training Loss: 11.135512034098307 - Avg Validation Loss: 8.70617961883545\n",
      "Epoch 68/87 - Avg Training Loss: 10.101287523905436 - Avg Validation Loss: 7.662312865257263\n",
      "Epoch 69/87 - Avg Training Loss: 10.581937789916992 - Avg Validation Loss: 7.587484121322632\n",
      "Epoch 70/87 - Avg Training Loss: 10.760739008585611 - Avg Validation Loss: 8.568266868591309\n",
      "Epoch 71/87 - Avg Training Loss: 11.301729043324789 - Avg Validation Loss: 9.671325445175171\n",
      "Epoch 72/87 - Avg Training Loss: 10.780519008636475 - Avg Validation Loss: 10.09921383857727\n",
      "Epoch 73/87 - Avg Training Loss: 10.869454065958658 - Avg Validation Loss: 8.375358939170837\n",
      "Epoch 74/87 - Avg Training Loss: 10.33995262781779 - Avg Validation Loss: 6.742278337478638\n",
      "Epoch 75/87 - Avg Training Loss: 10.268914540608725 - Avg Validation Loss: 7.033464670181274\n",
      "Epoch 76/87 - Avg Training Loss: 9.686360677083334 - Avg Validation Loss: 5.899424433708191\n",
      "Epoch 77/87 - Avg Training Loss: 10.01574675242106 - Avg Validation Loss: 5.026300549507141\n",
      "Epoch 78/87 - Avg Training Loss: 10.428408781687418 - Avg Validation Loss: 7.263051390647888\n",
      "Epoch 79/87 - Avg Training Loss: 9.997245152791342 - Avg Validation Loss: 6.144299268722534\n",
      "Epoch 80/87 - Avg Training Loss: 10.086829662322998 - Avg Validation Loss: 7.916074275970459\n",
      "Epoch 81/87 - Avg Training Loss: 10.436918258666992 - Avg Validation Loss: 6.015877723693848\n",
      "Epoch 82/87 - Avg Training Loss: 10.027929147084555 - Avg Validation Loss: 7.519644737243652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:13,890] Trial 1 finished with value: 8.187567234039307 and parameters: {'hidden_layer_size': 256, 'hidden_layer_count': 2, 'dropout_rate': 0.586563080922377, 'lr': 0.003472986511466822, 'batch_size': 240, 'epochs': 87}. Best is trial 1 with value: 8.187567234039307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/87 - Avg Training Loss: 10.288908958435059 - Avg Validation Loss: 10.480658292770386\n",
      "Epoch 84/87 - Avg Training Loss: 11.247787475585938 - Avg Validation Loss: 9.651989698410034\n",
      "Epoch 85/87 - Avg Training Loss: 10.834821065266928 - Avg Validation Loss: 8.647855997085571\n",
      "Epoch 86/87 - Avg Training Loss: 11.263177712758383 - Avg Validation Loss: 7.541650295257568\n",
      "Epoch 87/87 - Avg Training Loss: 10.996573766072592 - Avg Validation Loss: 8.187567234039307\n",
      "Finished Training\n",
      "Epoch 1/146 - Avg Training Loss: 221.90108108520508 - Avg Validation Loss: 83.34716542561848\n",
      "Epoch 2/146 - Avg Training Loss: 68.76584091186524 - Avg Validation Loss: 30.047595342000324\n",
      "Epoch 3/146 - Avg Training Loss: 37.53388442993164 - Avg Validation Loss: 15.502468427022299\n",
      "Epoch 4/146 - Avg Training Loss: 30.102820587158202 - Avg Validation Loss: 14.440399487813314\n",
      "Epoch 5/146 - Avg Training Loss: 32.61332130432129 - Avg Validation Loss: 12.659152666727701\n",
      "Epoch 6/146 - Avg Training Loss: 28.755377578735352 - Avg Validation Loss: 21.088594436645508\n",
      "Epoch 7/146 - Avg Training Loss: 26.910393905639648 - Avg Validation Loss: 33.43025207519531\n",
      "Epoch 8/146 - Avg Training Loss: 26.252862548828126 - Avg Validation Loss: 33.923379262288414\n",
      "Epoch 9/146 - Avg Training Loss: 23.989814376831056 - Avg Validation Loss: 17.92092450459798\n",
      "Epoch 10/146 - Avg Training Loss: 23.296975326538085 - Avg Validation Loss: 10.092523892720541\n",
      "Epoch 11/146 - Avg Training Loss: 22.958840942382814 - Avg Validation Loss: 16.784236272176106\n",
      "Epoch 12/146 - Avg Training Loss: 22.17069435119629 - Avg Validation Loss: 29.6563720703125\n",
      "Epoch 13/146 - Avg Training Loss: 23.47548141479492 - Avg Validation Loss: 16.447214126586914\n",
      "Epoch 14/146 - Avg Training Loss: 20.64999465942383 - Avg Validation Loss: 23.07505734761556\n",
      "Epoch 15/146 - Avg Training Loss: 20.935657119750978 - Avg Validation Loss: 24.71685282389323\n",
      "Epoch 16/146 - Avg Training Loss: 20.46279830932617 - Avg Validation Loss: 31.246037165323894\n",
      "Epoch 17/146 - Avg Training Loss: 19.33359298706055 - Avg Validation Loss: 29.794766108194988\n",
      "Epoch 18/146 - Avg Training Loss: 19.798211288452148 - Avg Validation Loss: 27.84872817993164\n",
      "Epoch 19/146 - Avg Training Loss: 19.04179801940918 - Avg Validation Loss: 35.730918884277344\n",
      "Epoch 20/146 - Avg Training Loss: 18.67900390625 - Avg Validation Loss: 21.231229782104492\n",
      "Epoch 21/146 - Avg Training Loss: 19.622033309936523 - Avg Validation Loss: 38.43414306640625\n",
      "Epoch 22/146 - Avg Training Loss: 18.779841041564943 - Avg Validation Loss: 13.991303126017252\n",
      "Epoch 23/146 - Avg Training Loss: 19.89379119873047 - Avg Validation Loss: 28.200525919596355\n",
      "Epoch 24/146 - Avg Training Loss: 17.678948211669923 - Avg Validation Loss: 23.15899658203125\n",
      "Epoch 25/146 - Avg Training Loss: 16.233585166931153 - Avg Validation Loss: 41.78545379638672\n",
      "Epoch 26/146 - Avg Training Loss: 17.39991111755371 - Avg Validation Loss: 20.954463322957356\n",
      "Epoch 27/146 - Avg Training Loss: 16.29220199584961 - Avg Validation Loss: 25.988534927368164\n",
      "Epoch 28/146 - Avg Training Loss: 15.032248306274415 - Avg Validation Loss: 21.197274525960285\n",
      "Epoch 29/146 - Avg Training Loss: 14.37734489440918 - Avg Validation Loss: 25.552091598510742\n",
      "Epoch 30/146 - Avg Training Loss: 16.37447566986084 - Avg Validation Loss: 43.12089538574219\n",
      "Epoch 31/146 - Avg Training Loss: 17.709683990478517 - Avg Validation Loss: 35.40015411376953\n",
      "Epoch 32/146 - Avg Training Loss: 15.391093063354493 - Avg Validation Loss: 19.692141850789387\n",
      "Epoch 33/146 - Avg Training Loss: 13.621151161193847 - Avg Validation Loss: 24.143333435058594\n",
      "Epoch 34/146 - Avg Training Loss: 12.373297119140625 - Avg Validation Loss: 15.998507817586264\n",
      "Epoch 35/146 - Avg Training Loss: 12.651995849609374 - Avg Validation Loss: 16.408504803975422\n",
      "Epoch 36/146 - Avg Training Loss: 11.789597129821777 - Avg Validation Loss: 17.98403040568034\n",
      "Epoch 37/146 - Avg Training Loss: 12.46639175415039 - Avg Validation Loss: 12.215826988220215\n",
      "Epoch 38/146 - Avg Training Loss: 12.337686538696289 - Avg Validation Loss: 15.703803062438965\n",
      "Epoch 39/146 - Avg Training Loss: 11.584301567077636 - Avg Validation Loss: 16.76208241780599\n",
      "Epoch 40/146 - Avg Training Loss: 11.667473602294923 - Avg Validation Loss: 17.433679580688477\n",
      "Epoch 41/146 - Avg Training Loss: 12.131656074523926 - Avg Validation Loss: 11.33843994140625\n",
      "Epoch 42/146 - Avg Training Loss: 10.75673999786377 - Avg Validation Loss: 9.095927874247232\n",
      "Epoch 43/146 - Avg Training Loss: 10.485214805603027 - Avg Validation Loss: 8.342153708140055\n",
      "Epoch 44/146 - Avg Training Loss: 11.59208984375 - Avg Validation Loss: 11.52419916788737\n",
      "Epoch 45/146 - Avg Training Loss: 11.398878860473634 - Avg Validation Loss: 10.807950655619303\n",
      "Epoch 46/146 - Avg Training Loss: 11.176141357421875 - Avg Validation Loss: 8.735671679178873\n",
      "Epoch 47/146 - Avg Training Loss: 11.433301734924317 - Avg Validation Loss: 8.914724349975586\n",
      "Epoch 48/146 - Avg Training Loss: 11.281475830078126 - Avg Validation Loss: 7.059800942738851\n",
      "Epoch 49/146 - Avg Training Loss: 11.189797401428223 - Avg Validation Loss: 6.829694906870524\n",
      "Epoch 50/146 - Avg Training Loss: 10.271491241455077 - Avg Validation Loss: 5.78726323445638\n",
      "Epoch 51/146 - Avg Training Loss: 10.91517162322998 - Avg Validation Loss: 5.794887065887451\n",
      "Epoch 52/146 - Avg Training Loss: 10.469337272644044 - Avg Validation Loss: 8.560117403666178\n",
      "Epoch 53/146 - Avg Training Loss: 10.012860870361328 - Avg Validation Loss: 7.433379491170247\n",
      "Epoch 54/146 - Avg Training Loss: 9.92004337310791 - Avg Validation Loss: 8.673232714335123\n",
      "Epoch 55/146 - Avg Training Loss: 9.830620574951173 - Avg Validation Loss: 9.439260164896647\n",
      "Epoch 56/146 - Avg Training Loss: 10.10803050994873 - Avg Validation Loss: 8.65283489227295\n",
      "Epoch 57/146 - Avg Training Loss: 9.525047874450683 - Avg Validation Loss: 7.288671652475993\n",
      "Epoch 58/146 - Avg Training Loss: 9.120854377746582 - Avg Validation Loss: 7.411847432454427\n",
      "Epoch 59/146 - Avg Training Loss: 9.15143871307373 - Avg Validation Loss: 6.279336293538411\n",
      "Epoch 60/146 - Avg Training Loss: 9.738144683837891 - Avg Validation Loss: 5.576066017150879\n",
      "Epoch 61/146 - Avg Training Loss: 9.607093620300294 - Avg Validation Loss: 6.608145078023274\n",
      "Epoch 62/146 - Avg Training Loss: 8.822121715545654 - Avg Validation Loss: 7.1179070472717285\n",
      "Epoch 63/146 - Avg Training Loss: 9.361580276489258 - Avg Validation Loss: 6.214891751607259\n",
      "Epoch 64/146 - Avg Training Loss: 9.044904708862305 - Avg Validation Loss: 4.93577241897583\n",
      "Epoch 65/146 - Avg Training Loss: 9.552568626403808 - Avg Validation Loss: 5.264100710550944\n",
      "Epoch 66/146 - Avg Training Loss: 9.15017032623291 - Avg Validation Loss: 6.237603823343913\n",
      "Epoch 67/146 - Avg Training Loss: 8.825500869750977 - Avg Validation Loss: 6.579889138539632\n",
      "Epoch 68/146 - Avg Training Loss: 8.466165924072266 - Avg Validation Loss: 6.014693895975749\n",
      "Epoch 69/146 - Avg Training Loss: 7.976735687255859 - Avg Validation Loss: 5.908045768737793\n",
      "Epoch 70/146 - Avg Training Loss: 8.700135326385498 - Avg Validation Loss: 6.208760579427083\n",
      "Epoch 71/146 - Avg Training Loss: 8.62485113143921 - Avg Validation Loss: 5.799966812133789\n",
      "Epoch 72/146 - Avg Training Loss: 7.997928047180176 - Avg Validation Loss: 5.004950841267903\n",
      "Epoch 73/146 - Avg Training Loss: 8.986677360534667 - Avg Validation Loss: 5.669053395589192\n",
      "Epoch 74/146 - Avg Training Loss: 8.345753955841065 - Avg Validation Loss: 5.319655577341716\n",
      "Epoch 75/146 - Avg Training Loss: 9.024137115478515 - Avg Validation Loss: 5.014259974161784\n",
      "Epoch 76/146 - Avg Training Loss: 8.433875560760498 - Avg Validation Loss: 5.232275485992432\n",
      "Epoch 77/146 - Avg Training Loss: 8.523082256317139 - Avg Validation Loss: 5.008163928985596\n",
      "Epoch 78/146 - Avg Training Loss: 8.295206832885743 - Avg Validation Loss: 4.93520466486613\n",
      "Epoch 79/146 - Avg Training Loss: 8.31990442276001 - Avg Validation Loss: 6.181958516438802\n",
      "Epoch 80/146 - Avg Training Loss: 8.283283901214599 - Avg Validation Loss: 5.799211502075195\n",
      "Epoch 81/146 - Avg Training Loss: 8.83020248413086 - Avg Validation Loss: 4.847402095794678\n",
      "Epoch 82/146 - Avg Training Loss: 8.579929161071778 - Avg Validation Loss: 4.668933868408203\n",
      "Epoch 83/146 - Avg Training Loss: 8.875790023803711 - Avg Validation Loss: 5.288992563883464\n",
      "Epoch 84/146 - Avg Training Loss: 8.240046310424805 - Avg Validation Loss: 5.574283599853516\n",
      "Epoch 85/146 - Avg Training Loss: 8.045630931854248 - Avg Validation Loss: 5.379446665445964\n",
      "Epoch 86/146 - Avg Training Loss: 7.995304965972901 - Avg Validation Loss: 4.968838532765706\n",
      "Epoch 87/146 - Avg Training Loss: 8.304850006103516 - Avg Validation Loss: 4.554999669392903\n",
      "Epoch 88/146 - Avg Training Loss: 7.6660792350769045 - Avg Validation Loss: 4.916068394978841\n",
      "Epoch 89/146 - Avg Training Loss: 7.952515888214111 - Avg Validation Loss: 4.9883809089660645\n",
      "Epoch 90/146 - Avg Training Loss: 7.608368587493897 - Avg Validation Loss: 4.873787085215251\n",
      "Epoch 91/146 - Avg Training Loss: 7.950865745544434 - Avg Validation Loss: 4.470502058664958\n",
      "Epoch 92/146 - Avg Training Loss: 7.868316173553467 - Avg Validation Loss: 5.413524627685547\n",
      "Epoch 93/146 - Avg Training Loss: 7.359818172454834 - Avg Validation Loss: 4.455074787139893\n",
      "Epoch 94/146 - Avg Training Loss: 7.578868198394775 - Avg Validation Loss: 4.971372922261556\n",
      "Epoch 95/146 - Avg Training Loss: 7.961211490631103 - Avg Validation Loss: 5.456367174784343\n",
      "Epoch 96/146 - Avg Training Loss: 7.379419803619385 - Avg Validation Loss: 4.57520866394043\n",
      "Epoch 97/146 - Avg Training Loss: 7.272762393951416 - Avg Validation Loss: 4.872798919677734\n",
      "Epoch 98/146 - Avg Training Loss: 6.973152351379395 - Avg Validation Loss: 4.998156229654948\n",
      "Epoch 99/146 - Avg Training Loss: 7.746656894683838 - Avg Validation Loss: 4.775265693664551\n",
      "Epoch 100/146 - Avg Training Loss: 7.55456314086914 - Avg Validation Loss: 4.852393786112468\n",
      "Epoch 101/146 - Avg Training Loss: 7.054184150695801 - Avg Validation Loss: 4.851527690887451\n",
      "Epoch 102/146 - Avg Training Loss: 7.5743683815002445 - Avg Validation Loss: 4.290329933166504\n",
      "Epoch 103/146 - Avg Training Loss: 6.876265239715576 - Avg Validation Loss: 5.035150527954102\n",
      "Epoch 104/146 - Avg Training Loss: 7.016019535064697 - Avg Validation Loss: 4.789715925852458\n",
      "Epoch 105/146 - Avg Training Loss: 6.876602840423584 - Avg Validation Loss: 4.8707350095113116\n",
      "Epoch 106/146 - Avg Training Loss: 7.0150001525878904 - Avg Validation Loss: 4.804186503092448\n",
      "Epoch 107/146 - Avg Training Loss: 7.280842018127442 - Avg Validation Loss: 4.672555923461914\n",
      "Epoch 108/146 - Avg Training Loss: 6.8368048667907715 - Avg Validation Loss: 4.279154141743978\n",
      "Epoch 109/146 - Avg Training Loss: 6.930341339111328 - Avg Validation Loss: 4.579727649688721\n",
      "Epoch 110/146 - Avg Training Loss: 6.7420961380004885 - Avg Validation Loss: 4.247891187667847\n",
      "Epoch 111/146 - Avg Training Loss: 7.497342205047607 - Avg Validation Loss: 5.329466660817464\n",
      "Epoch 112/146 - Avg Training Loss: 6.684038257598877 - Avg Validation Loss: 5.739724636077881\n",
      "Epoch 113/146 - Avg Training Loss: 7.037019443511963 - Avg Validation Loss: 4.399584611256917\n",
      "Epoch 114/146 - Avg Training Loss: 6.736836338043213 - Avg Validation Loss: 4.622942924499512\n",
      "Epoch 115/146 - Avg Training Loss: 6.692811298370361 - Avg Validation Loss: 5.023270765940349\n",
      "Epoch 116/146 - Avg Training Loss: 7.199386787414551 - Avg Validation Loss: 4.496926625569661\n",
      "Epoch 117/146 - Avg Training Loss: 6.7155149459838865 - Avg Validation Loss: 4.918891588846843\n",
      "Epoch 118/146 - Avg Training Loss: 7.138915634155273 - Avg Validation Loss: 5.646302858988444\n",
      "Epoch 119/146 - Avg Training Loss: 7.0928730964660645 - Avg Validation Loss: 4.536715030670166\n",
      "Epoch 120/146 - Avg Training Loss: 6.656473445892334 - Avg Validation Loss: 4.735726038614909\n",
      "Epoch 121/146 - Avg Training Loss: 6.629610538482666 - Avg Validation Loss: 4.526628017425537\n",
      "Epoch 122/146 - Avg Training Loss: 6.583322811126709 - Avg Validation Loss: 4.8998762766520185\n",
      "Epoch 123/146 - Avg Training Loss: 6.860765361785889 - Avg Validation Loss: 4.8062764803568525\n",
      "Epoch 124/146 - Avg Training Loss: 6.899395751953125 - Avg Validation Loss: 4.251118342081706\n",
      "Epoch 125/146 - Avg Training Loss: 6.791668701171875 - Avg Validation Loss: 4.33395783106486\n",
      "Epoch 126/146 - Avg Training Loss: 6.596655941009521 - Avg Validation Loss: 4.619454542795817\n",
      "Epoch 127/146 - Avg Training Loss: 6.94784460067749 - Avg Validation Loss: 5.385085741678874\n",
      "Epoch 128/146 - Avg Training Loss: 6.9720330238342285 - Avg Validation Loss: 4.291650772094727\n",
      "Epoch 129/146 - Avg Training Loss: 6.698954486846924 - Avg Validation Loss: 4.565001010894775\n",
      "Epoch 130/146 - Avg Training Loss: 6.604408264160156 - Avg Validation Loss: 4.248570521672566\n",
      "Epoch 131/146 - Avg Training Loss: 6.253360176086426 - Avg Validation Loss: 4.190293153127034\n",
      "Epoch 132/146 - Avg Training Loss: 6.596571922302246 - Avg Validation Loss: 4.918337821960449\n",
      "Epoch 133/146 - Avg Training Loss: 6.709474658966064 - Avg Validation Loss: 4.663811365763347\n",
      "Epoch 134/146 - Avg Training Loss: 6.132597923278809 - Avg Validation Loss: 4.195486227671306\n",
      "Epoch 135/146 - Avg Training Loss: 6.0315553665161135 - Avg Validation Loss: 4.914628346761067\n",
      "Epoch 136/146 - Avg Training Loss: 6.019656848907471 - Avg Validation Loss: 4.668578465779622\n",
      "Epoch 137/146 - Avg Training Loss: 5.923221492767334 - Avg Validation Loss: 4.665411154429118\n",
      "Epoch 138/146 - Avg Training Loss: 6.311083030700684 - Avg Validation Loss: 4.6013264656066895\n",
      "Epoch 139/146 - Avg Training Loss: 6.257819080352784 - Avg Validation Loss: 4.531819661458333\n",
      "Epoch 140/146 - Avg Training Loss: 6.165209865570068 - Avg Validation Loss: 4.9778258005778\n",
      "Epoch 141/146 - Avg Training Loss: 6.190846252441406 - Avg Validation Loss: 4.267478624979655\n",
      "Epoch 142/146 - Avg Training Loss: 6.105740928649903 - Avg Validation Loss: 4.4731597900390625\n",
      "Epoch 143/146 - Avg Training Loss: 6.309994506835937 - Avg Validation Loss: 4.8777689933776855\n",
      "Epoch 144/146 - Avg Training Loss: 6.342434120178223 - Avg Validation Loss: 4.622394402821858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:33,763] Trial 2 finished with value: 4.963185787200928 and parameters: {'hidden_layer_size': 700, 'hidden_layer_count': 4, 'dropout_rate': 0.5989529826213716, 'lr': 0.004786134259579512, 'batch_size': 336, 'epochs': 146}. Best is trial 2 with value: 4.963185787200928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/146 - Avg Training Loss: 6.030808639526367 - Avg Validation Loss: 4.529996871948242\n",
      "Epoch 146/146 - Avg Training Loss: 5.92544527053833 - Avg Validation Loss: 4.963185787200928\n",
      "Finished Training\n",
      "Epoch 1/134 - Avg Training Loss: 87.6964464187622 - Avg Validation Loss: 12.146599769592285\n",
      "Epoch 2/134 - Avg Training Loss: 17.600440926022 - Avg Validation Loss: 7.8721489472822705\n",
      "Epoch 3/134 - Avg Training Loss: 12.492950015597874 - Avg Validation Loss: 11.150262312455611\n",
      "Epoch 4/134 - Avg Training Loss: 10.420223818884956 - Avg Validation Loss: 7.327149131081321\n",
      "Epoch 5/134 - Avg Training Loss: 9.522328853607178 - Avg Validation Loss: 7.886514403603294\n",
      "Epoch 6/134 - Avg Training Loss: 9.01177348030938 - Avg Validation Loss: 6.310651215639981\n",
      "Epoch 7/134 - Avg Training Loss: 8.706205818388197 - Avg Validation Loss: 4.724692474712025\n",
      "Epoch 8/134 - Avg Training Loss: 7.991694874233669 - Avg Validation Loss: 5.733247583562678\n",
      "Epoch 9/134 - Avg Training Loss: 7.668573326534695 - Avg Validation Loss: 5.730584274638783\n",
      "Epoch 10/134 - Avg Training Loss: 7.723312907748753 - Avg Validation Loss: 5.44164497202093\n",
      "Epoch 11/134 - Avg Training Loss: 7.366691244973077 - Avg Validation Loss: 7.181644873185591\n",
      "Epoch 12/134 - Avg Training Loss: 7.570678578482734 - Avg Validation Loss: 5.761280840093439\n",
      "Epoch 13/134 - Avg Training Loss: 7.550923479927911 - Avg Validation Loss: 5.4169792478734795\n",
      "Epoch 14/134 - Avg Training Loss: 7.186460415522258 - Avg Validation Loss: 6.732186967676336\n",
      "Epoch 15/134 - Avg Training Loss: 7.090577310985989 - Avg Validation Loss: 7.807690620422363\n",
      "Epoch 16/134 - Avg Training Loss: 6.687778896755642 - Avg Validation Loss: 6.4395083080638535\n",
      "Epoch 17/134 - Avg Training Loss: 6.804643551508586 - Avg Validation Loss: 6.757525660774925\n",
      "Epoch 18/134 - Avg Training Loss: 7.094579431745741 - Avg Validation Loss: 6.824774525382302\n",
      "Epoch 19/134 - Avg Training Loss: 6.880935880872938 - Avg Validation Loss: 6.821083242242986\n",
      "Epoch 20/134 - Avg Training Loss: 7.313003646002875 - Avg Validation Loss: 4.190561164509166\n",
      "Epoch 21/134 - Avg Training Loss: 6.60333071814643 - Avg Validation Loss: 4.112787571820346\n",
      "Epoch 22/134 - Avg Training Loss: 6.522031810548571 - Avg Validation Loss: 6.925600311972878\n",
      "Epoch 23/134 - Avg Training Loss: 6.502663162019518 - Avg Validation Loss: 6.469883745366877\n",
      "Epoch 24/134 - Avg Training Loss: 6.69839882850647 - Avg Validation Loss: 5.1961261142383925\n",
      "Epoch 25/134 - Avg Training Loss: 6.546088563071357 - Avg Validation Loss: 9.021522998809814\n",
      "Epoch 26/134 - Avg Training Loss: 6.7356345653533936 - Avg Validation Loss: 5.1077752113342285\n",
      "Epoch 27/134 - Avg Training Loss: 6.775925715764363 - Avg Validation Loss: 4.409233461726796\n",
      "Epoch 28/134 - Avg Training Loss: 7.0915721787346735 - Avg Validation Loss: 5.334655393253673\n",
      "Epoch 29/134 - Avg Training Loss: 6.671279589335124 - Avg Validation Loss: 10.542703195051713\n",
      "Epoch 30/134 - Avg Training Loss: 7.023620234595405 - Avg Validation Loss: 5.8613638010892\n",
      "Epoch 31/134 - Avg Training Loss: 6.901706351174249 - Avg Validation Loss: 4.783094687895342\n",
      "Epoch 32/134 - Avg Training Loss: 6.557668765385945 - Avg Validation Loss: 8.523469534787266\n",
      "Epoch 33/134 - Avg Training Loss: 6.83888496292962 - Avg Validation Loss: 5.367240819064054\n",
      "Epoch 34/134 - Avg Training Loss: 6.794396744834052 - Avg Validation Loss: 9.08006832816384\n",
      "Epoch 35/134 - Avg Training Loss: 6.890693346659343 - Avg Validation Loss: 5.152781529860063\n",
      "Epoch 36/134 - Avg Training Loss: 6.465686188803779 - Avg Validation Loss: 7.897885669361461\n",
      "Epoch 37/134 - Avg Training Loss: 6.452456739213732 - Avg Validation Loss: 6.380336804823442\n",
      "Epoch 38/134 - Avg Training Loss: 6.612793895933363 - Avg Validation Loss: 4.008062947880138\n",
      "Epoch 39/134 - Avg Training Loss: 6.597186962763469 - Avg Validation Loss: 6.061981374567205\n",
      "Epoch 40/134 - Avg Training Loss: 6.5769028133816185 - Avg Validation Loss: 5.29254674911499\n",
      "Epoch 41/134 - Avg Training Loss: 6.134717437956068 - Avg Validation Loss: 5.964048992503773\n",
      "Epoch 42/134 - Avg Training Loss: 6.612474468019274 - Avg Validation Loss: 8.839911677620627\n",
      "Epoch 43/134 - Avg Training Loss: 6.153387917412652 - Avg Validation Loss: 6.809520417993719\n",
      "Epoch 44/134 - Avg Training Loss: 6.526477681265937 - Avg Validation Loss: 6.944237188859419\n",
      "Epoch 45/134 - Avg Training Loss: 6.2576682567596436 - Avg Validation Loss: 5.575231812217019\n",
      "Epoch 46/134 - Avg Training Loss: 6.085937288072374 - Avg Validation Loss: 7.656864209608599\n",
      "Epoch 47/134 - Avg Training Loss: 6.760633177227444 - Avg Validation Loss: 13.263516599481756\n",
      "Epoch 48/134 - Avg Training Loss: 7.147552437252468 - Avg Validation Loss: 5.8325276374816895\n",
      "Epoch 49/134 - Avg Training Loss: 6.196749448776245 - Avg Validation Loss: 4.404241540215232\n",
      "Epoch 50/134 - Avg Training Loss: 6.608550442589654 - Avg Validation Loss: 4.457141572778875\n",
      "Epoch 51/134 - Avg Training Loss: 6.858134216732449 - Avg Validation Loss: 4.968742348931053\n",
      "Epoch 52/134 - Avg Training Loss: 7.367693053351508 - Avg Validation Loss: 7.9255852699279785\n",
      "Epoch 53/134 - Avg Training Loss: 6.790807909435696 - Avg Validation Loss: 8.82941974293102\n",
      "Epoch 54/134 - Avg Training Loss: 6.33134478992886 - Avg Validation Loss: 6.1094465255737305\n",
      "Epoch 55/134 - Avg Training Loss: 6.05663537979126 - Avg Validation Loss: 6.117974281311035\n",
      "Epoch 56/134 - Avg Training Loss: 6.679860538906521 - Avg Validation Loss: 7.447349418293346\n",
      "Epoch 57/134 - Avg Training Loss: 5.897054407331678 - Avg Validation Loss: 5.055358973416415\n",
      "Epoch 58/134 - Avg Training Loss: 5.913461791144477 - Avg Validation Loss: 6.075598153201017\n",
      "Epoch 59/134 - Avg Training Loss: 6.178951051500109 - Avg Validation Loss: 5.773099162361839\n",
      "Epoch 60/134 - Avg Training Loss: 5.945256816016303 - Avg Validation Loss: 6.821784799749201\n",
      "Epoch 61/134 - Avg Training Loss: 5.900807274712457 - Avg Validation Loss: 7.194731972434304\n",
      "Epoch 62/134 - Avg Training Loss: 5.775516006681654 - Avg Validation Loss: 4.9684929414228955\n",
      "Epoch 63/134 - Avg Training Loss: 5.676820119222005 - Avg Validation Loss: 5.64502477645874\n",
      "Epoch 64/134 - Avg Training Loss: 6.160185919867621 - Avg Validation Loss: 8.084807699376887\n",
      "Epoch 65/134 - Avg Training Loss: 6.316700670454237 - Avg Validation Loss: 5.542026909914884\n",
      "Epoch 66/134 - Avg Training Loss: 5.618977705637614 - Avg Validation Loss: 4.618029919537631\n",
      "Epoch 67/134 - Avg Training Loss: 6.387153969870673 - Avg Validation Loss: 5.261148322712291\n",
      "Epoch 68/134 - Avg Training Loss: 6.394581927193536 - Avg Validation Loss: 7.130993713032115\n",
      "Epoch 69/134 - Avg Training Loss: 6.264147520065308 - Avg Validation Loss: 5.79762094671076\n",
      "Epoch 70/134 - Avg Training Loss: 6.011995448006524 - Avg Validation Loss: 4.505979256196455\n",
      "Epoch 71/134 - Avg Training Loss: 5.928185913297865 - Avg Validation Loss: 7.781686089255593\n",
      "Epoch 72/134 - Avg Training Loss: 5.929509162902832 - Avg Validation Loss: 6.66616999019276\n",
      "Epoch 73/134 - Avg Training Loss: 6.909181118011475 - Avg Validation Loss: 4.907949664375999\n",
      "Epoch 74/134 - Avg Training Loss: 6.486790895462036 - Avg Validation Loss: 3.490963675759055\n",
      "Epoch 75/134 - Avg Training Loss: 7.214282088809544 - Avg Validation Loss: 3.7931637330488726\n",
      "Epoch 76/134 - Avg Training Loss: 6.356566323174371 - Avg Validation Loss: 5.739696459336714\n",
      "Epoch 77/134 - Avg Training Loss: 5.7558943960401745 - Avg Validation Loss: 4.229794047095559\n",
      "Epoch 78/134 - Avg Training Loss: 6.084068377812703 - Avg Validation Loss: 4.18015287139199\n",
      "Epoch 79/134 - Avg Training Loss: 6.019153224097358 - Avg Validation Loss: 4.252866744995117\n",
      "Epoch 80/134 - Avg Training Loss: 6.0319892565409345 - Avg Validation Loss: 5.883068041367964\n",
      "Epoch 81/134 - Avg Training Loss: 6.09575883547465 - Avg Validation Loss: 10.0054329958829\n",
      "Epoch 82/134 - Avg Training Loss: 6.024947590298122 - Avg Validation Loss: 6.605513616041704\n",
      "Epoch 83/134 - Avg Training Loss: 5.714558521906535 - Avg Validation Loss: 8.002520994706588\n",
      "Epoch 84/134 - Avg Training Loss: 6.076978948381212 - Avg Validation Loss: 9.648688663135875\n",
      "Epoch 85/134 - Avg Training Loss: 6.242109855016072 - Avg Validation Loss: 4.938046477057717\n",
      "Epoch 86/134 - Avg Training Loss: 6.547317955229017 - Avg Validation Loss: 5.079019546508789\n",
      "Epoch 87/134 - Avg Training Loss: 5.936180061764187 - Avg Validation Loss: 5.56349026073109\n",
      "Epoch 88/134 - Avg Training Loss: 5.925978342692058 - Avg Validation Loss: 9.332415754144842\n",
      "Epoch 89/134 - Avg Training Loss: 6.167349312040541 - Avg Validation Loss: 4.277555660767988\n",
      "Epoch 90/134 - Avg Training Loss: 5.614412890540229 - Avg Validation Loss: 5.794228423725475\n",
      "Epoch 91/134 - Avg Training Loss: 5.488325993220012 - Avg Validation Loss: 5.683944962241433\n",
      "Epoch 92/134 - Avg Training Loss: 5.622398005591498 - Avg Validation Loss: 7.42504358291626\n",
      "Epoch 93/134 - Avg Training Loss: 5.812683502833049 - Avg Validation Loss: 6.037083192305132\n",
      "Epoch 94/134 - Avg Training Loss: 5.997283591164483 - Avg Validation Loss: 8.238840493288906\n",
      "Epoch 95/134 - Avg Training Loss: 6.2129648261600074 - Avg Validation Loss: 5.871957128698176\n",
      "Epoch 96/134 - Avg Training Loss: 5.815536419550578 - Avg Validation Loss: 4.2538299127058545\n",
      "Epoch 97/134 - Avg Training Loss: 5.70176445113288 - Avg Validation Loss: 6.364912986755371\n",
      "Epoch 98/134 - Avg Training Loss: 5.497244252098931 - Avg Validation Loss: 8.448558330535889\n",
      "Epoch 99/134 - Avg Training Loss: 5.601096179750231 - Avg Validation Loss: 7.411609823053533\n",
      "Epoch 100/134 - Avg Training Loss: 5.6755810843573675 - Avg Validation Loss: 6.651541103016246\n",
      "Epoch 101/134 - Avg Training Loss: 5.406470828586155 - Avg Validation Loss: 5.470980124040083\n",
      "Epoch 102/134 - Avg Training Loss: 5.746153752009074 - Avg Validation Loss: 6.002482977780429\n",
      "Epoch 103/134 - Avg Training Loss: 5.5098536014556885 - Avg Validation Loss: 5.371103286743164\n",
      "Epoch 104/134 - Avg Training Loss: 5.464584483040704 - Avg Validation Loss: 5.016742142764005\n",
      "Epoch 105/134 - Avg Training Loss: 5.616721762551202 - Avg Validation Loss: 4.600115711038763\n",
      "Epoch 106/134 - Avg Training Loss: 5.321201708581713 - Avg Validation Loss: 6.663509975780141\n",
      "Epoch 107/134 - Avg Training Loss: 5.678320911195543 - Avg Validation Loss: 7.147911461916837\n",
      "Epoch 108/134 - Avg Training Loss: 5.433483507898119 - Avg Validation Loss: 4.703695774078369\n",
      "Epoch 109/134 - Avg Training Loss: 5.432495911916097 - Avg Validation Loss: 7.408460400321267\n",
      "Epoch 110/134 - Avg Training Loss: 5.571372720930311 - Avg Validation Loss: 4.920680132779208\n",
      "Epoch 111/134 - Avg Training Loss: 5.162892513804966 - Avg Validation Loss: 4.580712730234319\n",
      "Epoch 112/134 - Avg Training Loss: 5.391870631111993 - Avg Validation Loss: 4.702877976677635\n",
      "Epoch 113/134 - Avg Training Loss: 5.584252542919582 - Avg Validation Loss: 6.513970201665705\n",
      "Epoch 114/134 - Avg Training Loss: 5.433349132537842 - Avg Validation Loss: 4.454363714564931\n",
      "Epoch 115/134 - Avg Training Loss: 5.412319262822469 - Avg Validation Loss: 4.914527979764071\n",
      "Epoch 116/134 - Avg Training Loss: 5.384492105907864 - Avg Validation Loss: 4.964506062594327\n",
      "Epoch 117/134 - Avg Training Loss: 5.4855248398251 - Avg Validation Loss: 3.937686876817183\n",
      "Epoch 118/134 - Avg Training Loss: 5.466982311672634 - Avg Validation Loss: 5.609118548306552\n",
      "Epoch 119/134 - Avg Training Loss: 5.263054887453715 - Avg Validation Loss: 5.092947873202237\n",
      "Epoch 120/134 - Avg Training Loss: 5.531699710422092 - Avg Validation Loss: 4.758898084813898\n",
      "Epoch 121/134 - Avg Training Loss: 5.617478423648411 - Avg Validation Loss: 4.874471837824041\n",
      "Epoch 122/134 - Avg Training Loss: 5.4573666254679365 - Avg Validation Loss: 5.172259157354182\n",
      "Epoch 123/134 - Avg Training Loss: 5.222570286856757 - Avg Validation Loss: 5.304217251864347\n",
      "Epoch 124/134 - Avg Training Loss: 5.301739162868923 - Avg Validation Loss: 3.6482246789065274\n",
      "Epoch 125/134 - Avg Training Loss: 5.566672139697605 - Avg Validation Loss: 5.4062651720913975\n",
      "Epoch 126/134 - Avg Training Loss: 5.336180607477824 - Avg Validation Loss: 3.8807477951049805\n",
      "Epoch 127/134 - Avg Training Loss: 5.432521820068359 - Avg Validation Loss: 3.541296460411765\n",
      "Epoch 128/134 - Avg Training Loss: 5.4891336229112415 - Avg Validation Loss: 5.196647665717385\n",
      "Epoch 129/134 - Avg Training Loss: 5.199427975548638 - Avg Validation Loss: 7.058571208607066\n",
      "Epoch 130/134 - Avg Training Loss: 6.189845005671184 - Avg Validation Loss: 11.441995707425205\n",
      "Epoch 131/134 - Avg Training Loss: 5.788640790515476 - Avg Validation Loss: 5.759636835618452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:40,876] Trial 3 finished with value: 6.318199981342662 and parameters: {'hidden_layer_size': 256, 'hidden_layer_count': 2, 'dropout_rate': 0.40973963365141153, 'lr': 0.0025618927011655046, 'batch_size': 80, 'epochs': 134}. Best is trial 2 with value: 4.963185787200928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/134 - Avg Training Loss: 5.383949054612054 - Avg Validation Loss: 3.923589142886075\n",
      "Epoch 133/134 - Avg Training Loss: 6.050284226735433 - Avg Validation Loss: 3.6562266783280806\n",
      "Epoch 134/134 - Avg Training Loss: 6.125062439176771 - Avg Validation Loss: 6.318199981342662\n",
      "Finished Training\n",
      "Epoch 1/82 - Avg Training Loss: 99.84847582303561 - Avg Validation Loss: 25.199996948242188\n",
      "Epoch 2/82 - Avg Training Loss: 30.501199869009163 - Avg Validation Loss: 14.602332353591919\n",
      "Epoch 3/82 - Avg Training Loss: 17.624737886282112 - Avg Validation Loss: 7.105111181735992\n",
      "Epoch 4/82 - Avg Training Loss: 12.770161848801832 - Avg Validation Loss: 8.166769087314606\n",
      "Epoch 5/82 - Avg Training Loss: 11.411792828486515 - Avg Validation Loss: 9.241082668304443\n",
      "Epoch 6/82 - Avg Training Loss: 9.986650613638071 - Avg Validation Loss: 6.049238741397858\n",
      "Epoch 7/82 - Avg Training Loss: 9.650853157043457 - Avg Validation Loss: 7.899480938911438\n",
      "Epoch 8/82 - Avg Training Loss: 9.104388823876015 - Avg Validation Loss: 6.756847083568573\n",
      "Epoch 9/82 - Avg Training Loss: 8.888743583972637 - Avg Validation Loss: 5.378310322761536\n",
      "Epoch 10/82 - Avg Training Loss: 8.459598357860859 - Avg Validation Loss: 6.9777907729148865\n",
      "Epoch 11/82 - Avg Training Loss: 8.032119897695688 - Avg Validation Loss: 6.079639911651611\n",
      "Epoch 12/82 - Avg Training Loss: 8.245816450852613 - Avg Validation Loss: 6.890162348747253\n",
      "Epoch 13/82 - Avg Training Loss: 8.04615464577308 - Avg Validation Loss: 6.79400897026062\n",
      "Epoch 14/82 - Avg Training Loss: 7.84680410531851 - Avg Validation Loss: 5.525553524494171\n",
      "Epoch 15/82 - Avg Training Loss: 7.802042704362136 - Avg Validation Loss: 5.905258893966675\n",
      "Epoch 16/82 - Avg Training Loss: 7.290357626401461 - Avg Validation Loss: 7.1186559200286865\n",
      "Epoch 17/82 - Avg Training Loss: 8.115710808680607 - Avg Validation Loss: 8.37010371685028\n",
      "Epoch 18/82 - Avg Training Loss: 7.4919879619891825 - Avg Validation Loss: 6.206969857215881\n",
      "Epoch 19/82 - Avg Training Loss: 8.016586670508751 - Avg Validation Loss: 5.450066447257996\n",
      "Epoch 20/82 - Avg Training Loss: 7.55929972575261 - Avg Validation Loss: 6.202943027019501\n",
      "Epoch 21/82 - Avg Training Loss: 7.5119973329397345 - Avg Validation Loss: 9.042592346668243\n",
      "Epoch 22/82 - Avg Training Loss: 8.594237400935246 - Avg Validation Loss: 8.35319173336029\n",
      "Epoch 23/82 - Avg Training Loss: 8.476790574880747 - Avg Validation Loss: 7.00702565908432\n",
      "Epoch 24/82 - Avg Training Loss: 8.098705585186298 - Avg Validation Loss: 5.005227744579315\n",
      "Epoch 25/82 - Avg Training Loss: 7.748888969421387 - Avg Validation Loss: 5.935929954051971\n",
      "Epoch 26/82 - Avg Training Loss: 7.843242461864765 - Avg Validation Loss: 10.964286804199219\n",
      "Epoch 27/82 - Avg Training Loss: 8.253568465893085 - Avg Validation Loss: 7.43316376209259\n",
      "Epoch 28/82 - Avg Training Loss: 7.350156894096961 - Avg Validation Loss: 7.336558103561401\n",
      "Epoch 29/82 - Avg Training Loss: 7.442998812748836 - Avg Validation Loss: 5.218570590019226\n",
      "Epoch 30/82 - Avg Training Loss: 7.13058431331928 - Avg Validation Loss: 4.08989280462265\n",
      "Epoch 31/82 - Avg Training Loss: 7.50709133881789 - Avg Validation Loss: 8.184893548488617\n",
      "Epoch 32/82 - Avg Training Loss: 7.777272627903865 - Avg Validation Loss: 5.253020107746124\n",
      "Epoch 33/82 - Avg Training Loss: 7.02196033184345 - Avg Validation Loss: 4.471779108047485\n",
      "Epoch 34/82 - Avg Training Loss: 7.069370599893423 - Avg Validation Loss: 5.093981385231018\n",
      "Epoch 35/82 - Avg Training Loss: 7.382090201744666 - Avg Validation Loss: 7.095230996608734\n",
      "Epoch 36/82 - Avg Training Loss: 7.447537458859957 - Avg Validation Loss: 5.789849281311035\n",
      "Epoch 37/82 - Avg Training Loss: 7.246826025155874 - Avg Validation Loss: 10.209833860397339\n",
      "Epoch 38/82 - Avg Training Loss: 7.637984275817871 - Avg Validation Loss: 4.716583728790283\n",
      "Epoch 39/82 - Avg Training Loss: 7.708078421079195 - Avg Validation Loss: 5.584843993186951\n",
      "Epoch 40/82 - Avg Training Loss: 7.43484720816979 - Avg Validation Loss: 8.211637318134308\n",
      "Epoch 41/82 - Avg Training Loss: 7.268914296076848 - Avg Validation Loss: 5.575663208961487\n",
      "Epoch 42/82 - Avg Training Loss: 7.383289667276236 - Avg Validation Loss: 9.96787965297699\n",
      "Epoch 43/82 - Avg Training Loss: 7.455299634199876 - Avg Validation Loss: 5.731885313987732\n",
      "Epoch 44/82 - Avg Training Loss: 7.36869712976309 - Avg Validation Loss: 7.006119728088379\n",
      "Epoch 45/82 - Avg Training Loss: 7.298246603745681 - Avg Validation Loss: 8.930015861988068\n",
      "Epoch 46/82 - Avg Training Loss: 7.576305902921236 - Avg Validation Loss: 4.046677261590958\n",
      "Epoch 47/82 - Avg Training Loss: 7.147192148061899 - Avg Validation Loss: 7.90233439207077\n",
      "Epoch 48/82 - Avg Training Loss: 7.247382420759934 - Avg Validation Loss: 5.3076266050338745\n",
      "Epoch 49/82 - Avg Training Loss: 6.667392437274639 - Avg Validation Loss: 4.689817428588867\n",
      "Epoch 50/82 - Avg Training Loss: 7.2972133709834175 - Avg Validation Loss: 7.048035681247711\n",
      "Epoch 51/82 - Avg Training Loss: 6.826370092538687 - Avg Validation Loss: 7.03719174861908\n",
      "Epoch 52/82 - Avg Training Loss: 7.291991013746995 - Avg Validation Loss: 7.0726980566978455\n",
      "Epoch 53/82 - Avg Training Loss: 7.284025412339431 - Avg Validation Loss: 3.809709370136261\n",
      "Epoch 54/82 - Avg Training Loss: 6.813799051138071 - Avg Validation Loss: 8.124001860618591\n",
      "Epoch 55/82 - Avg Training Loss: 7.533516553732065 - Avg Validation Loss: 8.389725267887115\n",
      "Epoch 56/82 - Avg Training Loss: 7.668724903693566 - Avg Validation Loss: 4.918517470359802\n",
      "Epoch 57/82 - Avg Training Loss: 7.062350713289701 - Avg Validation Loss: 9.649384021759033\n",
      "Epoch 58/82 - Avg Training Loss: 7.761315382443941 - Avg Validation Loss: 4.097469717264175\n",
      "Epoch 59/82 - Avg Training Loss: 6.956313206599309 - Avg Validation Loss: 6.658562898635864\n",
      "Epoch 60/82 - Avg Training Loss: 6.675072486584003 - Avg Validation Loss: 5.900660514831543\n",
      "Epoch 61/82 - Avg Training Loss: 6.72392148237962 - Avg Validation Loss: 4.966808378696442\n",
      "Epoch 62/82 - Avg Training Loss: 6.7073181592501125 - Avg Validation Loss: 7.3455342054367065\n",
      "Epoch 63/82 - Avg Training Loss: 6.567677974700928 - Avg Validation Loss: 4.7663547694683075\n",
      "Epoch 64/82 - Avg Training Loss: 6.895870428818923 - Avg Validation Loss: 5.354452550411224\n",
      "Epoch 65/82 - Avg Training Loss: 6.908845021174504 - Avg Validation Loss: 7.2735915184021\n",
      "Epoch 66/82 - Avg Training Loss: 7.061789806072529 - Avg Validation Loss: 6.148299992084503\n",
      "Epoch 67/82 - Avg Training Loss: 7.235540720132681 - Avg Validation Loss: 9.098528146743774\n",
      "Epoch 68/82 - Avg Training Loss: 6.9573307404151326 - Avg Validation Loss: 4.8174386620521545\n",
      "Epoch 69/82 - Avg Training Loss: 7.038281330695519 - Avg Validation Loss: 4.55194428563118\n",
      "Epoch 70/82 - Avg Training Loss: 6.755967800433819 - Avg Validation Loss: 5.121262311935425\n",
      "Epoch 71/82 - Avg Training Loss: 6.904709779299223 - Avg Validation Loss: 6.120917499065399\n",
      "Epoch 72/82 - Avg Training Loss: 6.699075111976037 - Avg Validation Loss: 6.765115439891815\n",
      "Epoch 73/82 - Avg Training Loss: 6.84467678803664 - Avg Validation Loss: 3.8265840113162994\n",
      "Epoch 74/82 - Avg Training Loss: 7.1370505186227655 - Avg Validation Loss: 8.258098065853119\n",
      "Epoch 75/82 - Avg Training Loss: 7.257039840404804 - Avg Validation Loss: 4.7954153418540955\n",
      "Epoch 76/82 - Avg Training Loss: 7.123364448547363 - Avg Validation Loss: 7.782306611537933\n",
      "Epoch 77/82 - Avg Training Loss: 7.276188336885893 - Avg Validation Loss: 10.751323223114014\n",
      "Epoch 78/82 - Avg Training Loss: 6.87197234080388 - Avg Validation Loss: 5.487939476966858\n",
      "Epoch 79/82 - Avg Training Loss: 6.844781362093412 - Avg Validation Loss: 7.566387951374054\n",
      "Epoch 80/82 - Avg Training Loss: 6.537595785581148 - Avg Validation Loss: 6.03572553396225\n",
      "Epoch 81/82 - Avg Training Loss: 6.787926490490253 - Avg Validation Loss: 5.898660778999329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:46,337] Trial 4 finished with value: 4.8633527755737305 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 2, 'dropout_rate': 0.5438316075581788, 'lr': 0.003909400699655463, 'batch_size': 112, 'epochs': 82}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/82 - Avg Training Loss: 7.0918289331289435 - Avg Validation Loss: 4.8633527755737305\n",
      "Finished Training\n",
      "Epoch 1/107 - Avg Training Loss: 236.60972595214844 - Avg Validation Loss: 44.63895225524902\n",
      "Epoch 2/107 - Avg Training Loss: 80.31699466705322 - Avg Validation Loss: 25.654436111450195\n",
      "Epoch 3/107 - Avg Training Loss: 57.933088302612305 - Avg Validation Loss: 64.59291458129883\n",
      "Epoch 4/107 - Avg Training Loss: 47.41653776168823 - Avg Validation Loss: 13.007195949554443\n",
      "Epoch 5/107 - Avg Training Loss: 43.98813724517822 - Avg Validation Loss: 15.777925491333008\n",
      "Epoch 6/107 - Avg Training Loss: 37.065513610839844 - Avg Validation Loss: 31.80468463897705\n",
      "Epoch 7/107 - Avg Training Loss: 34.49109125137329 - Avg Validation Loss: 10.992353916168213\n",
      "Epoch 8/107 - Avg Training Loss: 36.35307836532593 - Avg Validation Loss: 14.715487957000732\n",
      "Epoch 9/107 - Avg Training Loss: 35.073880672454834 - Avg Validation Loss: 25.47404670715332\n",
      "Epoch 10/107 - Avg Training Loss: 29.7900972366333 - Avg Validation Loss: 8.686115741729736\n",
      "Epoch 11/107 - Avg Training Loss: 28.85060977935791 - Avg Validation Loss: 16.93626117706299\n",
      "Epoch 12/107 - Avg Training Loss: 32.55463171005249 - Avg Validation Loss: 13.981003284454346\n",
      "Epoch 13/107 - Avg Training Loss: 31.42606830596924 - Avg Validation Loss: 12.048822402954102\n",
      "Epoch 14/107 - Avg Training Loss: 26.81264019012451 - Avg Validation Loss: 16.66631317138672\n",
      "Epoch 15/107 - Avg Training Loss: 26.38171625137329 - Avg Validation Loss: 9.308688163757324\n",
      "Epoch 16/107 - Avg Training Loss: 25.196617126464844 - Avg Validation Loss: 12.20359754562378\n",
      "Epoch 17/107 - Avg Training Loss: 25.4581880569458 - Avg Validation Loss: 10.90787935256958\n",
      "Epoch 18/107 - Avg Training Loss: 27.23323917388916 - Avg Validation Loss: 16.73904037475586\n",
      "Epoch 19/107 - Avg Training Loss: 23.68946123123169 - Avg Validation Loss: 7.931622266769409\n",
      "Epoch 20/107 - Avg Training Loss: 25.643822193145752 - Avg Validation Loss: 17.9276123046875\n",
      "Epoch 21/107 - Avg Training Loss: 25.682751178741455 - Avg Validation Loss: 7.229952096939087\n",
      "Epoch 22/107 - Avg Training Loss: 28.1811580657959 - Avg Validation Loss: 18.68686866760254\n",
      "Epoch 23/107 - Avg Training Loss: 27.81037139892578 - Avg Validation Loss: 8.831460952758789\n",
      "Epoch 24/107 - Avg Training Loss: 25.405497550964355 - Avg Validation Loss: 11.77368688583374\n",
      "Epoch 25/107 - Avg Training Loss: 22.501479625701904 - Avg Validation Loss: 11.750303268432617\n",
      "Epoch 26/107 - Avg Training Loss: 22.880516529083252 - Avg Validation Loss: 10.893484115600586\n",
      "Epoch 27/107 - Avg Training Loss: 20.83769464492798 - Avg Validation Loss: 11.668342113494873\n",
      "Epoch 28/107 - Avg Training Loss: 23.335192680358887 - Avg Validation Loss: 7.253802061080933\n",
      "Epoch 29/107 - Avg Training Loss: 20.41864538192749 - Avg Validation Loss: 10.175828456878662\n",
      "Epoch 30/107 - Avg Training Loss: 22.628921031951904 - Avg Validation Loss: 13.115365505218506\n",
      "Epoch 31/107 - Avg Training Loss: 21.148201942443848 - Avg Validation Loss: 7.420168161392212\n",
      "Epoch 32/107 - Avg Training Loss: 21.774587154388428 - Avg Validation Loss: 12.10857343673706\n",
      "Epoch 33/107 - Avg Training Loss: 21.396181106567383 - Avg Validation Loss: 8.264001369476318\n",
      "Epoch 34/107 - Avg Training Loss: 21.22008466720581 - Avg Validation Loss: 13.601315021514893\n",
      "Epoch 35/107 - Avg Training Loss: 21.32075548171997 - Avg Validation Loss: 9.607367038726807\n",
      "Epoch 36/107 - Avg Training Loss: 22.688515186309814 - Avg Validation Loss: 10.420404434204102\n",
      "Epoch 37/107 - Avg Training Loss: 22.54260015487671 - Avg Validation Loss: 14.781227588653564\n",
      "Epoch 38/107 - Avg Training Loss: 22.39469861984253 - Avg Validation Loss: 5.752758026123047\n",
      "Epoch 39/107 - Avg Training Loss: 22.569278717041016 - Avg Validation Loss: 18.415557861328125\n",
      "Epoch 40/107 - Avg Training Loss: 22.698609828948975 - Avg Validation Loss: 6.232033014297485\n",
      "Epoch 41/107 - Avg Training Loss: 22.046026706695557 - Avg Validation Loss: 13.838685035705566\n",
      "Epoch 42/107 - Avg Training Loss: 22.0064058303833 - Avg Validation Loss: 8.19976806640625\n",
      "Epoch 43/107 - Avg Training Loss: 23.023212909698486 - Avg Validation Loss: 10.260127544403076\n",
      "Epoch 44/107 - Avg Training Loss: 19.596679210662842 - Avg Validation Loss: 7.674793004989624\n",
      "Epoch 45/107 - Avg Training Loss: 22.528449058532715 - Avg Validation Loss: 18.987539291381836\n",
      "Epoch 46/107 - Avg Training Loss: 23.647085666656494 - Avg Validation Loss: 6.8634092807769775\n",
      "Epoch 47/107 - Avg Training Loss: 21.468714714050293 - Avg Validation Loss: 11.766531944274902\n",
      "Epoch 48/107 - Avg Training Loss: 22.253480911254883 - Avg Validation Loss: 7.398996829986572\n",
      "Epoch 49/107 - Avg Training Loss: 23.44146203994751 - Avg Validation Loss: 10.896408081054688\n",
      "Epoch 50/107 - Avg Training Loss: 21.243000507354736 - Avg Validation Loss: 12.139877796173096\n",
      "Epoch 51/107 - Avg Training Loss: 21.98036813735962 - Avg Validation Loss: 7.149920463562012\n",
      "Epoch 52/107 - Avg Training Loss: 22.18021535873413 - Avg Validation Loss: 11.73429250717163\n",
      "Epoch 53/107 - Avg Training Loss: 19.818986892700195 - Avg Validation Loss: 10.367104053497314\n",
      "Epoch 54/107 - Avg Training Loss: 21.729716777801514 - Avg Validation Loss: 7.007217645645142\n",
      "Epoch 55/107 - Avg Training Loss: 21.494665145874023 - Avg Validation Loss: 12.448004722595215\n",
      "Epoch 56/107 - Avg Training Loss: 20.4114089012146 - Avg Validation Loss: 8.252273559570312\n",
      "Epoch 57/107 - Avg Training Loss: 21.088200092315674 - Avg Validation Loss: 8.189519882202148\n",
      "Epoch 58/107 - Avg Training Loss: 22.430222988128662 - Avg Validation Loss: 14.65948486328125\n",
      "Epoch 59/107 - Avg Training Loss: 21.30653953552246 - Avg Validation Loss: 8.233717441558838\n",
      "Epoch 60/107 - Avg Training Loss: 20.56370449066162 - Avg Validation Loss: 13.576730728149414\n",
      "Epoch 61/107 - Avg Training Loss: 21.00658416748047 - Avg Validation Loss: 7.178180456161499\n",
      "Epoch 62/107 - Avg Training Loss: 21.55090618133545 - Avg Validation Loss: 11.352583885192871\n",
      "Epoch 63/107 - Avg Training Loss: 20.778811931610107 - Avg Validation Loss: 8.458931922912598\n",
      "Epoch 64/107 - Avg Training Loss: 20.283499240875244 - Avg Validation Loss: 10.017229080200195\n",
      "Epoch 65/107 - Avg Training Loss: 21.310871601104736 - Avg Validation Loss: 9.13696002960205\n",
      "Epoch 66/107 - Avg Training Loss: 19.1919105052948 - Avg Validation Loss: 8.736446380615234\n",
      "Epoch 67/107 - Avg Training Loss: 21.364400386810303 - Avg Validation Loss: 13.060012817382812\n",
      "Epoch 68/107 - Avg Training Loss: 19.03015899658203 - Avg Validation Loss: 7.002126216888428\n",
      "Epoch 69/107 - Avg Training Loss: 19.811131477355957 - Avg Validation Loss: 15.326528072357178\n",
      "Epoch 70/107 - Avg Training Loss: 21.5708966255188 - Avg Validation Loss: 7.288208961486816\n",
      "Epoch 71/107 - Avg Training Loss: 21.78725242614746 - Avg Validation Loss: 7.984399318695068\n",
      "Epoch 72/107 - Avg Training Loss: 19.80346155166626 - Avg Validation Loss: 10.67998456954956\n",
      "Epoch 73/107 - Avg Training Loss: 19.623640537261963 - Avg Validation Loss: 8.625617504119873\n",
      "Epoch 74/107 - Avg Training Loss: 19.64210796356201 - Avg Validation Loss: 11.973022937774658\n",
      "Epoch 75/107 - Avg Training Loss: 20.23417568206787 - Avg Validation Loss: 7.974612474441528\n",
      "Epoch 76/107 - Avg Training Loss: 18.61506414413452 - Avg Validation Loss: 9.076231956481934\n",
      "Epoch 77/107 - Avg Training Loss: 19.798891067504883 - Avg Validation Loss: 8.31418514251709\n",
      "Epoch 78/107 - Avg Training Loss: 20.58323907852173 - Avg Validation Loss: 10.307259559631348\n",
      "Epoch 79/107 - Avg Training Loss: 20.111770629882812 - Avg Validation Loss: 10.00883960723877\n",
      "Epoch 80/107 - Avg Training Loss: 18.725086212158203 - Avg Validation Loss: 9.575009822845459\n",
      "Epoch 81/107 - Avg Training Loss: 20.03768301010132 - Avg Validation Loss: 12.393805503845215\n",
      "Epoch 82/107 - Avg Training Loss: 19.150848388671875 - Avg Validation Loss: 7.767005205154419\n",
      "Epoch 83/107 - Avg Training Loss: 19.47404146194458 - Avg Validation Loss: 10.712736129760742\n",
      "Epoch 84/107 - Avg Training Loss: 20.2157301902771 - Avg Validation Loss: 6.1804914474487305\n",
      "Epoch 85/107 - Avg Training Loss: 19.717055559158325 - Avg Validation Loss: 14.704269886016846\n",
      "Epoch 86/107 - Avg Training Loss: 20.337697982788086 - Avg Validation Loss: 8.916637420654297\n",
      "Epoch 87/107 - Avg Training Loss: 20.54242467880249 - Avg Validation Loss: 6.484065294265747\n",
      "Epoch 88/107 - Avg Training Loss: 20.913785457611084 - Avg Validation Loss: 11.686460018157959\n",
      "Epoch 89/107 - Avg Training Loss: 18.43024778366089 - Avg Validation Loss: 7.080944061279297\n",
      "Epoch 90/107 - Avg Training Loss: 19.94666862487793 - Avg Validation Loss: 16.852020263671875\n",
      "Epoch 91/107 - Avg Training Loss: 19.535268783569336 - Avg Validation Loss: 7.500495910644531\n",
      "Epoch 92/107 - Avg Training Loss: 18.769408226013184 - Avg Validation Loss: 11.483972549438477\n",
      "Epoch 93/107 - Avg Training Loss: 20.944632053375244 - Avg Validation Loss: 14.09026050567627\n",
      "Epoch 94/107 - Avg Training Loss: 17.66416573524475 - Avg Validation Loss: 5.830103158950806\n",
      "Epoch 95/107 - Avg Training Loss: 19.330278873443604 - Avg Validation Loss: 15.739185333251953\n",
      "Epoch 96/107 - Avg Training Loss: 19.957934379577637 - Avg Validation Loss: 8.411184787750244\n",
      "Epoch 97/107 - Avg Training Loss: 21.66613006591797 - Avg Validation Loss: 12.876955509185791\n",
      "Epoch 98/107 - Avg Training Loss: 20.06834363937378 - Avg Validation Loss: 7.00106954574585\n",
      "Epoch 99/107 - Avg Training Loss: 20.12783908843994 - Avg Validation Loss: 13.453816413879395\n",
      "Epoch 100/107 - Avg Training Loss: 20.623162269592285 - Avg Validation Loss: 6.4245924949646\n",
      "Epoch 101/107 - Avg Training Loss: 18.765196800231934 - Avg Validation Loss: 16.536179542541504\n",
      "Epoch 102/107 - Avg Training Loss: 20.027930736541748 - Avg Validation Loss: 4.847608804702759\n",
      "Epoch 103/107 - Avg Training Loss: 22.00358486175537 - Avg Validation Loss: 13.295076847076416\n",
      "Epoch 104/107 - Avg Training Loss: 18.7617506980896 - Avg Validation Loss: 7.758167266845703\n",
      "Epoch 105/107 - Avg Training Loss: 17.66835379600525 - Avg Validation Loss: 6.507617235183716\n",
      "Epoch 106/107 - Avg Training Loss: 19.270538806915283 - Avg Validation Loss: 9.532469749450684\n",
      "Epoch 107/107 - Avg Training Loss: 19.14712381362915 - Avg Validation Loss: 9.52504587173462\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:48,662] Trial 5 finished with value: 9.52504587173462 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 2, 'dropout_rate': 0.5970347181491065, 'lr': 0.006304964516509327, 'batch_size': 464, 'epochs': 107}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/84 - Avg Training Loss: 158.56188042958578 - Avg Validation Loss: 143.0718116760254\n",
      "Epoch 2/84 - Avg Training Loss: 81.07589149475098 - Avg Validation Loss: 11.330080270767212\n",
      "Epoch 3/84 - Avg Training Loss: 35.10087903340658 - Avg Validation Loss: 24.42097568511963\n",
      "Epoch 4/84 - Avg Training Loss: 28.181907335917156 - Avg Validation Loss: 19.228286266326904\n",
      "Epoch 5/84 - Avg Training Loss: 26.251002311706543 - Avg Validation Loss: 12.818930625915527\n",
      "Epoch 6/84 - Avg Training Loss: 20.818750381469727 - Avg Validation Loss: 8.737242460250854\n",
      "Epoch 7/84 - Avg Training Loss: 20.595938364664715 - Avg Validation Loss: 12.179121971130371\n",
      "Epoch 8/84 - Avg Training Loss: 17.708049933115642 - Avg Validation Loss: 13.702928066253662\n",
      "Epoch 9/84 - Avg Training Loss: 17.013616402943928 - Avg Validation Loss: 9.028743743896484\n",
      "Epoch 10/84 - Avg Training Loss: 16.752067406972248 - Avg Validation Loss: 8.14200210571289\n",
      "Epoch 11/84 - Avg Training Loss: 15.810545126597086 - Avg Validation Loss: 8.610475301742554\n",
      "Epoch 12/84 - Avg Training Loss: 16.267905394236248 - Avg Validation Loss: 8.055291414260864\n",
      "Epoch 13/84 - Avg Training Loss: 15.983625729878744 - Avg Validation Loss: 7.543563365936279\n",
      "Epoch 14/84 - Avg Training Loss: 14.603681087493896 - Avg Validation Loss: 7.852467060089111\n",
      "Epoch 15/84 - Avg Training Loss: 13.371217409769693 - Avg Validation Loss: 7.185722827911377\n",
      "Epoch 16/84 - Avg Training Loss: 13.90429131189982 - Avg Validation Loss: 7.916869282722473\n",
      "Epoch 17/84 - Avg Training Loss: 13.288453896840414 - Avg Validation Loss: 7.820108413696289\n",
      "Epoch 18/84 - Avg Training Loss: 13.147090593973795 - Avg Validation Loss: 7.440444827079773\n",
      "Epoch 19/84 - Avg Training Loss: 12.779139200846354 - Avg Validation Loss: 7.318405508995056\n",
      "Epoch 20/84 - Avg Training Loss: 13.57595157623291 - Avg Validation Loss: 7.26179301738739\n",
      "Epoch 21/84 - Avg Training Loss: 13.276304880777994 - Avg Validation Loss: 6.156670808792114\n",
      "Epoch 22/84 - Avg Training Loss: 13.359532197316488 - Avg Validation Loss: 6.559751987457275\n",
      "Epoch 23/84 - Avg Training Loss: 12.857340971628824 - Avg Validation Loss: 6.8160436153411865\n",
      "Epoch 24/84 - Avg Training Loss: 12.945453643798828 - Avg Validation Loss: 8.031789779663086\n",
      "Epoch 25/84 - Avg Training Loss: 12.89822800954183 - Avg Validation Loss: 10.62223219871521\n",
      "Epoch 26/84 - Avg Training Loss: 12.720839182535807 - Avg Validation Loss: 9.751078128814697\n",
      "Epoch 27/84 - Avg Training Loss: 12.821887969970703 - Avg Validation Loss: 6.2291330099105835\n",
      "Epoch 28/84 - Avg Training Loss: 13.167871316274008 - Avg Validation Loss: 4.602349042892456\n",
      "Epoch 29/84 - Avg Training Loss: 13.808422883351644 - Avg Validation Loss: 4.8857786655426025\n",
      "Epoch 30/84 - Avg Training Loss: 13.541183471679688 - Avg Validation Loss: 8.046290636062622\n",
      "Epoch 31/84 - Avg Training Loss: 13.12384033203125 - Avg Validation Loss: 8.259422779083252\n",
      "Epoch 32/84 - Avg Training Loss: 12.744798501332602 - Avg Validation Loss: 7.089151620864868\n",
      "Epoch 33/84 - Avg Training Loss: 12.446578025817871 - Avg Validation Loss: 6.065949201583862\n",
      "Epoch 34/84 - Avg Training Loss: 12.087770779927572 - Avg Validation Loss: 6.809266448020935\n",
      "Epoch 35/84 - Avg Training Loss: 12.620566685994467 - Avg Validation Loss: 8.733274459838867\n",
      "Epoch 36/84 - Avg Training Loss: 12.122736295064291 - Avg Validation Loss: 10.625807046890259\n",
      "Epoch 37/84 - Avg Training Loss: 12.44808022181193 - Avg Validation Loss: 10.102067470550537\n",
      "Epoch 38/84 - Avg Training Loss: 12.14082145690918 - Avg Validation Loss: 6.208998918533325\n",
      "Epoch 39/84 - Avg Training Loss: 11.83806816736857 - Avg Validation Loss: 6.84812605381012\n",
      "Epoch 40/84 - Avg Training Loss: 12.23516050974528 - Avg Validation Loss: 5.9334553480148315\n",
      "Epoch 41/84 - Avg Training Loss: 12.635362148284912 - Avg Validation Loss: 11.80285120010376\n",
      "Epoch 42/84 - Avg Training Loss: 12.572153409322103 - Avg Validation Loss: 6.218259334564209\n",
      "Epoch 43/84 - Avg Training Loss: 11.935422897338867 - Avg Validation Loss: 6.237635493278503\n",
      "Epoch 44/84 - Avg Training Loss: 11.529256025950113 - Avg Validation Loss: 6.357084631919861\n",
      "Epoch 45/84 - Avg Training Loss: 11.652618567148844 - Avg Validation Loss: 6.366878271102905\n",
      "Epoch 46/84 - Avg Training Loss: 12.201138178507486 - Avg Validation Loss: 7.688736438751221\n",
      "Epoch 47/84 - Avg Training Loss: 12.033164819081625 - Avg Validation Loss: 7.347105860710144\n",
      "Epoch 48/84 - Avg Training Loss: 11.848114490509033 - Avg Validation Loss: 4.8534111976623535\n",
      "Epoch 49/84 - Avg Training Loss: 12.268491903940836 - Avg Validation Loss: 6.883309364318848\n",
      "Epoch 50/84 - Avg Training Loss: 11.54973872502645 - Avg Validation Loss: 9.01918625831604\n",
      "Epoch 51/84 - Avg Training Loss: 11.363213857014975 - Avg Validation Loss: 6.523053884506226\n",
      "Epoch 52/84 - Avg Training Loss: 12.260080178578695 - Avg Validation Loss: 12.658352851867676\n",
      "Epoch 53/84 - Avg Training Loss: 12.088922341664633 - Avg Validation Loss: 7.896859288215637\n",
      "Epoch 54/84 - Avg Training Loss: 11.765008767445883 - Avg Validation Loss: 5.614173173904419\n",
      "Epoch 55/84 - Avg Training Loss: 11.66039768854777 - Avg Validation Loss: 5.378198266029358\n",
      "Epoch 56/84 - Avg Training Loss: 11.924482504526773 - Avg Validation Loss: 6.988637804985046\n",
      "Epoch 57/84 - Avg Training Loss: 12.365379492441813 - Avg Validation Loss: 12.63560700416565\n",
      "Epoch 58/84 - Avg Training Loss: 12.870939095815023 - Avg Validation Loss: 7.218790292739868\n",
      "Epoch 59/84 - Avg Training Loss: 11.520962556203207 - Avg Validation Loss: 5.779800176620483\n",
      "Epoch 60/84 - Avg Training Loss: 10.976106643676758 - Avg Validation Loss: 8.029030442237854\n",
      "Epoch 61/84 - Avg Training Loss: 11.677638371785482 - Avg Validation Loss: 8.255257964134216\n",
      "Epoch 62/84 - Avg Training Loss: 12.290875911712646 - Avg Validation Loss: 10.863926887512207\n",
      "Epoch 63/84 - Avg Training Loss: 11.891921838124594 - Avg Validation Loss: 5.782190918922424\n",
      "Epoch 64/84 - Avg Training Loss: 11.557608127593994 - Avg Validation Loss: 3.9951239228248596\n",
      "Epoch 65/84 - Avg Training Loss: 13.48737621307373 - Avg Validation Loss: 14.40257477760315\n",
      "Epoch 66/84 - Avg Training Loss: 15.012198607126871 - Avg Validation Loss: 15.789653539657593\n",
      "Epoch 67/84 - Avg Training Loss: 13.242666562398275 - Avg Validation Loss: 6.375073432922363\n",
      "Epoch 68/84 - Avg Training Loss: 12.200895627339682 - Avg Validation Loss: 5.413496017456055\n",
      "Epoch 69/84 - Avg Training Loss: 12.043959458669027 - Avg Validation Loss: 6.75850772857666\n",
      "Epoch 70/84 - Avg Training Loss: 11.416127363840738 - Avg Validation Loss: 7.295904278755188\n",
      "Epoch 71/84 - Avg Training Loss: 11.698771635691324 - Avg Validation Loss: 4.842455863952637\n",
      "Epoch 72/84 - Avg Training Loss: 11.065340360005697 - Avg Validation Loss: 7.290102243423462\n",
      "Epoch 73/84 - Avg Training Loss: 11.475484371185303 - Avg Validation Loss: 7.368539929389954\n",
      "Epoch 74/84 - Avg Training Loss: 11.302902380625406 - Avg Validation Loss: 5.7332282066345215\n",
      "Epoch 75/84 - Avg Training Loss: 10.954116185506185 - Avg Validation Loss: 6.401318669319153\n",
      "Epoch 76/84 - Avg Training Loss: 10.982551097869873 - Avg Validation Loss: 4.571424603462219\n",
      "Epoch 77/84 - Avg Training Loss: 11.639294147491455 - Avg Validation Loss: 6.137796759605408\n",
      "Epoch 78/84 - Avg Training Loss: 11.215015252431234 - Avg Validation Loss: 8.052250385284424\n",
      "Epoch 79/84 - Avg Training Loss: 11.37321138381958 - Avg Validation Loss: 7.346123814582825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:51,620] Trial 6 finished with value: 6.776798486709595 and parameters: {'hidden_layer_size': 256, 'hidden_layer_count': 2, 'dropout_rate': 0.5701769543739879, 'lr': 0.008363548353318394, 'batch_size': 240, 'epochs': 84}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/84 - Avg Training Loss: 10.900962988535563 - Avg Validation Loss: 7.083227872848511\n",
      "Epoch 81/84 - Avg Training Loss: 10.862138112386068 - Avg Validation Loss: 4.41532039642334\n",
      "Epoch 82/84 - Avg Training Loss: 10.983968575795492 - Avg Validation Loss: 4.797054767608643\n",
      "Epoch 83/84 - Avg Training Loss: 10.523609638214111 - Avg Validation Loss: 10.723626375198364\n",
      "Epoch 84/84 - Avg Training Loss: 10.914872646331787 - Avg Validation Loss: 6.776798486709595\n",
      "Finished Training\n",
      "Epoch 1/71 - Avg Training Loss: 370.06323528289795 - Avg Validation Loss: 126.63217671712239\n",
      "Epoch 2/71 - Avg Training Loss: 230.0297908782959 - Avg Validation Loss: 301.89124552408856\n",
      "Epoch 3/71 - Avg Training Loss: 287.7536926269531 - Avg Validation Loss: 270.8472493489583\n",
      "Epoch 4/71 - Avg Training Loss: 235.56149291992188 - Avg Validation Loss: 175.7559356689453\n",
      "Epoch 5/71 - Avg Training Loss: 120.72885704040527 - Avg Validation Loss: 26.716875712076824\n",
      "Epoch 6/71 - Avg Training Loss: 96.08656883239746 - Avg Validation Loss: 18.898708979288738\n",
      "Epoch 7/71 - Avg Training Loss: 60.056105613708496 - Avg Validation Loss: 56.44257481892904\n",
      "Epoch 8/71 - Avg Training Loss: 55.50538921356201 - Avg Validation Loss: 26.591105143229168\n",
      "Epoch 9/71 - Avg Training Loss: 39.05207538604736 - Avg Validation Loss: 12.136405944824219\n",
      "Epoch 10/71 - Avg Training Loss: 34.71175765991211 - Avg Validation Loss: 29.649691899617512\n",
      "Epoch 11/71 - Avg Training Loss: 29.35086727142334 - Avg Validation Loss: 12.060404777526855\n",
      "Epoch 12/71 - Avg Training Loss: 27.443450927734375 - Avg Validation Loss: 20.393794377644856\n",
      "Epoch 13/71 - Avg Training Loss: 23.27921152114868 - Avg Validation Loss: 18.416849772135418\n",
      "Epoch 14/71 - Avg Training Loss: 21.619943141937256 - Avg Validation Loss: 21.134234110514324\n",
      "Epoch 15/71 - Avg Training Loss: 18.750417709350586 - Avg Validation Loss: 21.683677037556965\n",
      "Epoch 16/71 - Avg Training Loss: 17.02120876312256 - Avg Validation Loss: 23.80903498331706\n",
      "Epoch 17/71 - Avg Training Loss: 16.435463428497314 - Avg Validation Loss: 22.93397331237793\n",
      "Epoch 18/71 - Avg Training Loss: 15.55735158920288 - Avg Validation Loss: 33.35544967651367\n",
      "Epoch 19/71 - Avg Training Loss: 15.049505472183228 - Avg Validation Loss: 25.30347506205241\n",
      "Epoch 20/71 - Avg Training Loss: 14.271892786026001 - Avg Validation Loss: 36.809427897135414\n",
      "Epoch 21/71 - Avg Training Loss: 13.022532224655151 - Avg Validation Loss: 31.33425458272298\n",
      "Epoch 22/71 - Avg Training Loss: 12.196617841720581 - Avg Validation Loss: 34.97358957926432\n",
      "Epoch 23/71 - Avg Training Loss: 12.010642290115356 - Avg Validation Loss: 31.865012486775715\n",
      "Epoch 24/71 - Avg Training Loss: 11.748139142990112 - Avg Validation Loss: 34.74225616455078\n",
      "Epoch 25/71 - Avg Training Loss: 11.360246181488037 - Avg Validation Loss: 34.42959976196289\n",
      "Epoch 26/71 - Avg Training Loss: 11.110769748687744 - Avg Validation Loss: 34.732242584228516\n",
      "Epoch 27/71 - Avg Training Loss: 11.001911163330078 - Avg Validation Loss: 33.8237190246582\n",
      "Epoch 28/71 - Avg Training Loss: 10.543420553207397 - Avg Validation Loss: 34.218695322672524\n",
      "Epoch 29/71 - Avg Training Loss: 10.330352306365967 - Avg Validation Loss: 35.60619354248047\n",
      "Epoch 30/71 - Avg Training Loss: 10.564064741134644 - Avg Validation Loss: 37.54462178548177\n",
      "Epoch 31/71 - Avg Training Loss: 10.020700216293335 - Avg Validation Loss: 38.459710439046226\n",
      "Epoch 32/71 - Avg Training Loss: 9.433658838272095 - Avg Validation Loss: 33.45779546101888\n",
      "Epoch 33/71 - Avg Training Loss: 9.786154985427856 - Avg Validation Loss: 37.748006184895836\n",
      "Epoch 34/71 - Avg Training Loss: 9.769686698913574 - Avg Validation Loss: 32.90421994527181\n",
      "Epoch 35/71 - Avg Training Loss: 9.536847591400146 - Avg Validation Loss: 40.07833353678385\n",
      "Epoch 36/71 - Avg Training Loss: 9.188525199890137 - Avg Validation Loss: 37.27543640136719\n",
      "Epoch 37/71 - Avg Training Loss: 8.83774185180664 - Avg Validation Loss: 42.5369873046875\n",
      "Epoch 38/71 - Avg Training Loss: 9.472844362258911 - Avg Validation Loss: 35.863416035970054\n",
      "Epoch 39/71 - Avg Training Loss: 9.148185014724731 - Avg Validation Loss: 43.30973434448242\n",
      "Epoch 40/71 - Avg Training Loss: 8.97001838684082 - Avg Validation Loss: 36.84807586669922\n",
      "Epoch 41/71 - Avg Training Loss: 8.951026201248169 - Avg Validation Loss: 38.7549196879069\n",
      "Epoch 42/71 - Avg Training Loss: 8.55457353591919 - Avg Validation Loss: 35.747423807779946\n",
      "Epoch 43/71 - Avg Training Loss: 8.208197832107544 - Avg Validation Loss: 36.47018305460612\n",
      "Epoch 44/71 - Avg Training Loss: 8.43327260017395 - Avg Validation Loss: 36.63906478881836\n",
      "Epoch 45/71 - Avg Training Loss: 8.10117483139038 - Avg Validation Loss: 37.808250427246094\n",
      "Epoch 46/71 - Avg Training Loss: 8.053085565567017 - Avg Validation Loss: 37.97932434082031\n",
      "Epoch 47/71 - Avg Training Loss: 7.706945061683655 - Avg Validation Loss: 36.5366325378418\n",
      "Epoch 48/71 - Avg Training Loss: 7.916454076766968 - Avg Validation Loss: 39.42247517903646\n",
      "Epoch 49/71 - Avg Training Loss: 8.21029531955719 - Avg Validation Loss: 37.193562825520836\n",
      "Epoch 50/71 - Avg Training Loss: 7.799670696258545 - Avg Validation Loss: 40.15417226155599\n",
      "Epoch 51/71 - Avg Training Loss: 7.897226929664612 - Avg Validation Loss: 37.809319814046226\n",
      "Epoch 52/71 - Avg Training Loss: 7.926998138427734 - Avg Validation Loss: 36.66149648030599\n",
      "Epoch 53/71 - Avg Training Loss: 7.948611259460449 - Avg Validation Loss: 36.45372772216797\n",
      "Epoch 54/71 - Avg Training Loss: 7.798259019851685 - Avg Validation Loss: 39.07477951049805\n",
      "Epoch 55/71 - Avg Training Loss: 7.7516045570373535 - Avg Validation Loss: 38.527600606282554\n",
      "Epoch 56/71 - Avg Training Loss: 7.596614360809326 - Avg Validation Loss: 29.360047658284504\n",
      "Epoch 57/71 - Avg Training Loss: 7.710323929786682 - Avg Validation Loss: 38.14740244547526\n",
      "Epoch 58/71 - Avg Training Loss: 7.701007723808289 - Avg Validation Loss: 38.07966105143229\n",
      "Epoch 59/71 - Avg Training Loss: 7.891724467277527 - Avg Validation Loss: 32.43421936035156\n",
      "Epoch 60/71 - Avg Training Loss: 7.197772860527039 - Avg Validation Loss: 39.39959208170573\n",
      "Epoch 61/71 - Avg Training Loss: 7.195106148719788 - Avg Validation Loss: 37.40960947672526\n",
      "Epoch 62/71 - Avg Training Loss: 7.0083171129226685 - Avg Validation Loss: 40.12906392415365\n",
      "Epoch 63/71 - Avg Training Loss: 7.403989911079407 - Avg Validation Loss: 37.946119944254555\n",
      "Epoch 64/71 - Avg Training Loss: 6.988668322563171 - Avg Validation Loss: 43.5990358988444\n",
      "Epoch 65/71 - Avg Training Loss: 7.656829833984375 - Avg Validation Loss: 39.072149912516274\n",
      "Epoch 66/71 - Avg Training Loss: 6.875411629676819 - Avg Validation Loss: 39.00337219238281\n",
      "Epoch 67/71 - Avg Training Loss: 7.077811598777771 - Avg Validation Loss: 39.04968388875326\n",
      "Epoch 68/71 - Avg Training Loss: 6.926784157752991 - Avg Validation Loss: 39.49068832397461\n",
      "Epoch 69/71 - Avg Training Loss: 7.2598360776901245 - Avg Validation Loss: 41.46855545043945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:44:55,226] Trial 7 finished with value: 39.76143137613932 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 2, 'dropout_rate': 0.4597948647299921, 'lr': 0.006348083968503592, 'batch_size': 400, 'epochs': 71}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/71 - Avg Training Loss: 6.8039870262146 - Avg Validation Loss: 38.57983525594076\n",
      "Epoch 71/71 - Avg Training Loss: 7.012158989906311 - Avg Validation Loss: 39.76143137613932\n",
      "Finished Training\n",
      "Epoch 1/87 - Avg Training Loss: 287.50003655751544 - Avg Validation Loss: 270.4647750854492\n",
      "Epoch 2/87 - Avg Training Loss: 280.5655263264974 - Avg Validation Loss: 252.4462013244629\n",
      "Epoch 3/87 - Avg Training Loss: 176.68259811401367 - Avg Validation Loss: 41.17543697357178\n",
      "Epoch 4/87 - Avg Training Loss: 73.52748616536458 - Avg Validation Loss: 21.541972637176514\n",
      "Epoch 5/87 - Avg Training Loss: 50.057116190592446 - Avg Validation Loss: 33.78461408615112\n",
      "Epoch 6/87 - Avg Training Loss: 37.050964991251625 - Avg Validation Loss: 14.594756603240967\n",
      "Epoch 7/87 - Avg Training Loss: 32.15179443359375 - Avg Validation Loss: 19.8364896774292\n",
      "Epoch 8/87 - Avg Training Loss: 25.246574719746906 - Avg Validation Loss: 9.38593602180481\n",
      "Epoch 9/87 - Avg Training Loss: 20.984869639078777 - Avg Validation Loss: 13.014628887176514\n",
      "Epoch 10/87 - Avg Training Loss: 18.889769554138184 - Avg Validation Loss: 10.990105867385864\n",
      "Epoch 11/87 - Avg Training Loss: 17.475879828135174 - Avg Validation Loss: 8.223766446113586\n",
      "Epoch 12/87 - Avg Training Loss: 16.784685452779133 - Avg Validation Loss: 11.942326784133911\n",
      "Epoch 13/87 - Avg Training Loss: 15.667890548706055 - Avg Validation Loss: 11.407764196395874\n",
      "Epoch 14/87 - Avg Training Loss: 14.746801535288492 - Avg Validation Loss: 9.225417852401733\n",
      "Epoch 15/87 - Avg Training Loss: 15.122569878896078 - Avg Validation Loss: 11.338901996612549\n",
      "Epoch 16/87 - Avg Training Loss: 13.545887152353922 - Avg Validation Loss: 9.922963261604309\n",
      "Epoch 17/87 - Avg Training Loss: 12.993436495463053 - Avg Validation Loss: 11.615611553192139\n",
      "Epoch 18/87 - Avg Training Loss: 12.87189785639445 - Avg Validation Loss: 11.132323741912842\n",
      "Epoch 19/87 - Avg Training Loss: 12.181511243184408 - Avg Validation Loss: 10.51917576789856\n",
      "Epoch 20/87 - Avg Training Loss: 11.94278065363566 - Avg Validation Loss: 13.27309274673462\n",
      "Epoch 21/87 - Avg Training Loss: 11.967858155568441 - Avg Validation Loss: 15.61847448348999\n",
      "Epoch 22/87 - Avg Training Loss: 12.167853514353434 - Avg Validation Loss: 9.008740544319153\n",
      "Epoch 23/87 - Avg Training Loss: 11.308604717254639 - Avg Validation Loss: 11.581058263778687\n",
      "Epoch 24/87 - Avg Training Loss: 11.708210150400797 - Avg Validation Loss: 14.139969825744629\n",
      "Epoch 25/87 - Avg Training Loss: 11.47007417678833 - Avg Validation Loss: 17.623956203460693\n",
      "Epoch 26/87 - Avg Training Loss: 11.12263854344686 - Avg Validation Loss: 12.224375009536743\n",
      "Epoch 27/87 - Avg Training Loss: 9.904458204905191 - Avg Validation Loss: 10.382980585098267\n",
      "Epoch 28/87 - Avg Training Loss: 10.226443608601889 - Avg Validation Loss: 10.121679306030273\n",
      "Epoch 29/87 - Avg Training Loss: 10.142028331756592 - Avg Validation Loss: 11.775179386138916\n",
      "Epoch 30/87 - Avg Training Loss: 10.191680590311686 - Avg Validation Loss: 14.863648653030396\n",
      "Epoch 31/87 - Avg Training Loss: 10.236926078796387 - Avg Validation Loss: 12.370961904525757\n",
      "Epoch 32/87 - Avg Training Loss: 9.831985791524252 - Avg Validation Loss: 12.807957887649536\n",
      "Epoch 33/87 - Avg Training Loss: 9.845158259073893 - Avg Validation Loss: 14.368705034255981\n",
      "Epoch 34/87 - Avg Training Loss: 9.637706915537516 - Avg Validation Loss: 13.168304681777954\n",
      "Epoch 35/87 - Avg Training Loss: 10.352434635162354 - Avg Validation Loss: 12.371304512023926\n",
      "Epoch 36/87 - Avg Training Loss: 9.21608273188273 - Avg Validation Loss: 13.686166524887085\n",
      "Epoch 37/87 - Avg Training Loss: 9.54375727971395 - Avg Validation Loss: 14.387109994888306\n",
      "Epoch 38/87 - Avg Training Loss: 9.230340798695883 - Avg Validation Loss: 14.316039800643921\n",
      "Epoch 39/87 - Avg Training Loss: 8.969507137934366 - Avg Validation Loss: 14.238811731338501\n",
      "Epoch 40/87 - Avg Training Loss: 8.778799692789713 - Avg Validation Loss: 14.38857364654541\n",
      "Epoch 41/87 - Avg Training Loss: 8.5923490524292 - Avg Validation Loss: 12.560913801193237\n",
      "Epoch 42/87 - Avg Training Loss: 9.357007026672363 - Avg Validation Loss: 11.148948431015015\n",
      "Epoch 43/87 - Avg Training Loss: 8.98648993174235 - Avg Validation Loss: 12.140423774719238\n",
      "Epoch 44/87 - Avg Training Loss: 9.141506354014078 - Avg Validation Loss: 12.14722990989685\n",
      "Epoch 45/87 - Avg Training Loss: 9.255710283915201 - Avg Validation Loss: 14.967362403869629\n",
      "Epoch 46/87 - Avg Training Loss: 8.77233362197876 - Avg Validation Loss: 13.673424482345581\n",
      "Epoch 47/87 - Avg Training Loss: 9.786799907684326 - Avg Validation Loss: 11.922948122024536\n",
      "Epoch 48/87 - Avg Training Loss: 9.215733210245768 - Avg Validation Loss: 13.529266357421875\n",
      "Epoch 49/87 - Avg Training Loss: 9.137040297190348 - Avg Validation Loss: 11.164437770843506\n",
      "Epoch 50/87 - Avg Training Loss: 8.71244192123413 - Avg Validation Loss: 13.547061204910278\n",
      "Epoch 51/87 - Avg Training Loss: 9.258326053619385 - Avg Validation Loss: 16.347227811813354\n",
      "Epoch 52/87 - Avg Training Loss: 8.501039187113443 - Avg Validation Loss: 16.044196605682373\n",
      "Epoch 53/87 - Avg Training Loss: 8.7270401318868 - Avg Validation Loss: 13.729576110839844\n",
      "Epoch 54/87 - Avg Training Loss: 8.74759022394816 - Avg Validation Loss: 14.300128936767578\n",
      "Epoch 55/87 - Avg Training Loss: 8.6831796169281 - Avg Validation Loss: 12.495500326156616\n",
      "Epoch 56/87 - Avg Training Loss: 8.535793224970499 - Avg Validation Loss: 12.99899172782898\n",
      "Epoch 57/87 - Avg Training Loss: 8.320423285166422 - Avg Validation Loss: 13.90625262260437\n",
      "Epoch 58/87 - Avg Training Loss: 8.294577519098917 - Avg Validation Loss: 14.23821473121643\n",
      "Epoch 59/87 - Avg Training Loss: 8.368020137151083 - Avg Validation Loss: 14.18468976020813\n",
      "Epoch 60/87 - Avg Training Loss: 7.990087191263835 - Avg Validation Loss: 15.12481141090393\n",
      "Epoch 61/87 - Avg Training Loss: 7.904696782430013 - Avg Validation Loss: 13.932131052017212\n",
      "Epoch 62/87 - Avg Training Loss: 7.895913044611613 - Avg Validation Loss: 11.453827142715454\n",
      "Epoch 63/87 - Avg Training Loss: 8.260405619939169 - Avg Validation Loss: 12.486373901367188\n",
      "Epoch 64/87 - Avg Training Loss: 8.157139301300049 - Avg Validation Loss: 14.857264041900635\n",
      "Epoch 65/87 - Avg Training Loss: 7.993729035059611 - Avg Validation Loss: 17.76760196685791\n",
      "Epoch 66/87 - Avg Training Loss: 8.248088359832764 - Avg Validation Loss: 20.905356884002686\n",
      "Epoch 67/87 - Avg Training Loss: 8.24972407023112 - Avg Validation Loss: 18.022958755493164\n",
      "Epoch 68/87 - Avg Training Loss: 8.546214580535889 - Avg Validation Loss: 20.508066654205322\n",
      "Epoch 69/87 - Avg Training Loss: 9.0692138671875 - Avg Validation Loss: 17.158010005950928\n",
      "Epoch 70/87 - Avg Training Loss: 8.306394020716349 - Avg Validation Loss: 14.127223491668701\n",
      "Epoch 71/87 - Avg Training Loss: 8.2623344262441 - Avg Validation Loss: 13.504908323287964\n",
      "Epoch 72/87 - Avg Training Loss: 8.173914273579916 - Avg Validation Loss: 11.060049057006836\n",
      "Epoch 73/87 - Avg Training Loss: 7.70684289932251 - Avg Validation Loss: 11.830143213272095\n",
      "Epoch 74/87 - Avg Training Loss: 7.825807094573975 - Avg Validation Loss: 11.408826351165771\n",
      "Epoch 75/87 - Avg Training Loss: 8.607923746109009 - Avg Validation Loss: 14.454324007034302\n",
      "Epoch 76/87 - Avg Training Loss: 7.662237644195557 - Avg Validation Loss: 14.107710361480713\n",
      "Epoch 77/87 - Avg Training Loss: 7.874970595041911 - Avg Validation Loss: 16.636573314666748\n",
      "Epoch 78/87 - Avg Training Loss: 7.395752588907878 - Avg Validation Loss: 15.464364528656006\n",
      "Epoch 79/87 - Avg Training Loss: 7.662043412526448 - Avg Validation Loss: 17.56384801864624\n",
      "Epoch 80/87 - Avg Training Loss: 7.647864421208699 - Avg Validation Loss: 15.901593208312988\n",
      "Epoch 81/87 - Avg Training Loss: 7.611541350682576 - Avg Validation Loss: 15.47783899307251\n",
      "Epoch 82/87 - Avg Training Loss: 7.25476058324178 - Avg Validation Loss: 12.310804843902588\n",
      "Epoch 83/87 - Avg Training Loss: 7.884703795115153 - Avg Validation Loss: 13.064892053604126\n",
      "Epoch 84/87 - Avg Training Loss: 7.7575048605601 - Avg Validation Loss: 14.885338544845581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:01,603] Trial 8 finished with value: 17.069628715515137 and parameters: {'hidden_layer_size': 700, 'hidden_layer_count': 2, 'dropout_rate': 0.5446211142479822, 'lr': 0.005355532105000473, 'batch_size': 272, 'epochs': 87}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/87 - Avg Training Loss: 7.771697600682576 - Avg Validation Loss: 18.30028486251831\n",
      "Epoch 86/87 - Avg Training Loss: 7.5645668506622314 - Avg Validation Loss: 17.25719451904297\n",
      "Epoch 87/87 - Avg Training Loss: 7.368598937988281 - Avg Validation Loss: 17.069628715515137\n",
      "Finished Training\n",
      "Epoch 1/55 - Avg Training Loss: 100.62465116712782 - Avg Validation Loss: 34.566525268554685\n",
      "Epoch 2/55 - Avg Training Loss: 25.79910447862413 - Avg Validation Loss: 18.543157196044923\n",
      "Epoch 3/55 - Avg Training Loss: 18.360359615749783 - Avg Validation Loss: 7.052156734466553\n",
      "Epoch 4/55 - Avg Training Loss: 11.629760318332249 - Avg Validation Loss: 8.124241638183594\n",
      "Epoch 5/55 - Avg Training Loss: 9.511048316955566 - Avg Validation Loss: 8.100978565216064\n",
      "Epoch 6/55 - Avg Training Loss: 10.057641771104601 - Avg Validation Loss: 5.060882568359375\n",
      "Epoch 7/55 - Avg Training Loss: 9.425861040751139 - Avg Validation Loss: 8.55242748260498\n",
      "Epoch 8/55 - Avg Training Loss: 8.171278953552246 - Avg Validation Loss: 4.611751747131348\n",
      "Epoch 9/55 - Avg Training Loss: 8.701225121816 - Avg Validation Loss: 8.973420715332031\n",
      "Epoch 10/55 - Avg Training Loss: 7.948770682017009 - Avg Validation Loss: 4.7419665336608885\n",
      "Epoch 11/55 - Avg Training Loss: 7.769809299045139 - Avg Validation Loss: 4.954895114898681\n",
      "Epoch 12/55 - Avg Training Loss: 6.890098942650689 - Avg Validation Loss: 5.610533714294434\n",
      "Epoch 13/55 - Avg Training Loss: 6.7860361735026045 - Avg Validation Loss: 4.749222373962402\n",
      "Epoch 14/55 - Avg Training Loss: 7.856330394744873 - Avg Validation Loss: 4.184200763702393\n",
      "Epoch 15/55 - Avg Training Loss: 7.200891017913818 - Avg Validation Loss: 7.532589054107666\n",
      "Epoch 16/55 - Avg Training Loss: 6.968385378519694 - Avg Validation Loss: 7.032958316802978\n",
      "Epoch 17/55 - Avg Training Loss: 7.041797267066108 - Avg Validation Loss: 5.796870231628418\n",
      "Epoch 18/55 - Avg Training Loss: 6.766199217902289 - Avg Validation Loss: 4.74973611831665\n",
      "Epoch 19/55 - Avg Training Loss: 7.146441724565294 - Avg Validation Loss: 4.907437038421631\n",
      "Epoch 20/55 - Avg Training Loss: 7.124217722151014 - Avg Validation Loss: 4.359309387207031\n",
      "Epoch 21/55 - Avg Training Loss: 7.159676922692193 - Avg Validation Loss: 5.277817726135254\n",
      "Epoch 22/55 - Avg Training Loss: 6.814475377400716 - Avg Validation Loss: 5.7075550079345705\n",
      "Epoch 23/55 - Avg Training Loss: 7.167629771762424 - Avg Validation Loss: 4.418211269378662\n",
      "Epoch 24/55 - Avg Training Loss: 6.8930421405368385 - Avg Validation Loss: 5.617320537567139\n",
      "Epoch 25/55 - Avg Training Loss: 6.820858531528049 - Avg Validation Loss: 5.569662570953369\n",
      "Epoch 26/55 - Avg Training Loss: 7.593669785393609 - Avg Validation Loss: 3.67519097328186\n",
      "Epoch 27/55 - Avg Training Loss: 7.2082097795274525 - Avg Validation Loss: 8.557977867126464\n",
      "Epoch 28/55 - Avg Training Loss: 6.783972846137153 - Avg Validation Loss: 4.3237687110900875\n",
      "Epoch 29/55 - Avg Training Loss: 6.840931097666423 - Avg Validation Loss: 4.518295383453369\n",
      "Epoch 30/55 - Avg Training Loss: 6.5352187686496315 - Avg Validation Loss: 7.9862312316894535\n",
      "Epoch 31/55 - Avg Training Loss: 6.626847638024224 - Avg Validation Loss: 4.869883441925049\n",
      "Epoch 32/55 - Avg Training Loss: 6.680740674336751 - Avg Validation Loss: 4.078019475936889\n",
      "Epoch 33/55 - Avg Training Loss: 8.614173306359184 - Avg Validation Loss: 5.944356536865234\n",
      "Epoch 34/55 - Avg Training Loss: 7.994479868147108 - Avg Validation Loss: 7.38527774810791\n",
      "Epoch 35/55 - Avg Training Loss: 7.0605777104695635 - Avg Validation Loss: 4.218168306350708\n",
      "Epoch 36/55 - Avg Training Loss: 6.581974029541016 - Avg Validation Loss: 4.223793458938599\n",
      "Epoch 37/55 - Avg Training Loss: 6.89680094189114 - Avg Validation Loss: 6.765227603912353\n",
      "Epoch 38/55 - Avg Training Loss: 6.749559243520101 - Avg Validation Loss: 8.450776481628418\n",
      "Epoch 39/55 - Avg Training Loss: 7.26371791627672 - Avg Validation Loss: 5.472960472106934\n",
      "Epoch 40/55 - Avg Training Loss: 6.560386445787218 - Avg Validation Loss: 5.728981781005859\n",
      "Epoch 41/55 - Avg Training Loss: 7.043451362186008 - Avg Validation Loss: 7.895769214630127\n",
      "Epoch 42/55 - Avg Training Loss: 6.590477042728001 - Avg Validation Loss: 5.635943508148193\n",
      "Epoch 43/55 - Avg Training Loss: 6.059338516659206 - Avg Validation Loss: 4.4558248043060305\n",
      "Epoch 44/55 - Avg Training Loss: 7.569516499837239 - Avg Validation Loss: 7.5212455749511715\n",
      "Epoch 45/55 - Avg Training Loss: 6.988863997989231 - Avg Validation Loss: 3.7072913646698\n",
      "Epoch 46/55 - Avg Training Loss: 6.762672530280219 - Avg Validation Loss: 5.4970067024230955\n",
      "Epoch 47/55 - Avg Training Loss: 5.883226977454291 - Avg Validation Loss: 4.607862091064453\n",
      "Epoch 48/55 - Avg Training Loss: 6.522900899251302 - Avg Validation Loss: 12.96912441253662\n",
      "Epoch 49/55 - Avg Training Loss: 6.5509993765089245 - Avg Validation Loss: 3.9453583717346192\n",
      "Epoch 50/55 - Avg Training Loss: 5.994237687852648 - Avg Validation Loss: 4.569863414764404\n",
      "Epoch 51/55 - Avg Training Loss: 6.629172960917155 - Avg Validation Loss: 11.411260223388672\n",
      "Epoch 52/55 - Avg Training Loss: 7.08837350209554 - Avg Validation Loss: 3.772673416137695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:04,875] Trial 9 finished with value: 4.8824310302734375 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 2, 'dropout_rate': 0.42843229962760865, 'lr': 0.007962088095658789, 'batch_size': 176, 'epochs': 55}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/55 - Avg Training Loss: 7.816556453704834 - Avg Validation Loss: 11.203413200378417\n",
      "Epoch 54/55 - Avg Training Loss: 7.5801699426439075 - Avg Validation Loss: 6.350431346893311\n",
      "Epoch 55/55 - Avg Training Loss: 8.255227671729195 - Avg Validation Loss: 4.8824310302734375\n",
      "Finished Training\n",
      "Epoch 1/112 - Avg Training Loss: 145.83156623840333 - Avg Validation Loss: 40.372077306111656\n",
      "Epoch 2/112 - Avg Training Loss: 35.14931144714355 - Avg Validation Loss: 14.152237256368002\n",
      "Epoch 3/112 - Avg Training Loss: 20.662170314788817 - Avg Validation Loss: 10.553680737813314\n",
      "Epoch 4/112 - Avg Training Loss: 17.381615352630615 - Avg Validation Loss: 12.235769430796305\n",
      "Epoch 5/112 - Avg Training Loss: 14.65144681930542 - Avg Validation Loss: 12.993427594502768\n",
      "Epoch 6/112 - Avg Training Loss: 12.720989513397218 - Avg Validation Loss: 11.832245667775473\n",
      "Epoch 7/112 - Avg Training Loss: 11.268136310577393 - Avg Validation Loss: 9.112113237380981\n",
      "Epoch 8/112 - Avg Training Loss: 10.97264232635498 - Avg Validation Loss: 7.009753386179606\n",
      "Epoch 9/112 - Avg Training Loss: 10.022217559814454 - Avg Validation Loss: 7.645552237828572\n",
      "Epoch 10/112 - Avg Training Loss: 9.437222671508788 - Avg Validation Loss: 8.206791639328003\n",
      "Epoch 11/112 - Avg Training Loss: 9.069867181777955 - Avg Validation Loss: 6.814226229985555\n",
      "Epoch 12/112 - Avg Training Loss: 8.789876651763915 - Avg Validation Loss: 5.421694199244182\n",
      "Epoch 13/112 - Avg Training Loss: 8.609594774246215 - Avg Validation Loss: 5.656852801640828\n",
      "Epoch 14/112 - Avg Training Loss: 8.133611011505128 - Avg Validation Loss: 5.837748606999715\n",
      "Epoch 15/112 - Avg Training Loss: 8.211746120452881 - Avg Validation Loss: 5.17079496383667\n",
      "Epoch 16/112 - Avg Training Loss: 8.095532321929932 - Avg Validation Loss: 4.541020234425862\n",
      "Epoch 17/112 - Avg Training Loss: 8.092653369903564 - Avg Validation Loss: 7.923250834147136\n",
      "Epoch 18/112 - Avg Training Loss: 8.18299217224121 - Avg Validation Loss: 4.87907862663269\n",
      "Epoch 19/112 - Avg Training Loss: 8.298680067062378 - Avg Validation Loss: 9.244488398234049\n",
      "Epoch 20/112 - Avg Training Loss: 8.275786113739013 - Avg Validation Loss: 5.774736245473226\n",
      "Epoch 21/112 - Avg Training Loss: 8.021280574798585 - Avg Validation Loss: 6.305631240208943\n",
      "Epoch 22/112 - Avg Training Loss: 7.416681909561158 - Avg Validation Loss: 6.777565956115723\n",
      "Epoch 23/112 - Avg Training Loss: 7.736206531524658 - Avg Validation Loss: 4.72457488377889\n",
      "Epoch 24/112 - Avg Training Loss: 7.672506618499756 - Avg Validation Loss: 9.634666283925375\n",
      "Epoch 25/112 - Avg Training Loss: 7.812480926513672 - Avg Validation Loss: 4.295037269592285\n",
      "Epoch 26/112 - Avg Training Loss: 7.357466697692871 - Avg Validation Loss: 7.633150339126587\n",
      "Epoch 27/112 - Avg Training Loss: 7.218973445892334 - Avg Validation Loss: 6.935588200887044\n",
      "Epoch 28/112 - Avg Training Loss: 6.9039576053619385 - Avg Validation Loss: 6.183568398157756\n",
      "Epoch 29/112 - Avg Training Loss: 7.370724105834961 - Avg Validation Loss: 7.817453066507976\n",
      "Epoch 30/112 - Avg Training Loss: 7.18945574760437 - Avg Validation Loss: 8.863792101542154\n",
      "Epoch 31/112 - Avg Training Loss: 6.85822901725769 - Avg Validation Loss: 5.3275705973307295\n",
      "Epoch 32/112 - Avg Training Loss: 6.729704761505127 - Avg Validation Loss: 6.706134001413981\n",
      "Epoch 33/112 - Avg Training Loss: 6.91203122138977 - Avg Validation Loss: 9.49422518412272\n",
      "Epoch 34/112 - Avg Training Loss: 7.4519041061401365 - Avg Validation Loss: 5.972448269526164\n",
      "Epoch 35/112 - Avg Training Loss: 6.853572177886963 - Avg Validation Loss: 5.305997212727864\n",
      "Epoch 36/112 - Avg Training Loss: 7.148266744613648 - Avg Validation Loss: 8.325430790583292\n",
      "Epoch 37/112 - Avg Training Loss: 7.221103477478027 - Avg Validation Loss: 7.904074589411418\n",
      "Epoch 38/112 - Avg Training Loss: 6.46080207824707 - Avg Validation Loss: 5.952862819035848\n",
      "Epoch 39/112 - Avg Training Loss: 6.952730560302735 - Avg Validation Loss: 6.028205156326294\n",
      "Epoch 40/112 - Avg Training Loss: 6.822638034820557 - Avg Validation Loss: 6.36726959546407\n",
      "Epoch 41/112 - Avg Training Loss: 6.485128355026245 - Avg Validation Loss: 6.367054303487142\n",
      "Epoch 42/112 - Avg Training Loss: 6.661618375778199 - Avg Validation Loss: 9.282200336456299\n",
      "Epoch 43/112 - Avg Training Loss: 7.325222158432007 - Avg Validation Loss: 8.08754293123881\n",
      "Epoch 44/112 - Avg Training Loss: 6.658364152908325 - Avg Validation Loss: 8.447985887527466\n",
      "Epoch 45/112 - Avg Training Loss: 6.45744137763977 - Avg Validation Loss: 5.590184688568115\n",
      "Epoch 46/112 - Avg Training Loss: 6.398394012451172 - Avg Validation Loss: 5.644987106323242\n",
      "Epoch 47/112 - Avg Training Loss: 6.467913675308227 - Avg Validation Loss: 4.972925345102946\n",
      "Epoch 48/112 - Avg Training Loss: 6.405722570419312 - Avg Validation Loss: 5.977864106496175\n",
      "Epoch 49/112 - Avg Training Loss: 6.284173059463501 - Avg Validation Loss: 6.673730452855428\n",
      "Epoch 50/112 - Avg Training Loss: 6.369931077957153 - Avg Validation Loss: 6.843684673309326\n",
      "Epoch 51/112 - Avg Training Loss: 6.219503927230835 - Avg Validation Loss: 5.421197493871053\n",
      "Epoch 52/112 - Avg Training Loss: 6.531645679473877 - Avg Validation Loss: 4.2968103885650635\n",
      "Epoch 53/112 - Avg Training Loss: 6.482308006286621 - Avg Validation Loss: 6.828504165013631\n",
      "Epoch 54/112 - Avg Training Loss: 6.871452760696411 - Avg Validation Loss: 7.521552960077922\n",
      "Epoch 55/112 - Avg Training Loss: 6.344148588180542 - Avg Validation Loss: 5.84065858523051\n",
      "Epoch 56/112 - Avg Training Loss: 5.928018856048584 - Avg Validation Loss: 5.095726569493611\n",
      "Epoch 57/112 - Avg Training Loss: 6.171437835693359 - Avg Validation Loss: 6.854312340418498\n",
      "Epoch 58/112 - Avg Training Loss: 6.1387739181518555 - Avg Validation Loss: 5.763537327448527\n",
      "Epoch 59/112 - Avg Training Loss: 6.347835445404053 - Avg Validation Loss: 6.3289328416188555\n",
      "Epoch 60/112 - Avg Training Loss: 6.4325682640075685 - Avg Validation Loss: 5.715673526128133\n",
      "Epoch 61/112 - Avg Training Loss: 6.344886827468872 - Avg Validation Loss: 4.336728731791179\n",
      "Epoch 62/112 - Avg Training Loss: 6.491671991348267 - Avg Validation Loss: 4.119515975316365\n",
      "Epoch 63/112 - Avg Training Loss: 6.371403312683105 - Avg Validation Loss: 6.366411209106445\n",
      "Epoch 64/112 - Avg Training Loss: 6.474393558502197 - Avg Validation Loss: 6.614888032277425\n",
      "Epoch 65/112 - Avg Training Loss: 7.007161855697632 - Avg Validation Loss: 11.271405220031738\n",
      "Epoch 66/112 - Avg Training Loss: 6.614675045013428 - Avg Validation Loss: 7.5493020216623945\n",
      "Epoch 67/112 - Avg Training Loss: 6.032758665084839 - Avg Validation Loss: 6.0940314928690595\n",
      "Epoch 68/112 - Avg Training Loss: 6.492883396148682 - Avg Validation Loss: 4.960295677185059\n",
      "Epoch 69/112 - Avg Training Loss: 6.556035804748535 - Avg Validation Loss: 6.769371112187703\n",
      "Epoch 70/112 - Avg Training Loss: 6.283649158477783 - Avg Validation Loss: 5.427068551381429\n",
      "Epoch 71/112 - Avg Training Loss: 6.234023904800415 - Avg Validation Loss: 7.354186455408732\n",
      "Epoch 72/112 - Avg Training Loss: 6.120625400543213 - Avg Validation Loss: 5.357718547185262\n",
      "Epoch 73/112 - Avg Training Loss: 6.0228334903717045 - Avg Validation Loss: 5.792104959487915\n",
      "Epoch 74/112 - Avg Training Loss: 6.318069267272949 - Avg Validation Loss: 7.633625745773315\n",
      "Epoch 75/112 - Avg Training Loss: 6.195535898208618 - Avg Validation Loss: 7.9525667031606035\n",
      "Epoch 76/112 - Avg Training Loss: 6.076817464828491 - Avg Validation Loss: 5.686211744944255\n",
      "Epoch 77/112 - Avg Training Loss: 6.135463762283325 - Avg Validation Loss: 5.0956971645355225\n",
      "Epoch 78/112 - Avg Training Loss: 6.727988052368164 - Avg Validation Loss: 4.870908339818318\n",
      "Epoch 79/112 - Avg Training Loss: 6.628253936767578 - Avg Validation Loss: 6.560081481933594\n",
      "Epoch 80/112 - Avg Training Loss: 6.041536617279053 - Avg Validation Loss: 6.495009660720825\n",
      "Epoch 81/112 - Avg Training Loss: 6.228722429275512 - Avg Validation Loss: 5.176364819208781\n",
      "Epoch 82/112 - Avg Training Loss: 6.312346029281616 - Avg Validation Loss: 6.466725905736287\n",
      "Epoch 83/112 - Avg Training Loss: 6.0338726997375485 - Avg Validation Loss: 6.724677801132202\n",
      "Epoch 84/112 - Avg Training Loss: 5.862618827819825 - Avg Validation Loss: 7.603090604146321\n",
      "Epoch 85/112 - Avg Training Loss: 6.0977057933807375 - Avg Validation Loss: 6.379202922185262\n",
      "Epoch 86/112 - Avg Training Loss: 5.935686731338501 - Avg Validation Loss: 5.524281899134318\n",
      "Epoch 87/112 - Avg Training Loss: 5.55555534362793 - Avg Validation Loss: 4.784653107325236\n",
      "Epoch 88/112 - Avg Training Loss: 6.126219129562378 - Avg Validation Loss: 8.831528504689535\n",
      "Epoch 89/112 - Avg Training Loss: 6.123550701141357 - Avg Validation Loss: 5.415051539738973\n",
      "Epoch 90/112 - Avg Training Loss: 5.859136915206909 - Avg Validation Loss: 6.9559604326883955\n",
      "Epoch 91/112 - Avg Training Loss: 5.937387132644654 - Avg Validation Loss: 6.007321198781331\n",
      "Epoch 92/112 - Avg Training Loss: 6.101367473602295 - Avg Validation Loss: 7.898786147435506\n",
      "Epoch 93/112 - Avg Training Loss: 5.919009637832642 - Avg Validation Loss: 6.399683475494385\n",
      "Epoch 94/112 - Avg Training Loss: 6.073541927337646 - Avg Validation Loss: 8.645254770914713\n",
      "Epoch 95/112 - Avg Training Loss: 5.9774330139160154 - Avg Validation Loss: 8.076574325561523\n",
      "Epoch 96/112 - Avg Training Loss: 6.1880358219146725 - Avg Validation Loss: 8.181076526641846\n",
      "Epoch 97/112 - Avg Training Loss: 5.690811443328857 - Avg Validation Loss: 6.380092700322469\n",
      "Epoch 98/112 - Avg Training Loss: 5.369154167175293 - Avg Validation Loss: 5.7196541627248125\n",
      "Epoch 99/112 - Avg Training Loss: 5.71489782333374 - Avg Validation Loss: 8.45553151766459\n",
      "Epoch 100/112 - Avg Training Loss: 6.043285131454468 - Avg Validation Loss: 10.37594747543335\n",
      "Epoch 101/112 - Avg Training Loss: 6.0619140625 - Avg Validation Loss: 7.704523642857869\n",
      "Epoch 102/112 - Avg Training Loss: 5.6243932247161865 - Avg Validation Loss: 6.486664454142253\n",
      "Epoch 103/112 - Avg Training Loss: 5.86420750617981 - Avg Validation Loss: 4.866972049077352\n",
      "Epoch 104/112 - Avg Training Loss: 6.339047288894653 - Avg Validation Loss: 5.081690311431885\n",
      "Epoch 105/112 - Avg Training Loss: 5.771687412261963 - Avg Validation Loss: 4.281426906585693\n",
      "Epoch 106/112 - Avg Training Loss: 6.353024816513061 - Avg Validation Loss: 4.320694327354431\n",
      "Epoch 107/112 - Avg Training Loss: 5.660755968093872 - Avg Validation Loss: 5.489160696665446\n",
      "Epoch 108/112 - Avg Training Loss: 5.545847463607788 - Avg Validation Loss: 5.5397679805755615\n",
      "Epoch 109/112 - Avg Training Loss: 5.555651235580444 - Avg Validation Loss: 5.776346762975057\n",
      "Epoch 110/112 - Avg Training Loss: 6.232566547393799 - Avg Validation Loss: 5.853086153666179\n",
      "Epoch 111/112 - Avg Training Loss: 5.710657167434692 - Avg Validation Loss: 5.387925465901692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:14,171] Trial 10 finished with value: 6.24071462949117 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5138080536550589, 'lr': 0.0011255453493269653, 'batch_size': 144, 'epochs': 112}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/112 - Avg Training Loss: 5.737154245376587 - Avg Validation Loss: 6.24071462949117\n",
      "Finished Training\n",
      "Epoch 1/56 - Avg Training Loss: 276.61918411254885 - Avg Validation Loss: 300.4692128499349\n",
      "Epoch 2/56 - Avg Training Loss: 211.80433731079103 - Avg Validation Loss: 28.632359822591145\n",
      "Epoch 3/56 - Avg Training Loss: 70.71515846252441 - Avg Validation Loss: 47.22174326578776\n",
      "Epoch 4/56 - Avg Training Loss: 42.21009521484375 - Avg Validation Loss: 11.66327953338623\n",
      "Epoch 5/56 - Avg Training Loss: 31.717673301696777 - Avg Validation Loss: 31.752679189046223\n",
      "Epoch 6/56 - Avg Training Loss: 22.930506706237793 - Avg Validation Loss: 40.52874183654785\n",
      "Epoch 7/56 - Avg Training Loss: 20.505624198913573 - Avg Validation Loss: 50.50021425882975\n",
      "Epoch 8/56 - Avg Training Loss: 19.72081241607666 - Avg Validation Loss: 59.11246109008789\n",
      "Epoch 9/56 - Avg Training Loss: 16.48067626953125 - Avg Validation Loss: 53.244850158691406\n",
      "Epoch 10/56 - Avg Training Loss: 16.577159881591797 - Avg Validation Loss: 53.52565002441406\n",
      "Epoch 11/56 - Avg Training Loss: 14.982396030426026 - Avg Validation Loss: 47.75200398763021\n",
      "Epoch 12/56 - Avg Training Loss: 14.567272186279297 - Avg Validation Loss: 55.22723197937012\n",
      "Epoch 13/56 - Avg Training Loss: 13.39466314315796 - Avg Validation Loss: 45.070196787516274\n",
      "Epoch 14/56 - Avg Training Loss: 13.638543605804443 - Avg Validation Loss: 58.53950945536295\n",
      "Epoch 15/56 - Avg Training Loss: 12.607388687133788 - Avg Validation Loss: 44.8277219136556\n",
      "Epoch 16/56 - Avg Training Loss: 13.45977840423584 - Avg Validation Loss: 53.52800687154134\n",
      "Epoch 17/56 - Avg Training Loss: 13.298813247680664 - Avg Validation Loss: 48.708169301350914\n",
      "Epoch 18/56 - Avg Training Loss: 13.417484378814697 - Avg Validation Loss: 39.90069452921549\n",
      "Epoch 19/56 - Avg Training Loss: 12.374231243133545 - Avg Validation Loss: 58.78192901611328\n",
      "Epoch 20/56 - Avg Training Loss: 11.941113662719726 - Avg Validation Loss: 52.136149088541664\n",
      "Epoch 21/56 - Avg Training Loss: 11.297818279266357 - Avg Validation Loss: 43.43163045247396\n",
      "Epoch 22/56 - Avg Training Loss: 11.024247550964356 - Avg Validation Loss: 51.617865880330406\n",
      "Epoch 23/56 - Avg Training Loss: 10.33390474319458 - Avg Validation Loss: 46.29372914632162\n",
      "Epoch 24/56 - Avg Training Loss: 10.726660060882569 - Avg Validation Loss: 49.704718907674156\n",
      "Epoch 25/56 - Avg Training Loss: 10.264228916168213 - Avg Validation Loss: 49.903218587239586\n",
      "Epoch 26/56 - Avg Training Loss: 10.307355213165284 - Avg Validation Loss: 33.11234982808431\n",
      "Epoch 27/56 - Avg Training Loss: 10.416783428192138 - Avg Validation Loss: 41.0785338083903\n",
      "Epoch 28/56 - Avg Training Loss: 9.827857398986817 - Avg Validation Loss: 34.54079246520996\n",
      "Epoch 29/56 - Avg Training Loss: 9.81872067451477 - Avg Validation Loss: 34.69607734680176\n",
      "Epoch 30/56 - Avg Training Loss: 9.90835018157959 - Avg Validation Loss: 39.938577016194664\n",
      "Epoch 31/56 - Avg Training Loss: 9.662427997589111 - Avg Validation Loss: 33.26864814758301\n",
      "Epoch 32/56 - Avg Training Loss: 9.585001897811889 - Avg Validation Loss: 35.294243494669594\n",
      "Epoch 33/56 - Avg Training Loss: 8.99336895942688 - Avg Validation Loss: 33.50870831807455\n",
      "Epoch 34/56 - Avg Training Loss: 9.298942947387696 - Avg Validation Loss: 41.47619756062826\n",
      "Epoch 35/56 - Avg Training Loss: 9.347346973419189 - Avg Validation Loss: 30.591551780700684\n",
      "Epoch 36/56 - Avg Training Loss: 8.84031376838684 - Avg Validation Loss: 26.13023630777995\n",
      "Epoch 37/56 - Avg Training Loss: 9.338347625732421 - Avg Validation Loss: 36.611528396606445\n",
      "Epoch 38/56 - Avg Training Loss: 9.223312425613404 - Avg Validation Loss: 33.62615140279134\n",
      "Epoch 39/56 - Avg Training Loss: 8.335812854766846 - Avg Validation Loss: 33.9147523244222\n",
      "Epoch 40/56 - Avg Training Loss: 8.889506816864014 - Avg Validation Loss: 36.98329226175944\n",
      "Epoch 41/56 - Avg Training Loss: 9.426864433288575 - Avg Validation Loss: 29.826132774353027\n",
      "Epoch 42/56 - Avg Training Loss: 8.3240731716156 - Avg Validation Loss: 25.388218879699707\n",
      "Epoch 43/56 - Avg Training Loss: 8.743284130096436 - Avg Validation Loss: 32.014716148376465\n",
      "Epoch 44/56 - Avg Training Loss: 8.391312551498412 - Avg Validation Loss: 24.91987419128418\n",
      "Epoch 45/56 - Avg Training Loss: 8.647092008590699 - Avg Validation Loss: 26.648385365804035\n",
      "Epoch 46/56 - Avg Training Loss: 8.7150372505188 - Avg Validation Loss: 30.53831672668457\n",
      "Epoch 47/56 - Avg Training Loss: 8.598137807846069 - Avg Validation Loss: 24.49531141916911\n",
      "Epoch 48/56 - Avg Training Loss: 8.577975940704345 - Avg Validation Loss: 23.40556748708089\n",
      "Epoch 49/56 - Avg Training Loss: 8.875654077529907 - Avg Validation Loss: 31.60558255513509\n",
      "Epoch 50/56 - Avg Training Loss: 8.970305967330933 - Avg Validation Loss: 27.81134255727132\n",
      "Epoch 51/56 - Avg Training Loss: 8.997311305999755 - Avg Validation Loss: 22.07884629567464\n",
      "Epoch 52/56 - Avg Training Loss: 8.32034773826599 - Avg Validation Loss: 18.730295181274414\n",
      "Epoch 53/56 - Avg Training Loss: 9.02192850112915 - Avg Validation Loss: 21.356321970621746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:18,749] Trial 11 finished with value: 20.385468165079754 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.47188144918770425, 'lr': 0.007895769937599391, 'batch_size': 144, 'epochs': 56}. Best is trial 4 with value: 4.8633527755737305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/56 - Avg Training Loss: 8.555986261367797 - Avg Validation Loss: 23.59488296508789\n",
      "Epoch 55/56 - Avg Training Loss: 8.346051168441772 - Avg Validation Loss: 22.906869888305664\n",
      "Epoch 56/56 - Avg Training Loss: 8.761734247207642 - Avg Validation Loss: 20.385468165079754\n",
      "Finished Training\n",
      "Epoch 1/51 - Avg Training Loss: 87.6978702545166 - Avg Validation Loss: 35.15388897487095\n",
      "Epoch 2/51 - Avg Training Loss: 27.190483887990315 - Avg Validation Loss: 9.077953474862236\n",
      "Epoch 3/51 - Avg Training Loss: 17.832704067230225 - Avg Validation Loss: 17.686344419206893\n",
      "Epoch 4/51 - Avg Training Loss: 14.252779324849447 - Avg Validation Loss: 6.256975105830601\n",
      "Epoch 5/51 - Avg Training Loss: 12.573716560999552 - Avg Validation Loss: 5.99856914792742\n",
      "Epoch 6/51 - Avg Training Loss: 12.151181936264038 - Avg Validation Loss: 17.02444553375244\n",
      "Epoch 7/51 - Avg Training Loss: 13.46459674835205 - Avg Validation Loss: 11.029786791120257\n",
      "Epoch 8/51 - Avg Training Loss: 11.081133604049683 - Avg Validation Loss: 8.475851740155901\n",
      "Epoch 9/51 - Avg Training Loss: 10.98854653040568 - Avg Validation Loss: 4.695744378226144\n",
      "Epoch 10/51 - Avg Training Loss: 9.957750876744589 - Avg Validation Loss: 7.063858781542097\n",
      "Epoch 11/51 - Avg Training Loss: 10.17857321103414 - Avg Validation Loss: 5.973337037222726\n",
      "Epoch 12/51 - Avg Training Loss: 9.96241009235382 - Avg Validation Loss: 8.82701928274972\n",
      "Epoch 13/51 - Avg Training Loss: 10.083197593688965 - Avg Validation Loss: 4.615358420780727\n",
      "Epoch 14/51 - Avg Training Loss: 11.045633872350058 - Avg Validation Loss: 6.62000846862793\n",
      "Epoch 15/51 - Avg Training Loss: 9.844782749811808 - Avg Validation Loss: 7.8557891845703125\n",
      "Epoch 16/51 - Avg Training Loss: 10.325055201848349 - Avg Validation Loss: 8.720579419817243\n",
      "Epoch 17/51 - Avg Training Loss: 11.638276974360148 - Avg Validation Loss: 4.527845689228603\n",
      "Epoch 18/51 - Avg Training Loss: 11.822643200556437 - Avg Validation Loss: 11.696736471993583\n",
      "Epoch 19/51 - Avg Training Loss: 10.765373627344767 - Avg Validation Loss: 5.06629923411778\n",
      "Epoch 20/51 - Avg Training Loss: 10.846200784047445 - Avg Validation Loss: 6.220349039350237\n",
      "Epoch 21/51 - Avg Training Loss: 10.554756244023642 - Avg Validation Loss: 6.629268782479422\n",
      "Epoch 22/51 - Avg Training Loss: 10.392551263173422 - Avg Validation Loss: 12.22131838117327\n",
      "Epoch 23/51 - Avg Training Loss: 10.711001952489218 - Avg Validation Loss: 3.736654417855399\n",
      "Epoch 24/51 - Avg Training Loss: 10.925933996836344 - Avg Validation Loss: 5.635710716247559\n",
      "Epoch 25/51 - Avg Training Loss: 10.83546257019043 - Avg Validation Loss: 7.815539700644357\n",
      "Epoch 26/51 - Avg Training Loss: 10.743180910746256 - Avg Validation Loss: 8.106216975620814\n",
      "Epoch 27/51 - Avg Training Loss: 10.482059399286905 - Avg Validation Loss: 16.106952258518763\n",
      "Epoch 28/51 - Avg Training Loss: 13.18841846783956 - Avg Validation Loss: 9.569103649684362\n",
      "Epoch 29/51 - Avg Training Loss: 13.15474279721578 - Avg Validation Loss: 5.25491189956665\n",
      "Epoch 30/51 - Avg Training Loss: 10.800511995951334 - Avg Validation Loss: 14.097044944763184\n",
      "Epoch 31/51 - Avg Training Loss: 11.086859623591105 - Avg Validation Loss: 10.359877586364746\n",
      "Epoch 32/51 - Avg Training Loss: 12.729308207829794 - Avg Validation Loss: 5.611461162567139\n",
      "Epoch 33/51 - Avg Training Loss: 12.00813603401184 - Avg Validation Loss: 14.297294889177595\n",
      "Epoch 34/51 - Avg Training Loss: 12.38977344830831 - Avg Validation Loss: 5.213563510349819\n",
      "Epoch 35/51 - Avg Training Loss: 12.475898504257202 - Avg Validation Loss: 18.926808493477957\n",
      "Epoch 36/51 - Avg Training Loss: 11.126505295435587 - Avg Validation Loss: 3.896676642554147\n",
      "Epoch 37/51 - Avg Training Loss: 9.837863286336264 - Avg Validation Loss: 8.732767581939697\n",
      "Epoch 38/51 - Avg Training Loss: 9.564391215642294 - Avg Validation Loss: 4.803943157196045\n",
      "Epoch 39/51 - Avg Training Loss: 12.259268363316854 - Avg Validation Loss: 8.039794036320277\n",
      "Epoch 40/51 - Avg Training Loss: 10.885548035303751 - Avg Validation Loss: 8.677025590624128\n",
      "Epoch 41/51 - Avg Training Loss: 9.29796278476715 - Avg Validation Loss: 4.0139643124171664\n",
      "Epoch 42/51 - Avg Training Loss: 10.151729504267374 - Avg Validation Loss: 14.435513496398926\n",
      "Epoch 43/51 - Avg Training Loss: 12.125285267829895 - Avg Validation Loss: 4.129447323935373\n",
      "Epoch 44/51 - Avg Training Loss: 9.963471412658691 - Avg Validation Loss: 3.995445285524641\n",
      "Epoch 45/51 - Avg Training Loss: 10.749235471089682 - Avg Validation Loss: 8.832660947527204\n",
      "Epoch 46/51 - Avg Training Loss: 9.758233070373535 - Avg Validation Loss: 4.769548211778913\n",
      "Epoch 47/51 - Avg Training Loss: 9.772044062614441 - Avg Validation Loss: 7.119134971073696\n",
      "Epoch 48/51 - Avg Training Loss: 10.685643275578817 - Avg Validation Loss: 8.343449592590332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:23,279] Trial 12 finished with value: 4.298051016671317 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5196503028466288, 'lr': 0.0037833456837237456, 'batch_size': 128, 'epochs': 51}. Best is trial 12 with value: 4.298051016671317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/51 - Avg Training Loss: 9.72680958112081 - Avg Validation Loss: 4.011771644864764\n",
      "Epoch 50/51 - Avg Training Loss: 10.141003966331482 - Avg Validation Loss: 6.533040455409458\n",
      "Epoch 51/51 - Avg Training Loss: 8.881807764371237 - Avg Validation Loss: 4.298051016671317\n",
      "Finished Training\n",
      "Epoch 1/68 - Avg Training Loss: 87.79336403740777 - Avg Validation Loss: 23.977721885398584\n",
      "Epoch 2/68 - Avg Training Loss: 20.682276492648654 - Avg Validation Loss: 12.723709883513274\n",
      "Epoch 3/68 - Avg Training Loss: 18.236980374654134 - Avg Validation Loss: 6.083791856412534\n",
      "Epoch 4/68 - Avg Training Loss: 16.009318584865994 - Avg Validation Loss: 9.711867668010571\n",
      "Epoch 5/68 - Avg Training Loss: 15.740804757012262 - Avg Validation Loss: 6.192350846749765\n",
      "Epoch 6/68 - Avg Training Loss: 16.14997215270996 - Avg Validation Loss: 8.877843185707375\n",
      "Epoch 7/68 - Avg Training Loss: 16.392384571499296 - Avg Validation Loss: 14.63758588720251\n",
      "Epoch 8/68 - Avg Training Loss: 15.719260915120442 - Avg Validation Loss: 6.945853780817102\n",
      "Epoch 9/68 - Avg Training Loss: 15.176531918843587 - Avg Validation Loss: 14.199329340899432\n",
      "Epoch 10/68 - Avg Training Loss: 16.03047801123725 - Avg Validation Loss: 6.646133899688721\n",
      "Epoch 11/68 - Avg Training Loss: 15.912651507059733 - Avg Validation Loss: 5.400011663083677\n",
      "Epoch 12/68 - Avg Training Loss: 16.250086530049643 - Avg Validation Loss: 25.541859803376376\n",
      "Epoch 13/68 - Avg Training Loss: 16.387277687920463 - Avg Validation Loss: 7.962000864523429\n",
      "Epoch 14/68 - Avg Training Loss: 15.782386059231229 - Avg Validation Loss: 17.131169001261394\n",
      "Epoch 15/68 - Avg Training Loss: 18.35553552839491 - Avg Validation Loss: 7.946236239539252\n",
      "Epoch 16/68 - Avg Training Loss: 16.54335329267714 - Avg Validation Loss: 6.664448967686406\n",
      "Epoch 17/68 - Avg Training Loss: 16.379472520616318 - Avg Validation Loss: 4.766737831963433\n",
      "Epoch 18/68 - Avg Training Loss: 16.078750356038412 - Avg Validation Loss: 8.517401024147317\n",
      "Epoch 19/68 - Avg Training Loss: 14.845631980895996 - Avg Validation Loss: 6.270637759455928\n",
      "Epoch 20/68 - Avg Training Loss: 17.529075707329643 - Avg Validation Loss: 8.863555148795799\n",
      "Epoch 21/68 - Avg Training Loss: 15.464869774712456 - Avg Validation Loss: 4.954982766398677\n",
      "Epoch 22/68 - Avg Training Loss: 20.893345981174043 - Avg Validation Loss: 6.46316420590436\n",
      "Epoch 23/68 - Avg Training Loss: 15.044758330451117 - Avg Validation Loss: 10.222017464814362\n",
      "Epoch 24/68 - Avg Training Loss: 14.567529932657878 - Avg Validation Loss: 8.315218801851627\n",
      "Epoch 25/68 - Avg Training Loss: 15.005823029412163 - Avg Validation Loss: 9.68944736763283\n",
      "Epoch 26/68 - Avg Training Loss: 13.127758365207248 - Avg Validation Loss: 8.931656802142108\n",
      "Epoch 27/68 - Avg Training Loss: 12.054085403018528 - Avg Validation Loss: 8.440243844632748\n",
      "Epoch 28/68 - Avg Training Loss: 11.796185313330756 - Avg Validation Loss: 11.989242765638563\n",
      "Epoch 29/68 - Avg Training Loss: 11.026680172814263 - Avg Validation Loss: 5.589527659946018\n",
      "Epoch 30/68 - Avg Training Loss: 11.58923077053494 - Avg Validation Loss: 10.820293426513672\n",
      "Epoch 31/68 - Avg Training Loss: 10.080788389841716 - Avg Validation Loss: 7.1780759316903575\n",
      "Epoch 32/68 - Avg Training Loss: 9.205761528015136 - Avg Validation Loss: 5.89858231721101\n",
      "Epoch 33/68 - Avg Training Loss: 9.034094640943739 - Avg Validation Loss: 5.6053759080392345\n",
      "Epoch 34/68 - Avg Training Loss: 8.759119669596354 - Avg Validation Loss: 5.254400041368273\n",
      "Epoch 35/68 - Avg Training Loss: 9.349703523847792 - Avg Validation Loss: 5.670157644483778\n",
      "Epoch 36/68 - Avg Training Loss: 8.598995780944824 - Avg Validation Loss: 4.889564637784605\n",
      "Epoch 37/68 - Avg Training Loss: 9.033626143137614 - Avg Validation Loss: 5.859939628177219\n",
      "Epoch 38/68 - Avg Training Loss: 8.20746038224962 - Avg Validation Loss: 6.79020447201199\n",
      "Epoch 39/68 - Avg Training Loss: 8.043123785654704 - Avg Validation Loss: 4.649737119674683\n",
      "Epoch 40/68 - Avg Training Loss: 8.6948992729187 - Avg Validation Loss: 5.5623390674591064\n",
      "Epoch 41/68 - Avg Training Loss: 8.2743624581231 - Avg Validation Loss: 5.4909004811887385\n",
      "Epoch 42/68 - Avg Training Loss: 8.194656795925564 - Avg Validation Loss: 4.515338182449341\n",
      "Epoch 43/68 - Avg Training Loss: 8.87767776913113 - Avg Validation Loss: 4.541142092810737\n",
      "Epoch 44/68 - Avg Training Loss: 8.13484590318468 - Avg Validation Loss: 4.771473390084726\n",
      "Epoch 45/68 - Avg Training Loss: 8.077030150095622 - Avg Validation Loss: 4.389663201791269\n",
      "Epoch 46/68 - Avg Training Loss: 7.647622553507487 - Avg Validation Loss: 6.3145509295993385\n",
      "Epoch 47/68 - Avg Training Loss: 7.118763054741754 - Avg Validation Loss: 5.962142767729582\n",
      "Epoch 48/68 - Avg Training Loss: 7.6234023412068685 - Avg Validation Loss: 5.227726998152556\n",
      "Epoch 49/68 - Avg Training Loss: 7.464012103610568 - Avg Validation Loss: 5.6129262094144465\n",
      "Epoch 50/68 - Avg Training Loss: 7.58371900982327 - Avg Validation Loss: 5.1337371049103915\n",
      "Epoch 51/68 - Avg Training Loss: 7.218819035424127 - Avg Validation Loss: 5.1635250338801635\n",
      "Epoch 52/68 - Avg Training Loss: 7.315066199832493 - Avg Validation Loss: 3.9587209401307284\n",
      "Epoch 53/68 - Avg Training Loss: 7.365190537770589 - Avg Validation Loss: 5.592706574334039\n",
      "Epoch 54/68 - Avg Training Loss: 7.147565301259359 - Avg Validation Loss: 4.649270278436166\n",
      "Epoch 55/68 - Avg Training Loss: 6.7921072430080836 - Avg Validation Loss: 5.863662022131461\n",
      "Epoch 56/68 - Avg Training Loss: 7.158240922292074 - Avg Validation Loss: 4.635681673332497\n",
      "Epoch 57/68 - Avg Training Loss: 7.195423354042901 - Avg Validation Loss: 4.425176161306876\n",
      "Epoch 58/68 - Avg Training Loss: 6.8082462734646265 - Avg Validation Loss: 4.938462566446375\n",
      "Epoch 59/68 - Avg Training Loss: 6.623762702941894 - Avg Validation Loss: 4.16752795819883\n",
      "Epoch 60/68 - Avg Training Loss: 6.574030388726128 - Avg Validation Loss: 5.094919072257148\n",
      "Epoch 61/68 - Avg Training Loss: 6.890109146965875 - Avg Validation Loss: 5.221221906167489\n",
      "Epoch 62/68 - Avg Training Loss: 6.549207867516412 - Avg Validation Loss: 4.390727608292191\n",
      "Epoch 63/68 - Avg Training Loss: 6.608125962151421 - Avg Validation Loss: 5.498620783841169\n",
      "Epoch 64/68 - Avg Training Loss: 6.730044502682156 - Avg Validation Loss: 4.174650050975658\n",
      "Epoch 65/68 - Avg Training Loss: 6.834168677859836 - Avg Validation Loss: 4.436026140495583\n",
      "Epoch 66/68 - Avg Training Loss: 6.545516226026747 - Avg Validation Loss: 4.533587994398894\n",
      "Epoch 67/68 - Avg Training Loss: 6.19307377603319 - Avg Validation Loss: 4.6608791174712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:35,125] Trial 13 finished with value: 4.09535269384031 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5311443567511174, 'lr': 0.003462823744917432, 'batch_size': 32, 'epochs': 68}. Best is trial 13 with value: 4.09535269384031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/68 - Avg Training Loss: 6.407653384738499 - Avg Validation Loss: 4.09535269384031\n",
      "Finished Training\n",
      "Epoch 1/65 - Avg Training Loss: 56.96649285422431 - Avg Validation Loss: 15.994736600805211\n",
      "Epoch 2/65 - Avg Training Loss: 25.496150779724122 - Avg Validation Loss: 9.387500515690556\n",
      "Epoch 3/65 - Avg Training Loss: 22.462570020887586 - Avg Validation Loss: 18.383190172689932\n",
      "Epoch 4/65 - Avg Training Loss: 23.08806702295939 - Avg Validation Loss: 16.525431474049885\n",
      "Epoch 5/65 - Avg Training Loss: 22.313602913750543 - Avg Validation Loss: 7.026939286126031\n",
      "Epoch 6/65 - Avg Training Loss: 21.66811754438612 - Avg Validation Loss: 12.646400698909053\n",
      "Epoch 7/65 - Avg Training Loss: 20.398601542578803 - Avg Validation Loss: 9.117553066324305\n",
      "Epoch 8/65 - Avg Training Loss: 21.14360565609402 - Avg Validation Loss: 21.22207507380733\n",
      "Epoch 9/65 - Avg Training Loss: 19.48660726547241 - Avg Validation Loss: 10.207303188465259\n",
      "Epoch 10/65 - Avg Training Loss: 19.16411230299208 - Avg Validation Loss: 9.592085105401498\n",
      "Epoch 11/65 - Avg Training Loss: 18.633203347524006 - Avg Validation Loss: 15.125723732842339\n",
      "Epoch 12/65 - Avg Training Loss: 18.718122429317898 - Avg Validation Loss: 14.285070083759448\n",
      "Epoch 13/65 - Avg Training Loss: 16.770923858218723 - Avg Validation Loss: 18.40568604292693\n",
      "Epoch 14/65 - Avg Training Loss: 16.86584226820204 - Avg Validation Loss: 21.35476126494231\n",
      "Epoch 15/65 - Avg Training Loss: 15.322072060902913 - Avg Validation Loss: 16.384654610245317\n",
      "Epoch 16/65 - Avg Training Loss: 13.853623490863376 - Avg Validation Loss: 14.590266545613607\n",
      "Epoch 17/65 - Avg Training Loss: 14.040310912662083 - Avg Validation Loss: 11.728557489536426\n",
      "Epoch 18/65 - Avg Training Loss: 12.207734325197007 - Avg Validation Loss: 9.994962462672481\n",
      "Epoch 19/65 - Avg Training Loss: 12.720864513185289 - Avg Validation Loss: 14.156088758397985\n",
      "Epoch 20/65 - Avg Training Loss: 12.07656037542555 - Avg Validation Loss: 6.480469606540821\n",
      "Epoch 21/65 - Avg Training Loss: 11.295413536495632 - Avg Validation Loss: 7.78184719438906\n",
      "Epoch 22/65 - Avg Training Loss: 11.160311317443847 - Avg Validation Loss: 7.35191375237924\n",
      "Epoch 23/65 - Avg Training Loss: 10.669103129704792 - Avg Validation Loss: 6.5440897941589355\n",
      "Epoch 24/65 - Avg Training Loss: 10.902143245273166 - Avg Validation Loss: 5.210303549413328\n",
      "Epoch 25/65 - Avg Training Loss: 10.717453336715698 - Avg Validation Loss: 7.958259503046672\n",
      "Epoch 26/65 - Avg Training Loss: 10.628757731119792 - Avg Validation Loss: 4.5011734697553845\n",
      "Epoch 27/65 - Avg Training Loss: 10.111505217022366 - Avg Validation Loss: 8.812588665220472\n",
      "Epoch 28/65 - Avg Training Loss: 9.534006418122186 - Avg Validation Loss: 6.792536638401173\n",
      "Epoch 29/65 - Avg Training Loss: 9.707723956637912 - Avg Validation Loss: 6.60635248819987\n",
      "Epoch 30/65 - Avg Training Loss: 9.52353228992886 - Avg Validation Loss: 5.51282392166279\n",
      "Epoch 31/65 - Avg Training Loss: 8.966323963801067 - Avg Validation Loss: 4.198242770300971\n",
      "Epoch 32/65 - Avg Training Loss: 8.79024084938897 - Avg Validation Loss: 5.223106565298857\n",
      "Epoch 33/65 - Avg Training Loss: 8.528819733195835 - Avg Validation Loss: 5.217341665868406\n",
      "Epoch 34/65 - Avg Training Loss: 8.318412976794773 - Avg Validation Loss: 4.040130842615057\n",
      "Epoch 35/65 - Avg Training Loss: 7.94347620540195 - Avg Validation Loss: 5.913765364223057\n",
      "Epoch 36/65 - Avg Training Loss: 8.607689899868435 - Avg Validation Loss: 6.9463238892731844\n",
      "Epoch 37/65 - Avg Training Loss: 8.706837439537049 - Avg Validation Loss: 4.771995147069295\n",
      "Epoch 38/65 - Avg Training Loss: 8.021616191334195 - Avg Validation Loss: 4.773858909253721\n",
      "Epoch 39/65 - Avg Training Loss: 7.773675073517693 - Avg Validation Loss: 4.9115679175765425\n",
      "Epoch 40/65 - Avg Training Loss: 8.46966619491577 - Avg Validation Loss: 5.580035302374098\n",
      "Epoch 41/65 - Avg Training Loss: 7.651567019356621 - Avg Validation Loss: 5.563319409335101\n",
      "Epoch 42/65 - Avg Training Loss: 7.68461487558153 - Avg Validation Loss: 4.699469844500224\n",
      "Epoch 43/65 - Avg Training Loss: 7.738138122028775 - Avg Validation Loss: 5.043880303700765\n",
      "Epoch 44/65 - Avg Training Loss: 7.938581275939941 - Avg Validation Loss: 4.906718611717224\n",
      "Epoch 45/65 - Avg Training Loss: 7.867752043406169 - Avg Validation Loss: 4.5187225341796875\n",
      "Epoch 46/65 - Avg Training Loss: 7.79652152856191 - Avg Validation Loss: 5.472911936265451\n",
      "Epoch 47/65 - Avg Training Loss: 7.320082116127014 - Avg Validation Loss: 4.680494158356278\n",
      "Epoch 48/65 - Avg Training Loss: 7.857534694671631 - Avg Validation Loss: 6.466454867963438\n",
      "Epoch 49/65 - Avg Training Loss: 7.821336152818468 - Avg Validation Loss: 4.206354322256865\n",
      "Epoch 50/65 - Avg Training Loss: 7.597037521998088 - Avg Validation Loss: 4.219575100474888\n",
      "Epoch 51/65 - Avg Training Loss: 7.161696192953322 - Avg Validation Loss: 5.056349586557459\n",
      "Epoch 52/65 - Avg Training Loss: 7.231332776281569 - Avg Validation Loss: 4.312554944444586\n",
      "Epoch 53/65 - Avg Training Loss: 7.270157859060499 - Avg Validation Loss: 4.362410112663552\n",
      "Epoch 54/65 - Avg Training Loss: 7.184047275119358 - Avg Validation Loss: 3.7587035254195884\n",
      "Epoch 55/65 - Avg Training Loss: 7.3102964825100365 - Avg Validation Loss: 4.411686058397646\n",
      "Epoch 56/65 - Avg Training Loss: 6.692113343874613 - Avg Validation Loss: 5.101004159009015\n",
      "Epoch 57/65 - Avg Training Loss: 7.233346403969659 - Avg Validation Loss: 5.06380108109227\n",
      "Epoch 58/65 - Avg Training Loss: 7.003829577234057 - Avg Validation Loss: 4.988452800997981\n",
      "Epoch 59/65 - Avg Training Loss: 6.992953475316366 - Avg Validation Loss: 4.705934762954712\n",
      "Epoch 60/65 - Avg Training Loss: 6.820708624521891 - Avg Validation Loss: 4.2337296803792315\n",
      "Epoch 61/65 - Avg Training Loss: 6.451669642660353 - Avg Validation Loss: 4.123954989292003\n",
      "Epoch 62/65 - Avg Training Loss: 6.96034247080485 - Avg Validation Loss: 5.23268610018271\n",
      "Epoch 63/65 - Avg Training Loss: 7.122290812598335 - Avg Validation Loss: 4.273083086367007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:44,846] Trial 14 finished with value: 3.9730366865793862 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 3, 'dropout_rate': 0.5077424177283034, 'lr': 0.00213362462464209, 'batch_size': 16, 'epochs': 65}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/65 - Avg Training Loss: 7.146181032392714 - Avg Validation Loss: 6.075936494050203\n",
      "Epoch 65/65 - Avg Training Loss: 7.531794161266751 - Avg Validation Loss: 3.9730366865793862\n",
      "Finished Training\n",
      "Epoch 1/70 - Avg Training Loss: 74.76790613598294 - Avg Validation Loss: 16.023126319602685\n",
      "Epoch 2/70 - Avg Training Loss: 23.778588422139485 - Avg Validation Loss: 9.97424382633633\n",
      "Epoch 3/70 - Avg Training Loss: 19.97334434721205 - Avg Validation Loss: 29.341245969136555\n",
      "Epoch 4/70 - Avg Training Loss: 20.42169647216797 - Avg Validation Loss: 10.05015539239954\n",
      "Epoch 5/70 - Avg Training Loss: 19.024804634518095 - Avg Validation Loss: 12.396247104362205\n",
      "Epoch 6/70 - Avg Training Loss: 18.573446602291533 - Avg Validation Loss: 11.25296265107614\n",
      "Epoch 7/70 - Avg Training Loss: 16.846674760182697 - Avg Validation Loss: 15.882523783931026\n",
      "Epoch 8/70 - Avg Training Loss: 17.265716764662002 - Avg Validation Loss: 20.023912888986093\n",
      "Epoch 9/70 - Avg Training Loss: 17.27398731443617 - Avg Validation Loss: 7.693471758453934\n",
      "Epoch 10/70 - Avg Training Loss: 18.045377529992 - Avg Validation Loss: 12.18650789614077\n",
      "Epoch 11/70 - Avg Training Loss: 17.357655435138277 - Avg Validation Loss: 18.83200020260281\n",
      "Epoch 12/70 - Avg Training Loss: 17.5445378780365 - Avg Validation Loss: 7.253086761192039\n",
      "Epoch 13/70 - Avg Training Loss: 16.279195510016546 - Avg Validation Loss: 15.052841786985043\n",
      "Epoch 14/70 - Avg Training Loss: 15.365513012144302 - Avg Validation Loss: 14.498867988586426\n",
      "Epoch 15/70 - Avg Training Loss: 16.102363236745198 - Avg Validation Loss: 10.600208741647226\n",
      "Epoch 16/70 - Avg Training Loss: 15.025336986117892 - Avg Validation Loss: 8.767601507681388\n",
      "Epoch 17/70 - Avg Training Loss: 15.487310960557725 - Avg Validation Loss: 7.1324305887575505\n",
      "Epoch 18/70 - Avg Training Loss: 14.67227545844184 - Avg Validation Loss: 6.965299747608326\n",
      "Epoch 19/70 - Avg Training Loss: 14.599487585491604 - Avg Validation Loss: 11.518197306880245\n",
      "Epoch 20/70 - Avg Training Loss: 13.48282299041748 - Avg Validation Loss: 11.688015513949924\n",
      "Epoch 21/70 - Avg Training Loss: 13.864942296346028 - Avg Validation Loss: 10.653622503633853\n",
      "Epoch 22/70 - Avg Training Loss: 13.73987300131056 - Avg Validation Loss: 14.206589239614981\n",
      "Epoch 23/70 - Avg Training Loss: 14.125012493133545 - Avg Validation Loss: 10.090558069723624\n",
      "Epoch 24/70 - Avg Training Loss: 12.403026093377008 - Avg Validation Loss: 13.59607873139558\n",
      "Epoch 25/70 - Avg Training Loss: 12.511100149154663 - Avg Validation Loss: 10.009132164495963\n",
      "Epoch 26/70 - Avg Training Loss: 12.93641931215922 - Avg Validation Loss: 10.70675712161594\n",
      "Epoch 27/70 - Avg Training Loss: 12.148106230629814 - Avg Validation Loss: 9.900825421015421\n",
      "Epoch 28/70 - Avg Training Loss: 11.043761608335707 - Avg Validation Loss: 10.988022795429936\n",
      "Epoch 29/70 - Avg Training Loss: 12.124744706683689 - Avg Validation Loss: 7.4942741570649325\n",
      "Epoch 30/70 - Avg Training Loss: 10.989454306496514 - Avg Validation Loss: 5.177581694391039\n",
      "Epoch 31/70 - Avg Training Loss: 10.251791127522786 - Avg Validation Loss: 5.693730226269475\n",
      "Epoch 32/70 - Avg Training Loss: 10.739498419231838 - Avg Validation Loss: 5.285873077533863\n",
      "Epoch 33/70 - Avg Training Loss: 10.336222743988037 - Avg Validation Loss: 7.479607396655613\n",
      "Epoch 34/70 - Avg Training Loss: 9.705405211448669 - Avg Validation Loss: 5.574112256368001\n",
      "Epoch 35/70 - Avg Training Loss: 10.145256095462376 - Avg Validation Loss: 5.705824070506626\n",
      "Epoch 36/70 - Avg Training Loss: 9.512841357125176 - Avg Validation Loss: 4.4145334252604735\n",
      "Epoch 37/70 - Avg Training Loss: 9.646237818400065 - Avg Validation Loss: 4.830939125131677\n",
      "Epoch 38/70 - Avg Training Loss: 8.99011897246043 - Avg Validation Loss: 6.6523115016795975\n",
      "Epoch 39/70 - Avg Training Loss: 9.304378011491563 - Avg Validation Loss: 4.779438570693687\n",
      "Epoch 40/70 - Avg Training Loss: 9.253582196765477 - Avg Validation Loss: 7.34500855869717\n",
      "Epoch 41/70 - Avg Training Loss: 8.680117352803547 - Avg Validation Loss: 6.430722947473879\n",
      "Epoch 42/70 - Avg Training Loss: 8.521261141035293 - Avg Validation Loss: 4.391034347039682\n",
      "Epoch 43/70 - Avg Training Loss: 8.668246873219807 - Avg Validation Loss: 4.891030647136547\n",
      "Epoch 44/70 - Avg Training Loss: 9.507012179162768 - Avg Validation Loss: 4.747934705681271\n",
      "Epoch 45/70 - Avg Training Loss: 8.585135131412082 - Avg Validation Loss: 4.589996523327297\n",
      "Epoch 46/70 - Avg Training Loss: 8.803917137781779 - Avg Validation Loss: 6.1196744883501974\n",
      "Epoch 47/70 - Avg Training Loss: 8.91176495552063 - Avg Validation Loss: 5.775556745352568\n",
      "Epoch 48/70 - Avg Training Loss: 8.832263350486755 - Avg Validation Loss: 4.198685027934887\n",
      "Epoch 49/70 - Avg Training Loss: 8.25048303604126 - Avg Validation Loss: 3.986557633788497\n",
      "Epoch 50/70 - Avg Training Loss: 7.993483214908176 - Avg Validation Loss: 4.296733260154724\n",
      "Epoch 51/70 - Avg Training Loss: 8.417311514748468 - Avg Validation Loss: 5.392935192143476\n",
      "Epoch 52/70 - Avg Training Loss: 7.901960484186808 - Avg Validation Loss: 4.283014610961631\n",
      "Epoch 53/70 - Avg Training Loss: 8.100903259383308 - Avg Validation Loss: 4.250362943719934\n",
      "Epoch 54/70 - Avg Training Loss: 8.048023170895046 - Avg Validation Loss: 5.485848113342568\n",
      "Epoch 55/70 - Avg Training Loss: 7.743832699457804 - Avg Validation Loss: 4.083560404954134\n",
      "Epoch 56/70 - Avg Training Loss: 7.389952778816223 - Avg Validation Loss: 4.610138815862161\n",
      "Epoch 57/70 - Avg Training Loss: 7.8061495145161945 - Avg Validation Loss: 4.426603396733602\n",
      "Epoch 58/70 - Avg Training Loss: 7.063063218858507 - Avg Validation Loss: 3.851521642119796\n",
      "Epoch 59/70 - Avg Training Loss: 7.677821591165331 - Avg Validation Loss: 4.892032106717427\n",
      "Epoch 60/70 - Avg Training Loss: 7.3717773569954765 - Avg Validation Loss: 3.547267743834743\n",
      "Epoch 61/70 - Avg Training Loss: 7.3093240525987415 - Avg Validation Loss: 4.498010012838575\n",
      "Epoch 62/70 - Avg Training Loss: 7.045052586661445 - Avg Validation Loss: 4.300973490432456\n",
      "Epoch 63/70 - Avg Training Loss: 7.028302266862657 - Avg Validation Loss: 4.68372063725083\n",
      "Epoch 64/70 - Avg Training Loss: 7.15107356707255 - Avg Validation Loss: 3.7268811994128757\n",
      "Epoch 65/70 - Avg Training Loss: 6.5938512616687355 - Avg Validation Loss: 4.137898074256049\n",
      "Epoch 66/70 - Avg Training Loss: 7.223127932018704 - Avg Validation Loss: 3.804407126373715\n",
      "Epoch 67/70 - Avg Training Loss: 6.659044170379639 - Avg Validation Loss: 4.659926167240849\n",
      "Epoch 68/70 - Avg Training Loss: 6.6629113753636675 - Avg Validation Loss: 4.130938446080243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:45:55,292] Trial 15 finished with value: 4.639523267745972 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 3, 'dropout_rate': 0.48557056349848693, 'lr': 0.0010594467793461175, 'batch_size': 16, 'epochs': 70}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70 - Avg Training Loss: 6.819723508093092 - Avg Validation Loss: 4.07570692786464\n",
      "Epoch 70/70 - Avg Training Loss: 6.4695532374911835 - Avg Validation Loss: 4.639523267745972\n",
      "Finished Training\n",
      "Epoch 1/71 - Avg Training Loss: 66.00121739705403 - Avg Validation Loss: 57.624518924289276\n",
      "Epoch 2/71 - Avg Training Loss: 41.206521330939395 - Avg Validation Loss: 54.18871801870841\n",
      "Epoch 3/71 - Avg Training Loss: 38.710598182678225 - Avg Validation Loss: 36.10861979590522\n",
      "Epoch 4/71 - Avg Training Loss: 34.21573855082194 - Avg Validation Loss: 29.793880921823007\n",
      "Epoch 5/71 - Avg Training Loss: 35.093009334140355 - Avg Validation Loss: 42.34347590693721\n",
      "Epoch 6/71 - Avg Training Loss: 29.93242689768473 - Avg Validation Loss: 24.476673090899432\n",
      "Epoch 7/71 - Avg Training Loss: 24.64804654651218 - Avg Validation Loss: 34.21239764602096\n",
      "Epoch 8/71 - Avg Training Loss: 20.153272067175973 - Avg Validation Loss: 21.626317624692565\n",
      "Epoch 9/71 - Avg Training Loss: 19.181752268473307 - Avg Validation Loss: 15.09026790548254\n",
      "Epoch 10/71 - Avg Training Loss: 17.78423390918308 - Avg Validation Loss: 11.817131042480469\n",
      "Epoch 11/71 - Avg Training Loss: 17.197085263994005 - Avg Validation Loss: 11.56081767435427\n",
      "Epoch 12/71 - Avg Training Loss: 17.708246114518907 - Avg Validation Loss: 20.077195202862775\n",
      "Epoch 13/71 - Avg Training Loss: 16.84268505308363 - Avg Validation Loss: 11.33951367272271\n",
      "Epoch 14/71 - Avg Training Loss: 16.437180444929336 - Avg Validation Loss: 12.88066237061112\n",
      "Epoch 15/71 - Avg Training Loss: 17.02466385099623 - Avg Validation Loss: 9.935917571738914\n",
      "Epoch 16/71 - Avg Training Loss: 16.622195879618328 - Avg Validation Loss: 10.352621608310276\n",
      "Epoch 17/71 - Avg Training Loss: 16.507591448889837 - Avg Validation Loss: 9.436851801695648\n",
      "Epoch 18/71 - Avg Training Loss: 15.599658298492432 - Avg Validation Loss: 10.106482373343574\n",
      "Epoch 19/71 - Avg Training Loss: 14.738806687461006 - Avg Validation Loss: 10.065163259153012\n",
      "Epoch 20/71 - Avg Training Loss: 15.333148331112332 - Avg Validation Loss: 9.520919526064837\n",
      "Epoch 21/71 - Avg Training Loss: 15.611614576975505 - Avg Validation Loss: 10.447422248345834\n",
      "Epoch 22/71 - Avg Training Loss: 14.496465057796902 - Avg Validation Loss: 11.331705269990143\n",
      "Epoch 23/71 - Avg Training Loss: 15.184804677963257 - Avg Validation Loss: 9.63504108676204\n",
      "Epoch 24/71 - Avg Training Loss: 14.492657735612656 - Avg Validation Loss: 8.839460991047046\n",
      "Epoch 25/71 - Avg Training Loss: 13.811756663852268 - Avg Validation Loss: 8.077656595795244\n",
      "Epoch 26/71 - Avg Training Loss: 13.799054050445557 - Avg Validation Loss: 7.747569675798769\n",
      "Epoch 27/71 - Avg Training Loss: 13.633807955847846 - Avg Validation Loss: 9.278711195345279\n",
      "Epoch 28/71 - Avg Training Loss: 13.509171088536581 - Avg Validation Loss: 9.39423355349788\n",
      "Epoch 29/71 - Avg Training Loss: 13.561194070180257 - Avg Validation Loss: 8.801203445152\n",
      "Epoch 30/71 - Avg Training Loss: 13.49140780766805 - Avg Validation Loss: 8.104231004361752\n",
      "Epoch 31/71 - Avg Training Loss: 13.3898112932841 - Avg Validation Loss: 9.172792125631261\n",
      "Epoch 32/71 - Avg Training Loss: 13.231704945034451 - Avg Validation Loss: 7.224545258062857\n",
      "Epoch 33/71 - Avg Training Loss: 13.725046830707127 - Avg Validation Loss: 7.56613035555239\n",
      "Epoch 34/71 - Avg Training Loss: 13.18190278477139 - Avg Validation Loss: 8.26069423004433\n",
      "Epoch 35/71 - Avg Training Loss: 13.48304958873325 - Avg Validation Loss: 7.4790997681794344\n",
      "Epoch 36/71 - Avg Training Loss: 12.439526059892442 - Avg Validation Loss: 6.674442821078831\n",
      "Epoch 37/71 - Avg Training Loss: 12.965395514170329 - Avg Validation Loss: 6.980724767402366\n",
      "Epoch 38/71 - Avg Training Loss: 12.714490138159858 - Avg Validation Loss: 7.720636473761664\n",
      "Epoch 39/71 - Avg Training Loss: 11.708169984817506 - Avg Validation Loss: 7.799942413965861\n",
      "Epoch 40/71 - Avg Training Loss: 12.461423105663723 - Avg Validation Loss: 7.506302277247111\n",
      "Epoch 41/71 - Avg Training Loss: 11.885198879241944 - Avg Validation Loss: 6.594358541347362\n",
      "Epoch 42/71 - Avg Training Loss: 12.242750337388781 - Avg Validation Loss: 7.022005443219785\n",
      "Epoch 43/71 - Avg Training Loss: 11.7627413643731 - Avg Validation Loss: 7.112934086057875\n",
      "Epoch 44/71 - Avg Training Loss: 11.502166191736857 - Avg Validation Loss: 6.7151683701409235\n",
      "Epoch 45/71 - Avg Training Loss: 10.837195512983534 - Avg Validation Loss: 6.937105421666746\n",
      "Epoch 46/71 - Avg Training Loss: 11.39709373050266 - Avg Validation Loss: 6.721283259215178\n",
      "Epoch 47/71 - Avg Training Loss: 11.478426202138264 - Avg Validation Loss: 6.654064619982684\n",
      "Epoch 48/71 - Avg Training Loss: 11.562614976035224 - Avg Validation Loss: 6.556520267769143\n",
      "Epoch 49/71 - Avg Training Loss: 10.986838743421767 - Avg Validation Loss: 6.013420149132058\n",
      "Epoch 50/71 - Avg Training Loss: 11.180294285880194 - Avg Validation Loss: 6.719291320553532\n",
      "Epoch 51/71 - Avg Training Loss: 10.610593567954169 - Avg Validation Loss: 6.351995048699556\n",
      "Epoch 52/71 - Avg Training Loss: 11.197002262539334 - Avg Validation Loss: 6.537487286108512\n",
      "Epoch 53/71 - Avg Training Loss: 10.446044789420235 - Avg Validation Loss: 5.595721024054068\n",
      "Epoch 54/71 - Avg Training Loss: 10.810767740673489 - Avg Validation Loss: 5.79959942234887\n",
      "Epoch 55/71 - Avg Training Loss: 10.225452926423815 - Avg Validation Loss: 6.061626341607836\n",
      "Epoch 56/71 - Avg Training Loss: 10.84654197163052 - Avg Validation Loss: 6.351440010247408\n",
      "Epoch 57/71 - Avg Training Loss: 10.43289024564955 - Avg Validation Loss: 6.567857614269963\n",
      "Epoch 58/71 - Avg Training Loss: 10.145786587397257 - Avg Validation Loss: 5.852816458101626\n",
      "Epoch 59/71 - Avg Training Loss: 10.869702932569716 - Avg Validation Loss: 6.383804784880744\n",
      "Epoch 60/71 - Avg Training Loss: 10.788977395163641 - Avg Validation Loss: 6.088000191582574\n",
      "Epoch 61/71 - Avg Training Loss: 10.473335679372152 - Avg Validation Loss: 6.175504790412055\n",
      "Epoch 62/71 - Avg Training Loss: 10.750916613472832 - Avg Validation Loss: 6.452099663239938\n",
      "Epoch 63/71 - Avg Training Loss: 10.552748452292548 - Avg Validation Loss: 5.886546580879776\n",
      "Epoch 64/71 - Avg Training Loss: 10.41888920466105 - Avg Validation Loss: 5.9209389068462235\n",
      "Epoch 65/71 - Avg Training Loss: 9.643011744817098 - Avg Validation Loss: 6.56347080513283\n",
      "Epoch 66/71 - Avg Training Loss: 9.774521583980984 - Avg Validation Loss: 6.398570594964204\n",
      "Epoch 67/71 - Avg Training Loss: 10.000315226448906 - Avg Validation Loss: 5.9396104591864125\n",
      "Epoch 68/71 - Avg Training Loss: 9.852714607450697 - Avg Validation Loss: 6.054962582058376\n",
      "Epoch 69/71 - Avg Training Loss: 9.877066447999741 - Avg Validation Loss: 6.268763886557685\n",
      "Epoch 70/71 - Avg Training Loss: 10.09012933837043 - Avg Validation Loss: 6.538411021232605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:07,927] Trial 16 finished with value: 6.585379159009015 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 4, 'dropout_rate': 0.5397102989319795, 'lr': 0.0026302228902952574, 'batch_size': 16, 'epochs': 71}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/71 - Avg Training Loss: 10.498724905649821 - Avg Validation Loss: 6.585379159009015\n",
      "Finished Training\n",
      "Epoch 1/99 - Avg Training Loss: 114.59789873206097 - Avg Validation Loss: 17.385094710758754\n",
      "Epoch 2/99 - Avg Training Loss: 31.992070239523184 - Avg Validation Loss: 22.767155511038645\n",
      "Epoch 3/99 - Avg Training Loss: 26.48811506188434 - Avg Validation Loss: 11.536950588226318\n",
      "Epoch 4/99 - Avg Training Loss: 23.77991054369056 - Avg Validation Loss: 13.670168263571602\n",
      "Epoch 5/99 - Avg Training Loss: 20.763303507929262 - Avg Validation Loss: 15.353155544825963\n",
      "Epoch 6/99 - Avg Training Loss: 19.804967755856723 - Avg Validation Loss: 15.037658895765032\n",
      "Epoch 7/99 - Avg Training Loss: 19.053624401921812 - Avg Validation Loss: 16.528761182512557\n",
      "Epoch 8/99 - Avg Training Loss: 19.165335116179094 - Avg Validation Loss: 13.075459344046456\n",
      "Epoch 9/99 - Avg Training Loss: 19.123726015505582 - Avg Validation Loss: 10.460554872240339\n",
      "Epoch 10/99 - Avg Training Loss: 18.681735080221426 - Avg Validation Loss: 12.878127302442278\n",
      "Epoch 11/99 - Avg Training Loss: 18.372986254484758 - Avg Validation Loss: 15.2681884765625\n",
      "Epoch 12/99 - Avg Training Loss: 17.61677659076193 - Avg Validation Loss: 14.676203114645821\n",
      "Epoch 13/99 - Avg Training Loss: 16.227523679318637 - Avg Validation Loss: 13.347198554447719\n",
      "Epoch 14/99 - Avg Training Loss: 16.667813466942828 - Avg Validation Loss: 19.307375771658762\n",
      "Epoch 15/99 - Avg Training Loss: 16.508388850999914 - Avg Validation Loss: 13.983306816646031\n",
      "Epoch 16/99 - Avg Training Loss: 16.923564123070758 - Avg Validation Loss: 18.486189569745743\n",
      "Epoch 17/99 - Avg Training Loss: 17.86361723360808 - Avg Validation Loss: 10.044930730547224\n",
      "Epoch 18/99 - Avg Training Loss: 16.47331196328868 - Avg Validation Loss: 15.976501669202532\n",
      "Epoch 19/99 - Avg Training Loss: 16.49579782071321 - Avg Validation Loss: 15.17203256062099\n",
      "Epoch 20/99 - Avg Training Loss: 15.941390493641729 - Avg Validation Loss: 12.14227649143764\n",
      "Epoch 21/99 - Avg Training Loss: 15.000824306322182 - Avg Validation Loss: 14.490397930145264\n",
      "Epoch 22/99 - Avg Training Loss: 16.545731171317723 - Avg Validation Loss: 11.488115651266915\n",
      "Epoch 23/99 - Avg Training Loss: 15.019100977026898 - Avg Validation Loss: 11.323754174368721\n",
      "Epoch 24/99 - Avg Training Loss: 15.587599629941193 - Avg Validation Loss: 21.099362782069615\n",
      "Epoch 25/99 - Avg Training Loss: 15.801408270130986 - Avg Validation Loss: 11.317297390529088\n",
      "Epoch 26/99 - Avg Training Loss: 15.128568441971488 - Avg Validation Loss: 16.30011224746704\n",
      "Epoch 27/99 - Avg Training Loss: 14.48302351910135 - Avg Validation Loss: 7.956463166645595\n",
      "Epoch 28/99 - Avg Training Loss: 14.64652156829834 - Avg Validation Loss: 14.477647236415319\n",
      "Epoch 29/99 - Avg Training Loss: 13.856737510017727 - Avg Validation Loss: 10.14520720073155\n",
      "Epoch 30/99 - Avg Training Loss: 16.201430320739746 - Avg Validation Loss: 13.553518772125244\n",
      "Epoch 31/99 - Avg Training Loss: 14.630590604699176 - Avg Validation Loss: 12.203949792044503\n",
      "Epoch 32/99 - Avg Training Loss: 14.274148236150326 - Avg Validation Loss: 9.759731701442174\n",
      "Epoch 33/99 - Avg Training Loss: 14.591032981872559 - Avg Validation Loss: 11.892891338893346\n",
      "Epoch 34/99 - Avg Training Loss: 13.453724695288617 - Avg Validation Loss: 15.013306822095599\n",
      "Epoch 35/99 - Avg Training Loss: 13.414923253266707 - Avg Validation Loss: 19.504530906677246\n",
      "Epoch 36/99 - Avg Training Loss: 13.025736062423043 - Avg Validation Loss: 9.932228224618095\n",
      "Epoch 37/99 - Avg Training Loss: 12.628700090491254 - Avg Validation Loss: 10.480402605874199\n",
      "Epoch 38/99 - Avg Training Loss: 12.312933673029361 - Avg Validation Loss: 12.299721854073661\n",
      "Epoch 39/99 - Avg Training Loss: 12.578745095626168 - Avg Validation Loss: 9.359707151140485\n",
      "Epoch 40/99 - Avg Training Loss: 12.458575020665707 - Avg Validation Loss: 12.957794189453125\n",
      "Epoch 41/99 - Avg Training Loss: 12.378209984820822 - Avg Validation Loss: 7.134790897369385\n",
      "Epoch 42/99 - Avg Training Loss: 12.031881373861562 - Avg Validation Loss: 9.813505104609899\n",
      "Epoch 43/99 - Avg Training Loss: 11.02216563017472 - Avg Validation Loss: 11.211945397513253\n",
      "Epoch 44/99 - Avg Training Loss: 11.510444226472273 - Avg Validation Loss: 6.091734681810651\n",
      "Epoch 45/99 - Avg Training Loss: 11.468447353528893 - Avg Validation Loss: 5.289054410798209\n",
      "Epoch 46/99 - Avg Training Loss: 10.252013932103695 - Avg Validation Loss: 6.864945275442941\n",
      "Epoch 47/99 - Avg Training Loss: 9.908649216527524 - Avg Validation Loss: 9.218247652053833\n",
      "Epoch 48/99 - Avg Training Loss: 10.76347707665485 - Avg Validation Loss: 10.61817114693778\n",
      "Epoch 49/99 - Avg Training Loss: 11.120155375936758 - Avg Validation Loss: 8.50978854724339\n",
      "Epoch 50/99 - Avg Training Loss: 10.939041324283766 - Avg Validation Loss: 7.725642238344465\n",
      "Epoch 51/99 - Avg Training Loss: 10.327821254730225 - Avg Validation Loss: 6.195955344608852\n",
      "Epoch 52/99 - Avg Training Loss: 11.090409175209377 - Avg Validation Loss: 4.678298371178763\n",
      "Epoch 53/99 - Avg Training Loss: 10.788342392962912 - Avg Validation Loss: 3.8833085809435164\n",
      "Epoch 54/99 - Avg Training Loss: 10.495126765707266 - Avg Validation Loss: 4.108289088521685\n",
      "Epoch 55/99 - Avg Training Loss: 9.714345227117123 - Avg Validation Loss: 6.347627673830305\n",
      "Epoch 56/99 - Avg Training Loss: 9.677055172298266 - Avg Validation Loss: 5.184086254664829\n",
      "Epoch 57/99 - Avg Training Loss: 10.013228209122367 - Avg Validation Loss: 5.842155286243984\n",
      "Epoch 58/99 - Avg Training Loss: 10.059895950814951 - Avg Validation Loss: 4.430256741387503\n",
      "Epoch 59/99 - Avg Training Loss: 9.56332598561826 - Avg Validation Loss: 7.35723226411002\n",
      "Epoch 60/99 - Avg Training Loss: 9.974276418271272 - Avg Validation Loss: 4.474167994090489\n",
      "Epoch 61/99 - Avg Training Loss: 9.679645351741625 - Avg Validation Loss: 5.76122202192034\n",
      "Epoch 62/99 - Avg Training Loss: 8.9153180744337 - Avg Validation Loss: 5.7818326609475275\n",
      "Epoch 63/99 - Avg Training Loss: 8.883739823880402 - Avg Validation Loss: 8.171557971409388\n",
      "Epoch 64/99 - Avg Training Loss: 9.408251555069633 - Avg Validation Loss: 4.6082899911063055\n",
      "Epoch 65/99 - Avg Training Loss: 9.115487699923309 - Avg Validation Loss: 6.435031379972186\n",
      "Epoch 66/99 - Avg Training Loss: 9.209003697270932 - Avg Validation Loss: 5.372322695595877\n",
      "Epoch 67/99 - Avg Training Loss: 9.322389188020125 - Avg Validation Loss: 5.707674264907837\n",
      "Epoch 68/99 - Avg Training Loss: 8.997658397840416 - Avg Validation Loss: 4.1901825325829645\n",
      "Epoch 69/99 - Avg Training Loss: 8.462665806645932 - Avg Validation Loss: 4.982867445264544\n",
      "Epoch 70/99 - Avg Training Loss: 8.92867784914763 - Avg Validation Loss: 4.944183213370187\n",
      "Epoch 71/99 - Avg Training Loss: 8.625151572020158 - Avg Validation Loss: 4.781593833650861\n",
      "Epoch 72/99 - Avg Training Loss: 9.05466826065727 - Avg Validation Loss: 4.498742376055036\n",
      "Epoch 73/99 - Avg Training Loss: 8.131905037423838 - Avg Validation Loss: 4.816868373325893\n",
      "Epoch 74/99 - Avg Training Loss: 9.041104772816533 - Avg Validation Loss: 5.422154205186026\n",
      "Epoch 75/99 - Avg Training Loss: 9.00055748483409 - Avg Validation Loss: 4.215947934559414\n",
      "Epoch 76/99 - Avg Training Loss: 9.219783700030783 - Avg Validation Loss: 5.451188325881958\n",
      "Epoch 77/99 - Avg Training Loss: 8.3905058529066 - Avg Validation Loss: 4.46794559274401\n",
      "Epoch 78/99 - Avg Training Loss: 8.649874687194824 - Avg Validation Loss: 5.260699255125863\n",
      "Epoch 79/99 - Avg Training Loss: 8.455151640850564 - Avg Validation Loss: 5.376122849328177\n",
      "Epoch 80/99 - Avg Training Loss: 8.433752868486488 - Avg Validation Loss: 4.492562225886753\n",
      "Epoch 81/99 - Avg Training Loss: 8.522235517916473 - Avg Validation Loss: 6.018328530447824\n",
      "Epoch 82/99 - Avg Training Loss: 8.330221010291059 - Avg Validation Loss: 4.371925575392587\n",
      "Epoch 83/99 - Avg Training Loss: 8.151149957076363 - Avg Validation Loss: 5.276817900793893\n",
      "Epoch 84/99 - Avg Training Loss: 8.437366423399553 - Avg Validation Loss: 4.421321630477905\n",
      "Epoch 85/99 - Avg Training Loss: 8.306976981784986 - Avg Validation Loss: 4.9897797618593485\n",
      "Epoch 86/99 - Avg Training Loss: 7.507070956022843 - Avg Validation Loss: 4.671071648597717\n",
      "Epoch 87/99 - Avg Training Loss: 7.763175176537556 - Avg Validation Loss: 4.698059780257089\n",
      "Epoch 88/99 - Avg Training Loss: 7.56160367053488 - Avg Validation Loss: 4.2798352752413065\n",
      "Epoch 89/99 - Avg Training Loss: 7.624006209166153 - Avg Validation Loss: 4.763673782348633\n",
      "Epoch 90/99 - Avg Training Loss: 7.286173405854599 - Avg Validation Loss: 4.411362494741168\n",
      "Epoch 91/99 - Avg Training Loss: 7.6941118240356445 - Avg Validation Loss: 3.9629068885530745\n",
      "Epoch 92/99 - Avg Training Loss: 7.762048970098081 - Avg Validation Loss: 4.3693220274788995\n",
      "Epoch 93/99 - Avg Training Loss: 7.981074270994767 - Avg Validation Loss: 4.534326655524118\n",
      "Epoch 94/99 - Avg Training Loss: 7.437641745028288 - Avg Validation Loss: 5.177816476140704\n",
      "Epoch 95/99 - Avg Training Loss: 7.301281887552013 - Avg Validation Loss: 4.8772503307887485\n",
      "Epoch 96/99 - Avg Training Loss: 7.690122625102168 - Avg Validation Loss: 3.7951717376708984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:13,342] Trial 17 finished with value: 4.547011034829276 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 3, 'dropout_rate': 0.5016862704256384, 'lr': 0.0020968943904403365, 'batch_size': 64, 'epochs': 99}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/99 - Avg Training Loss: 7.330980798472529 - Avg Validation Loss: 4.123343127114432\n",
      "Epoch 98/99 - Avg Training Loss: 7.4972468044446865 - Avg Validation Loss: 4.425627452986581\n",
      "Epoch 99/99 - Avg Training Loss: 7.465008880781091 - Avg Validation Loss: 4.547011034829276\n",
      "Finished Training\n",
      "Epoch 1/124 - Avg Training Loss: 178.67118215560913 - Avg Validation Loss: 29.380949020385742\n",
      "Epoch 2/124 - Avg Training Loss: 47.36976385116577 - Avg Validation Loss: 16.07899112701416\n",
      "Epoch 3/124 - Avg Training Loss: 32.94851040840149 - Avg Validation Loss: 16.068034744262697\n",
      "Epoch 4/124 - Avg Training Loss: 30.880611896514893 - Avg Validation Loss: 16.881774139404296\n",
      "Epoch 5/124 - Avg Training Loss: 26.098514080047607 - Avg Validation Loss: 14.41964340209961\n",
      "Epoch 6/124 - Avg Training Loss: 23.3233380317688 - Avg Validation Loss: 14.898517036437989\n",
      "Epoch 7/124 - Avg Training Loss: 22.092673540115356 - Avg Validation Loss: 13.323655700683593\n",
      "Epoch 8/124 - Avg Training Loss: 21.6831316947937 - Avg Validation Loss: 11.476618766784668\n",
      "Epoch 9/124 - Avg Training Loss: 21.229684352874756 - Avg Validation Loss: 12.241747283935547\n",
      "Epoch 10/124 - Avg Training Loss: 20.012212991714478 - Avg Validation Loss: 15.965456008911133\n",
      "Epoch 11/124 - Avg Training Loss: 18.535467743873596 - Avg Validation Loss: 10.37474365234375\n",
      "Epoch 12/124 - Avg Training Loss: 19.170600414276123 - Avg Validation Loss: 15.299702644348145\n",
      "Epoch 13/124 - Avg Training Loss: 18.49387550354004 - Avg Validation Loss: 16.789318084716797\n",
      "Epoch 14/124 - Avg Training Loss: 17.721848726272583 - Avg Validation Loss: 7.427291965484619\n",
      "Epoch 15/124 - Avg Training Loss: 19.05333971977234 - Avg Validation Loss: 12.767097473144531\n",
      "Epoch 16/124 - Avg Training Loss: 17.44814920425415 - Avg Validation Loss: 12.209332275390626\n",
      "Epoch 17/124 - Avg Training Loss: 16.405388951301575 - Avg Validation Loss: 19.168751525878907\n",
      "Epoch 18/124 - Avg Training Loss: 17.686643481254578 - Avg Validation Loss: 10.959656715393066\n",
      "Epoch 19/124 - Avg Training Loss: 17.017385363578796 - Avg Validation Loss: 14.074575614929199\n",
      "Epoch 20/124 - Avg Training Loss: 16.948893666267395 - Avg Validation Loss: 12.835732460021973\n",
      "Epoch 21/124 - Avg Training Loss: 16.7072856426239 - Avg Validation Loss: 18.26686897277832\n",
      "Epoch 22/124 - Avg Training Loss: 16.146928668022156 - Avg Validation Loss: 16.786249732971193\n",
      "Epoch 23/124 - Avg Training Loss: 15.954186081886292 - Avg Validation Loss: 9.918625259399414\n",
      "Epoch 24/124 - Avg Training Loss: 17.578704357147217 - Avg Validation Loss: 26.23519859313965\n",
      "Epoch 25/124 - Avg Training Loss: 17.483590602874756 - Avg Validation Loss: 9.049907302856445\n",
      "Epoch 26/124 - Avg Training Loss: 18.192232728004456 - Avg Validation Loss: 27.540198516845702\n",
      "Epoch 27/124 - Avg Training Loss: 18.75382137298584 - Avg Validation Loss: 21.536820220947266\n",
      "Epoch 28/124 - Avg Training Loss: 16.534494400024414 - Avg Validation Loss: 6.791676139831543\n",
      "Epoch 29/124 - Avg Training Loss: 16.714438915252686 - Avg Validation Loss: 17.6971794128418\n",
      "Epoch 30/124 - Avg Training Loss: 16.51107156276703 - Avg Validation Loss: 25.377619171142577\n",
      "Epoch 31/124 - Avg Training Loss: 15.6893630027771 - Avg Validation Loss: 16.301364707946778\n",
      "Epoch 32/124 - Avg Training Loss: 15.112015128135681 - Avg Validation Loss: 16.677830123901366\n",
      "Epoch 33/124 - Avg Training Loss: 14.086340546607971 - Avg Validation Loss: 15.29134407043457\n",
      "Epoch 34/124 - Avg Training Loss: 15.563325762748718 - Avg Validation Loss: 26.0203067779541\n",
      "Epoch 35/124 - Avg Training Loss: 15.812631607055664 - Avg Validation Loss: 9.689886665344238\n",
      "Epoch 36/124 - Avg Training Loss: 16.129971027374268 - Avg Validation Loss: 17.874885177612306\n",
      "Epoch 37/124 - Avg Training Loss: 14.775238633155823 - Avg Validation Loss: 15.28141918182373\n",
      "Epoch 38/124 - Avg Training Loss: 14.377224206924438 - Avg Validation Loss: 17.212858200073242\n",
      "Epoch 39/124 - Avg Training Loss: 14.937750220298767 - Avg Validation Loss: 21.0025276184082\n",
      "Epoch 40/124 - Avg Training Loss: 15.1526859998703 - Avg Validation Loss: 10.75623722076416\n",
      "Epoch 41/124 - Avg Training Loss: 13.844750165939331 - Avg Validation Loss: 16.99007339477539\n",
      "Epoch 42/124 - Avg Training Loss: 14.109177708625793 - Avg Validation Loss: 16.28455696105957\n",
      "Epoch 43/124 - Avg Training Loss: 14.274543523788452 - Avg Validation Loss: 15.787946319580078\n",
      "Epoch 44/124 - Avg Training Loss: 14.017300963401794 - Avg Validation Loss: 19.115575790405273\n",
      "Epoch 45/124 - Avg Training Loss: 14.108367562294006 - Avg Validation Loss: 13.367072677612304\n",
      "Epoch 46/124 - Avg Training Loss: 13.47490382194519 - Avg Validation Loss: 13.427408218383789\n",
      "Epoch 47/124 - Avg Training Loss: 14.04708731174469 - Avg Validation Loss: 13.383135986328124\n",
      "Epoch 48/124 - Avg Training Loss: 13.807672381401062 - Avg Validation Loss: 20.08684730529785\n",
      "Epoch 49/124 - Avg Training Loss: 13.730246186256409 - Avg Validation Loss: 14.366445159912109\n",
      "Epoch 50/124 - Avg Training Loss: 13.116400122642517 - Avg Validation Loss: 22.567918014526366\n",
      "Epoch 51/124 - Avg Training Loss: 12.957151651382446 - Avg Validation Loss: 16.08005313873291\n",
      "Epoch 52/124 - Avg Training Loss: 12.837248802185059 - Avg Validation Loss: 11.208238410949708\n",
      "Epoch 53/124 - Avg Training Loss: 12.87201738357544 - Avg Validation Loss: 19.30707130432129\n",
      "Epoch 54/124 - Avg Training Loss: 12.87981379032135 - Avg Validation Loss: 12.236165237426757\n",
      "Epoch 55/124 - Avg Training Loss: 12.25333559513092 - Avg Validation Loss: 18.780071640014647\n",
      "Epoch 56/124 - Avg Training Loss: 12.702115416526794 - Avg Validation Loss: 15.975102806091309\n",
      "Epoch 57/124 - Avg Training Loss: 12.630126714706421 - Avg Validation Loss: 10.436481094360351\n",
      "Epoch 58/124 - Avg Training Loss: 11.640143394470215 - Avg Validation Loss: 15.255575370788574\n",
      "Epoch 59/124 - Avg Training Loss: 11.809165716171265 - Avg Validation Loss: 11.468472862243653\n",
      "Epoch 60/124 - Avg Training Loss: 11.73005747795105 - Avg Validation Loss: 13.14096279144287\n",
      "Epoch 61/124 - Avg Training Loss: 11.062437057495117 - Avg Validation Loss: 9.979361724853515\n",
      "Epoch 62/124 - Avg Training Loss: 11.293641924858093 - Avg Validation Loss: 9.45399875640869\n",
      "Epoch 63/124 - Avg Training Loss: 11.139104843139648 - Avg Validation Loss: 8.845753479003907\n",
      "Epoch 64/124 - Avg Training Loss: 11.440417051315308 - Avg Validation Loss: 10.215956497192384\n",
      "Epoch 65/124 - Avg Training Loss: 11.127747416496277 - Avg Validation Loss: 11.03831672668457\n",
      "Epoch 66/124 - Avg Training Loss: 10.968953967094421 - Avg Validation Loss: 9.86806354522705\n",
      "Epoch 67/124 - Avg Training Loss: 10.867695927619934 - Avg Validation Loss: 9.809822273254394\n",
      "Epoch 68/124 - Avg Training Loss: 10.930190086364746 - Avg Validation Loss: 7.817954635620117\n",
      "Epoch 69/124 - Avg Training Loss: 11.039603233337402 - Avg Validation Loss: 6.476900959014893\n",
      "Epoch 70/124 - Avg Training Loss: 10.879584550857544 - Avg Validation Loss: 7.94970293045044\n",
      "Epoch 71/124 - Avg Training Loss: 10.497190594673157 - Avg Validation Loss: 9.0175142288208\n",
      "Epoch 72/124 - Avg Training Loss: 10.619771838188171 - Avg Validation Loss: 9.090794372558594\n",
      "Epoch 73/124 - Avg Training Loss: 10.62500548362732 - Avg Validation Loss: 9.07761116027832\n",
      "Epoch 74/124 - Avg Training Loss: 10.759308695793152 - Avg Validation Loss: 7.369151306152344\n",
      "Epoch 75/124 - Avg Training Loss: 10.407382369041443 - Avg Validation Loss: 7.210928058624267\n",
      "Epoch 76/124 - Avg Training Loss: 10.039320349693298 - Avg Validation Loss: 7.84378604888916\n",
      "Epoch 77/124 - Avg Training Loss: 10.71369993686676 - Avg Validation Loss: 6.793830394744873\n",
      "Epoch 78/124 - Avg Training Loss: 9.911237001419067 - Avg Validation Loss: 6.953431606292725\n",
      "Epoch 79/124 - Avg Training Loss: 10.564302563667297 - Avg Validation Loss: 5.635145378112793\n",
      "Epoch 80/124 - Avg Training Loss: 10.616764187812805 - Avg Validation Loss: 5.54071159362793\n",
      "Epoch 81/124 - Avg Training Loss: 10.329018473625183 - Avg Validation Loss: 6.014875793457032\n",
      "Epoch 82/124 - Avg Training Loss: 10.430645108222961 - Avg Validation Loss: 8.080529689788818\n",
      "Epoch 83/124 - Avg Training Loss: 9.835803866386414 - Avg Validation Loss: 7.801819705963135\n",
      "Epoch 84/124 - Avg Training Loss: 9.753090739250183 - Avg Validation Loss: 8.873309516906739\n",
      "Epoch 85/124 - Avg Training Loss: 10.538114428520203 - Avg Validation Loss: 6.764179611206055\n",
      "Epoch 86/124 - Avg Training Loss: 10.416920065879822 - Avg Validation Loss: 6.936664009094239\n",
      "Epoch 87/124 - Avg Training Loss: 9.569371461868286 - Avg Validation Loss: 6.859771251678467\n",
      "Epoch 88/124 - Avg Training Loss: 9.942728877067566 - Avg Validation Loss: 6.921613502502441\n",
      "Epoch 89/124 - Avg Training Loss: 9.868330717086792 - Avg Validation Loss: 6.704800605773926\n",
      "Epoch 90/124 - Avg Training Loss: 10.049847602844238 - Avg Validation Loss: 6.589958953857422\n",
      "Epoch 91/124 - Avg Training Loss: 9.998650550842285 - Avg Validation Loss: 6.56590576171875\n",
      "Epoch 92/124 - Avg Training Loss: 9.631226658821106 - Avg Validation Loss: 7.977995300292969\n",
      "Epoch 93/124 - Avg Training Loss: 9.984489798545837 - Avg Validation Loss: 6.62688455581665\n",
      "Epoch 94/124 - Avg Training Loss: 9.470663666725159 - Avg Validation Loss: 5.73233814239502\n",
      "Epoch 95/124 - Avg Training Loss: 9.876844763755798 - Avg Validation Loss: 7.314025497436523\n",
      "Epoch 96/124 - Avg Training Loss: 9.028970003128052 - Avg Validation Loss: 6.907114124298095\n",
      "Epoch 97/124 - Avg Training Loss: 9.41214907169342 - Avg Validation Loss: 6.692075252532959\n",
      "Epoch 98/124 - Avg Training Loss: 9.582799792289734 - Avg Validation Loss: 5.067336082458496\n",
      "Epoch 99/124 - Avg Training Loss: 9.259225010871887 - Avg Validation Loss: 5.575966739654541\n",
      "Epoch 100/124 - Avg Training Loss: 9.659656286239624 - Avg Validation Loss: 5.734405612945556\n",
      "Epoch 101/124 - Avg Training Loss: 9.24480265378952 - Avg Validation Loss: 5.153981018066406\n",
      "Epoch 102/124 - Avg Training Loss: 8.945010781288147 - Avg Validation Loss: 5.3645322799682615\n",
      "Epoch 103/124 - Avg Training Loss: 8.977482557296753 - Avg Validation Loss: 7.131694984436035\n",
      "Epoch 104/124 - Avg Training Loss: 9.105712175369263 - Avg Validation Loss: 8.1519606590271\n",
      "Epoch 105/124 - Avg Training Loss: 8.56514286994934 - Avg Validation Loss: 6.225666904449463\n",
      "Epoch 106/124 - Avg Training Loss: 9.504966020584106 - Avg Validation Loss: 6.275392627716064\n",
      "Epoch 107/124 - Avg Training Loss: 9.130753636360168 - Avg Validation Loss: 6.291356563568115\n",
      "Epoch 108/124 - Avg Training Loss: 8.866657674312592 - Avg Validation Loss: 6.955115222930909\n",
      "Epoch 109/124 - Avg Training Loss: 8.77813982963562 - Avg Validation Loss: 6.774109172821045\n",
      "Epoch 110/124 - Avg Training Loss: 8.850912094116211 - Avg Validation Loss: 6.971396350860596\n",
      "Epoch 111/124 - Avg Training Loss: 9.268394470214844 - Avg Validation Loss: 7.831633281707764\n",
      "Epoch 112/124 - Avg Training Loss: 8.893427789211273 - Avg Validation Loss: 7.438286685943604\n",
      "Epoch 113/124 - Avg Training Loss: 8.91037106513977 - Avg Validation Loss: 5.4717615127563475\n",
      "Epoch 114/124 - Avg Training Loss: 8.956035137176514 - Avg Validation Loss: 6.806049442291259\n",
      "Epoch 115/124 - Avg Training Loss: 8.689636468887329 - Avg Validation Loss: 5.624907207489014\n",
      "Epoch 116/124 - Avg Training Loss: 8.628802835941315 - Avg Validation Loss: 5.623671340942383\n",
      "Epoch 117/124 - Avg Training Loss: 9.360775113105774 - Avg Validation Loss: 5.485837364196778\n",
      "Epoch 118/124 - Avg Training Loss: 8.854731559753418 - Avg Validation Loss: 5.106681823730469\n",
      "Epoch 119/124 - Avg Training Loss: 8.529260516166687 - Avg Validation Loss: 5.116282367706299\n",
      "Epoch 120/124 - Avg Training Loss: 8.676306962966919 - Avg Validation Loss: 6.028836059570312\n",
      "Epoch 121/124 - Avg Training Loss: 8.34864056110382 - Avg Validation Loss: 5.610617446899414\n",
      "Epoch 122/124 - Avg Training Loss: 8.979289531707764 - Avg Validation Loss: 5.238485050201416\n",
      "Epoch 123/124 - Avg Training Loss: 8.065927565097809 - Avg Validation Loss: 4.992489337921143\n",
      "Epoch 124/124 - Avg Training Loss: 8.564243257045746 - Avg Validation Loss: 6.446177577972412\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:17,417] Trial 18 finished with value: 6.446177577972412 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 3, 'dropout_rate': 0.4442599568626467, 'lr': 0.0045830228574292855, 'batch_size': 192, 'epochs': 124}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/63 - Avg Training Loss: 307.8861999511719 - Avg Validation Loss: 287.7580261230469\n",
      "Epoch 2/63 - Avg Training Loss: 248.97003682454428 - Avg Validation Loss: 176.92935180664062\n",
      "Epoch 3/63 - Avg Training Loss: 105.7301419576009 - Avg Validation Loss: 26.58224105834961\n",
      "Epoch 4/63 - Avg Training Loss: 87.42145919799805 - Avg Validation Loss: 23.635969161987305\n",
      "Epoch 5/63 - Avg Training Loss: 51.235190073649086 - Avg Validation Loss: 48.35581970214844\n",
      "Epoch 6/63 - Avg Training Loss: 56.03553263346354 - Avg Validation Loss: 69.48552322387695\n",
      "Epoch 7/63 - Avg Training Loss: 58.18875376383463 - Avg Validation Loss: 41.614545822143555\n",
      "Epoch 8/63 - Avg Training Loss: 37.38009770711263 - Avg Validation Loss: 13.036414623260498\n",
      "Epoch 9/63 - Avg Training Loss: 45.27411142985026 - Avg Validation Loss: 11.753300189971924\n",
      "Epoch 10/63 - Avg Training Loss: 37.506978352864586 - Avg Validation Loss: 21.374478340148926\n",
      "Epoch 11/63 - Avg Training Loss: 36.58568445841471 - Avg Validation Loss: 36.708112716674805\n",
      "Epoch 12/63 - Avg Training Loss: 39.2535285949707 - Avg Validation Loss: 28.722549438476562\n",
      "Epoch 13/63 - Avg Training Loss: 33.75560760498047 - Avg Validation Loss: 14.734072208404541\n",
      "Epoch 14/63 - Avg Training Loss: 32.78245735168457 - Avg Validation Loss: 14.115951538085938\n",
      "Epoch 15/63 - Avg Training Loss: 31.92850176493327 - Avg Validation Loss: 21.414864540100098\n",
      "Epoch 16/63 - Avg Training Loss: 30.74276606241862 - Avg Validation Loss: 23.006507873535156\n",
      "Epoch 17/63 - Avg Training Loss: 30.310675303141277 - Avg Validation Loss: 17.130024909973145\n",
      "Epoch 18/63 - Avg Training Loss: 30.004833221435547 - Avg Validation Loss: 16.65602970123291\n",
      "Epoch 19/63 - Avg Training Loss: 27.984777450561523 - Avg Validation Loss: 19.970892906188965\n",
      "Epoch 20/63 - Avg Training Loss: 28.07486406962077 - Avg Validation Loss: 18.847224235534668\n",
      "Epoch 21/63 - Avg Training Loss: 26.195981979370117 - Avg Validation Loss: 17.610111236572266\n",
      "Epoch 22/63 - Avg Training Loss: 26.623557408650715 - Avg Validation Loss: 19.810827255249023\n",
      "Epoch 23/63 - Avg Training Loss: 25.453529993693035 - Avg Validation Loss: 16.22524642944336\n",
      "Epoch 24/63 - Avg Training Loss: 24.756200154622395 - Avg Validation Loss: 16.95663547515869\n",
      "Epoch 25/63 - Avg Training Loss: 23.984066009521484 - Avg Validation Loss: 19.64347553253174\n",
      "Epoch 26/63 - Avg Training Loss: 23.824996948242188 - Avg Validation Loss: 15.467504978179932\n",
      "Epoch 27/63 - Avg Training Loss: 24.214754104614258 - Avg Validation Loss: 18.771254539489746\n",
      "Epoch 28/63 - Avg Training Loss: 24.025917053222656 - Avg Validation Loss: 16.589285850524902\n",
      "Epoch 29/63 - Avg Training Loss: 22.925812403361004 - Avg Validation Loss: 17.32437515258789\n",
      "Epoch 30/63 - Avg Training Loss: 24.08584213256836 - Avg Validation Loss: 19.496237754821777\n",
      "Epoch 31/63 - Avg Training Loss: 22.995707194010418 - Avg Validation Loss: 12.728394508361816\n",
      "Epoch 32/63 - Avg Training Loss: 24.002178192138672 - Avg Validation Loss: 20.907763481140137\n",
      "Epoch 33/63 - Avg Training Loss: 22.17462221781413 - Avg Validation Loss: 16.466055870056152\n",
      "Epoch 34/63 - Avg Training Loss: 21.683627446492512 - Avg Validation Loss: 13.347886562347412\n",
      "Epoch 35/63 - Avg Training Loss: 22.49680010477702 - Avg Validation Loss: 26.00313091278076\n",
      "Epoch 36/63 - Avg Training Loss: 22.477083841959637 - Avg Validation Loss: 11.930436134338379\n",
      "Epoch 37/63 - Avg Training Loss: 22.214921951293945 - Avg Validation Loss: 19.0766544342041\n",
      "Epoch 38/63 - Avg Training Loss: 23.251670837402344 - Avg Validation Loss: 19.94033908843994\n",
      "Epoch 39/63 - Avg Training Loss: 21.963640848795574 - Avg Validation Loss: 10.856756687164307\n",
      "Epoch 40/63 - Avg Training Loss: 22.461488087972004 - Avg Validation Loss: 28.208624839782715\n",
      "Epoch 41/63 - Avg Training Loss: 23.035154342651367 - Avg Validation Loss: 13.953915119171143\n",
      "Epoch 42/63 - Avg Training Loss: 21.914321899414062 - Avg Validation Loss: 16.269246578216553\n",
      "Epoch 43/63 - Avg Training Loss: 20.85203679402669 - Avg Validation Loss: 22.497486114501953\n",
      "Epoch 44/63 - Avg Training Loss: 20.94205665588379 - Avg Validation Loss: 13.878377437591553\n",
      "Epoch 45/63 - Avg Training Loss: 20.776229858398438 - Avg Validation Loss: 20.368200302124023\n",
      "Epoch 46/63 - Avg Training Loss: 20.359445571899414 - Avg Validation Loss: 20.509748458862305\n",
      "Epoch 47/63 - Avg Training Loss: 21.81356366475423 - Avg Validation Loss: 16.366520881652832\n",
      "Epoch 48/63 - Avg Training Loss: 20.56785265604655 - Avg Validation Loss: 19.05648708343506\n",
      "Epoch 49/63 - Avg Training Loss: 20.801618576049805 - Avg Validation Loss: 15.48198652267456\n",
      "Epoch 50/63 - Avg Training Loss: 19.179147720336914 - Avg Validation Loss: 18.571256637573242\n",
      "Epoch 51/63 - Avg Training Loss: 19.514232635498047 - Avg Validation Loss: 17.054916381835938\n",
      "Epoch 52/63 - Avg Training Loss: 20.4479554494222 - Avg Validation Loss: 16.736620903015137\n",
      "Epoch 53/63 - Avg Training Loss: 21.08978017171224 - Avg Validation Loss: 19.890612602233887\n",
      "Epoch 54/63 - Avg Training Loss: 20.568082809448242 - Avg Validation Loss: 15.710671424865723\n",
      "Epoch 55/63 - Avg Training Loss: 19.47439193725586 - Avg Validation Loss: 18.763983726501465\n",
      "Epoch 56/63 - Avg Training Loss: 19.734057744344074 - Avg Validation Loss: 16.746445655822754\n",
      "Epoch 57/63 - Avg Training Loss: 19.42380650838216 - Avg Validation Loss: 17.39455795288086\n",
      "Epoch 58/63 - Avg Training Loss: 19.83277638753255 - Avg Validation Loss: 12.828171253204346\n",
      "Epoch 59/63 - Avg Training Loss: 19.77757962544759 - Avg Validation Loss: 20.41672134399414\n",
      "Epoch 60/63 - Avg Training Loss: 20.09032440185547 - Avg Validation Loss: 15.43228530883789\n",
      "Epoch 61/63 - Avg Training Loss: 19.35377311706543 - Avg Validation Loss: 18.858787536621094\n",
      "Epoch 62/63 - Avg Training Loss: 20.34187889099121 - Avg Validation Loss: 17.35120391845703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:18,953] Trial 19 finished with value: 15.595119953155518 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 3, 'dropout_rate': 0.5607804108337902, 'lr': 0.0021512011335485426, 'batch_size': 512, 'epochs': 63}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/63 - Avg Training Loss: 19.51555633544922 - Avg Validation Loss: 15.595119953155518\n",
      "Finished Training\n",
      "Epoch 1/99 - Avg Training Loss: 267.16234596915865 - Avg Validation Loss: 55.530199595860076\n",
      "Epoch 2/99 - Avg Training Loss: 69.10513786647631 - Avg Validation Loss: 22.39053753444127\n",
      "Epoch 3/99 - Avg Training Loss: 41.05757389897885 - Avg Validation Loss: 60.60056223188128\n",
      "Epoch 4/99 - Avg Training Loss: 32.02497697913128 - Avg Validation Loss: 59.012053625924246\n",
      "Epoch 5/99 - Avg Training Loss: 24.424780057824176 - Avg Validation Loss: 81.2947017124721\n",
      "Epoch 6/99 - Avg Training Loss: 24.33080822488536 - Avg Validation Loss: 90.22450419834682\n",
      "Epoch 7/99 - Avg Training Loss: 22.37497371176015 - Avg Validation Loss: 77.97781481061664\n",
      "Epoch 8/99 - Avg Training Loss: 20.59745747110118 - Avg Validation Loss: 74.34469713483539\n",
      "Epoch 9/99 - Avg Training Loss: 19.09330699754798 - Avg Validation Loss: 51.53441946847098\n",
      "Epoch 10/99 - Avg Training Loss: 20.079171139260996 - Avg Validation Loss: 87.26655088152204\n",
      "Epoch 11/99 - Avg Training Loss: 18.43682687178902 - Avg Validation Loss: 66.59263093130929\n",
      "Epoch 12/99 - Avg Training Loss: 18.25708177815313 - Avg Validation Loss: 74.10274941580636\n",
      "Epoch 13/99 - Avg Training Loss: 18.21325501151707 - Avg Validation Loss: 55.13764871869768\n",
      "Epoch 14/99 - Avg Training Loss: 16.8615024400794 - Avg Validation Loss: 52.13576071602957\n",
      "Epoch 15/99 - Avg Training Loss: 16.341912269592285 - Avg Validation Loss: 49.033858163016184\n",
      "Epoch 16/99 - Avg Training Loss: 15.765682386315387 - Avg Validation Loss: 50.06873893737793\n",
      "Epoch 17/99 - Avg Training Loss: 14.800173427747643 - Avg Validation Loss: 48.211682183401926\n",
      "Epoch 18/99 - Avg Training Loss: 15.246785827305006 - Avg Validation Loss: 53.68524415152414\n",
      "Epoch 19/99 - Avg Training Loss: 15.102325107740318 - Avg Validation Loss: 40.1989563533238\n",
      "Epoch 20/99 - Avg Training Loss: 14.572494506835938 - Avg Validation Loss: 50.7964357648577\n",
      "Epoch 21/99 - Avg Training Loss: 14.537818950155508 - Avg Validation Loss: 36.72243281773159\n",
      "Epoch 22/99 - Avg Training Loss: 13.061929951543393 - Avg Validation Loss: 37.08870778764997\n",
      "Epoch 23/99 - Avg Training Loss: 13.140420581983483 - Avg Validation Loss: 41.10132762363979\n",
      "Epoch 24/99 - Avg Training Loss: 14.287820401399031 - Avg Validation Loss: 35.37799971444266\n",
      "Epoch 25/99 - Avg Training Loss: 14.144924371138863 - Avg Validation Loss: 35.064071791512625\n",
      "Epoch 26/99 - Avg Training Loss: 12.694381983383842 - Avg Validation Loss: 26.69392122541155\n",
      "Epoch 27/99 - Avg Training Loss: 13.353401432866635 - Avg Validation Loss: 21.22275529588972\n",
      "Epoch 28/99 - Avg Training Loss: 12.306767214899478 - Avg Validation Loss: 21.220200402396067\n",
      "Epoch 29/99 - Avg Training Loss: 13.200854177060334 - Avg Validation Loss: 33.70386627742222\n",
      "Epoch 30/99 - Avg Training Loss: 13.01429619996444 - Avg Validation Loss: 22.018204144069127\n",
      "Epoch 31/99 - Avg Training Loss: 12.863736899002738 - Avg Validation Loss: 26.007613045828684\n",
      "Epoch 32/99 - Avg Training Loss: 13.328918581423553 - Avg Validation Loss: 29.862639018467494\n",
      "Epoch 33/99 - Avg Training Loss: 12.315337139627207 - Avg Validation Loss: 15.833195277622767\n",
      "Epoch 34/99 - Avg Training Loss: 11.386303984600564 - Avg Validation Loss: 22.312426839556014\n",
      "Epoch 35/99 - Avg Training Loss: 10.939254532689633 - Avg Validation Loss: 16.085268906184606\n",
      "Epoch 36/99 - Avg Training Loss: 10.850116439487623 - Avg Validation Loss: 19.359204019818986\n",
      "Epoch 37/99 - Avg Training Loss: 10.799770230832307 - Avg Validation Loss: 19.57948589324951\n",
      "Epoch 38/99 - Avg Training Loss: 10.914277076721191 - Avg Validation Loss: 20.302226611546107\n",
      "Epoch 39/99 - Avg Training Loss: 10.71344813056614 - Avg Validation Loss: 17.006044183458602\n",
      "Epoch 40/99 - Avg Training Loss: 10.40984987175983 - Avg Validation Loss: 18.820940017700195\n",
      "Epoch 41/99 - Avg Training Loss: 10.2784947934358 - Avg Validation Loss: 16.696892738342285\n",
      "Epoch 42/99 - Avg Training Loss: 10.120589339214822 - Avg Validation Loss: 14.355264118739537\n",
      "Epoch 43/99 - Avg Training Loss: 10.75595186067664 - Avg Validation Loss: 15.650459493909564\n",
      "Epoch 44/99 - Avg Training Loss: 10.554252458655316 - Avg Validation Loss: 11.564397743770055\n",
      "Epoch 45/99 - Avg Training Loss: 9.701015576072361 - Avg Validation Loss: 13.802343504769462\n",
      "Epoch 46/99 - Avg Training Loss: 9.474186627761178 - Avg Validation Loss: 12.614735807691302\n",
      "Epoch 47/99 - Avg Training Loss: 9.375798930292545 - Avg Validation Loss: 15.59929255076817\n",
      "Epoch 48/99 - Avg Training Loss: 9.268876386725385 - Avg Validation Loss: 14.174107892172676\n",
      "Epoch 49/99 - Avg Training Loss: 9.119933978370998 - Avg Validation Loss: 12.429438522883824\n",
      "Epoch 50/99 - Avg Training Loss: 9.21680736541748 - Avg Validation Loss: 12.784621783665248\n",
      "Epoch 51/99 - Avg Training Loss: 8.348542462224545 - Avg Validation Loss: 11.814030851636614\n",
      "Epoch 52/99 - Avg Training Loss: 8.20574777022652 - Avg Validation Loss: 11.728384017944336\n",
      "Epoch 53/99 - Avg Training Loss: 8.402775743733281 - Avg Validation Loss: 9.754865101405553\n",
      "Epoch 54/99 - Avg Training Loss: 8.381621132726254 - Avg Validation Loss: 8.151283775057111\n",
      "Epoch 55/99 - Avg Training Loss: 8.27718454858531 - Avg Validation Loss: 9.930087906973702\n",
      "Epoch 56/99 - Avg Training Loss: 8.036766010781992 - Avg Validation Loss: 8.216347183500018\n",
      "Epoch 57/99 - Avg Training Loss: 7.7362426882204804 - Avg Validation Loss: 8.088495901652745\n",
      "Epoch 58/99 - Avg Training Loss: 7.96289582874464 - Avg Validation Loss: 9.804274763379778\n",
      "Epoch 59/99 - Avg Training Loss: 7.673557903455651 - Avg Validation Loss: 7.489167417798724\n",
      "Epoch 60/99 - Avg Training Loss: 7.067880734153416 - Avg Validation Loss: 7.447614295142038\n",
      "Epoch 61/99 - Avg Training Loss: 7.33469589896824 - Avg Validation Loss: 8.64242158617292\n",
      "Epoch 62/99 - Avg Training Loss: 7.475272842075514 - Avg Validation Loss: 8.003080504281181\n",
      "Epoch 63/99 - Avg Training Loss: 7.039007021033245 - Avg Validation Loss: 7.036984886441912\n",
      "Epoch 64/99 - Avg Training Loss: 6.864446038785188 - Avg Validation Loss: 5.935399770736694\n",
      "Epoch 65/99 - Avg Training Loss: 6.639481751815133 - Avg Validation Loss: 7.6568611689976285\n",
      "Epoch 66/99 - Avg Training Loss: 7.1473482588063115 - Avg Validation Loss: 7.016223703111921\n",
      "Epoch 67/99 - Avg Training Loss: 6.938968658447266 - Avg Validation Loss: 6.21389559337071\n",
      "Epoch 68/99 - Avg Training Loss: 6.7173035041145654 - Avg Validation Loss: 5.5635121549878805\n",
      "Epoch 69/99 - Avg Training Loss: 6.776032510011093 - Avg Validation Loss: 6.073896578380039\n",
      "Epoch 70/99 - Avg Training Loss: 6.191756372866423 - Avg Validation Loss: 6.489061287471226\n",
      "Epoch 71/99 - Avg Training Loss: 6.591801601907481 - Avg Validation Loss: 5.955181019646781\n",
      "Epoch 72/99 - Avg Training Loss: 6.863927758258322 - Avg Validation Loss: 5.4404593876429965\n",
      "Epoch 73/99 - Avg Training Loss: 6.314522992009702 - Avg Validation Loss: 5.441787787846157\n",
      "Epoch 74/99 - Avg Training Loss: 6.183568083721658 - Avg Validation Loss: 5.011536308697292\n",
      "Epoch 75/99 - Avg Training Loss: 6.292335655378259 - Avg Validation Loss: 4.903040664536612\n",
      "Epoch 76/99 - Avg Training Loss: 6.375355927840523 - Avg Validation Loss: 5.309557267597744\n",
      "Epoch 77/99 - Avg Training Loss: 6.086958698604418 - Avg Validation Loss: 4.603312952177865\n",
      "Epoch 78/99 - Avg Training Loss: 5.715992450714111 - Avg Validation Loss: 6.18928633417402\n",
      "Epoch 79/99 - Avg Training Loss: 5.642391826795495 - Avg Validation Loss: 4.894498518535069\n",
      "Epoch 80/99 - Avg Training Loss: 6.020719839178997 - Avg Validation Loss: 4.9631461415972025\n",
      "Epoch 81/99 - Avg Training Loss: 6.555503907410995 - Avg Validation Loss: 5.431863580431257\n",
      "Epoch 82/99 - Avg Training Loss: 6.018840623938519 - Avg Validation Loss: 4.401473420006888\n",
      "Epoch 83/99 - Avg Training Loss: 5.596584745075392 - Avg Validation Loss: 4.663151604788644\n",
      "Epoch 84/99 - Avg Training Loss: 5.880449958469557 - Avg Validation Loss: 4.491534948348999\n",
      "Epoch 85/99 - Avg Training Loss: 5.84698952799258 - Avg Validation Loss: 6.445331232888358\n",
      "Epoch 86/99 - Avg Training Loss: 5.804896002230437 - Avg Validation Loss: 4.731653843607221\n",
      "Epoch 87/99 - Avg Training Loss: 5.657840749491816 - Avg Validation Loss: 4.90803827558245\n",
      "Epoch 88/99 - Avg Training Loss: 5.911112536554751 - Avg Validation Loss: 4.648352333477566\n",
      "Epoch 89/99 - Avg Training Loss: 5.729883546414583 - Avg Validation Loss: 4.796903780528477\n",
      "Epoch 90/99 - Avg Training Loss: 5.815440799878991 - Avg Validation Loss: 4.548998083387103\n",
      "Epoch 91/99 - Avg Training Loss: 5.6332505267599355 - Avg Validation Loss: 4.373037559645517\n",
      "Epoch 92/99 - Avg Training Loss: 5.592723659847094 - Avg Validation Loss: 6.352126734597342\n",
      "Epoch 93/99 - Avg Training Loss: 5.779236409975135 - Avg Validation Loss: 5.298274176461356\n",
      "Epoch 94/99 - Avg Training Loss: 5.251956214075503 - Avg Validation Loss: 4.6155014889580865\n",
      "Epoch 95/99 - Avg Training Loss: 5.515943257705025 - Avg Validation Loss: 4.992213828223092\n",
      "Epoch 96/99 - Avg Training Loss: 5.6097036859263545 - Avg Validation Loss: 4.379529510225568\n",
      "Epoch 97/99 - Avg Training Loss: 5.325915699419768 - Avg Validation Loss: 4.40239155292511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:33,926] Trial 20 finished with value: 4.352297851017544 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 4, 'dropout_rate': 0.523051421531414, 'lr': 0.0031098467310723766, 'batch_size': 64, 'epochs': 99}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99 - Avg Training Loss: 5.677209978518278 - Avg Validation Loss: 5.306005954742432\n",
      "Epoch 99/99 - Avg Training Loss: 5.340942196224047 - Avg Validation Loss: 4.352297851017544\n",
      "Finished Training\n",
      "Epoch 1/51 - Avg Training Loss: 112.60887615497296 - Avg Validation Loss: 68.87887477874756\n",
      "Epoch 2/51 - Avg Training Loss: 34.19759134145883 - Avg Validation Loss: 11.881340265274048\n",
      "Epoch 3/51 - Avg Training Loss: 19.82445995624249 - Avg Validation Loss: 10.314114809036255\n",
      "Epoch 4/51 - Avg Training Loss: 17.20700036562406 - Avg Validation Loss: 9.394330501556396\n",
      "Epoch 5/51 - Avg Training Loss: 14.156483283409706 - Avg Validation Loss: 10.43938398361206\n",
      "Epoch 6/51 - Avg Training Loss: 13.191118387075571 - Avg Validation Loss: 12.027384996414185\n",
      "Epoch 7/51 - Avg Training Loss: 13.489258986253004 - Avg Validation Loss: 9.200011253356934\n",
      "Epoch 8/51 - Avg Training Loss: 11.371421667245718 - Avg Validation Loss: 6.897553563117981\n",
      "Epoch 9/51 - Avg Training Loss: 11.394191448505108 - Avg Validation Loss: 8.32583749294281\n",
      "Epoch 10/51 - Avg Training Loss: 10.633628625136156 - Avg Validation Loss: 8.602514028549194\n",
      "Epoch 11/51 - Avg Training Loss: 11.928974078251766 - Avg Validation Loss: 4.2910497188568115\n",
      "Epoch 12/51 - Avg Training Loss: 10.977193465599647 - Avg Validation Loss: 6.637605726718903\n",
      "Epoch 13/51 - Avg Training Loss: 10.838545432457558 - Avg Validation Loss: 4.930548548698425\n",
      "Epoch 14/51 - Avg Training Loss: 10.433977053715633 - Avg Validation Loss: 9.962017178535461\n",
      "Epoch 15/51 - Avg Training Loss: 10.358476602114164 - Avg Validation Loss: 5.98736047744751\n",
      "Epoch 16/51 - Avg Training Loss: 9.919491327725924 - Avg Validation Loss: 5.393715023994446\n",
      "Epoch 17/51 - Avg Training Loss: 10.36414329822247 - Avg Validation Loss: 6.788792073726654\n",
      "Epoch 18/51 - Avg Training Loss: 10.536582506619967 - Avg Validation Loss: 4.965278387069702\n",
      "Epoch 19/51 - Avg Training Loss: 11.44299815251277 - Avg Validation Loss: 8.890530109405518\n",
      "Epoch 20/51 - Avg Training Loss: 11.009374178372896 - Avg Validation Loss: 11.83144474029541\n",
      "Epoch 21/51 - Avg Training Loss: 10.683874863844652 - Avg Validation Loss: 5.624853849411011\n",
      "Epoch 22/51 - Avg Training Loss: 10.129469504723183 - Avg Validation Loss: 11.919861316680908\n",
      "Epoch 23/51 - Avg Training Loss: 10.963843125563402 - Avg Validation Loss: 8.757915019989014\n",
      "Epoch 24/51 - Avg Training Loss: 11.14144728733943 - Avg Validation Loss: 4.996333539485931\n",
      "Epoch 25/51 - Avg Training Loss: 11.105670048640324 - Avg Validation Loss: 4.460789084434509\n",
      "Epoch 26/51 - Avg Training Loss: 10.716485426976131 - Avg Validation Loss: 7.542704641819\n",
      "Epoch 27/51 - Avg Training Loss: 10.414545646080605 - Avg Validation Loss: 7.855021178722382\n",
      "Epoch 28/51 - Avg Training Loss: 10.410178477947529 - Avg Validation Loss: 4.252429008483887\n",
      "Epoch 29/51 - Avg Training Loss: 10.11525638286884 - Avg Validation Loss: 6.705141484737396\n",
      "Epoch 30/51 - Avg Training Loss: 10.169226279625526 - Avg Validation Loss: 8.535752654075623\n",
      "Epoch 31/51 - Avg Training Loss: 9.593238317049467 - Avg Validation Loss: 4.383522719144821\n",
      "Epoch 32/51 - Avg Training Loss: 9.777265915503868 - Avg Validation Loss: 8.772608578205109\n",
      "Epoch 33/51 - Avg Training Loss: 10.32391749895536 - Avg Validation Loss: 7.130286931991577\n",
      "Epoch 34/51 - Avg Training Loss: 10.44822806578416 - Avg Validation Loss: 4.89995151758194\n",
      "Epoch 35/51 - Avg Training Loss: 10.386260399451622 - Avg Validation Loss: 4.37479305267334\n",
      "Epoch 36/51 - Avg Training Loss: 10.655305715707632 - Avg Validation Loss: 5.7633620500564575\n",
      "Epoch 37/51 - Avg Training Loss: 9.91003997509296 - Avg Validation Loss: 6.796032190322876\n",
      "Epoch 38/51 - Avg Training Loss: 10.19145624454205 - Avg Validation Loss: 5.485593140125275\n",
      "Epoch 39/51 - Avg Training Loss: 11.128363682673527 - Avg Validation Loss: 4.798264712095261\n",
      "Epoch 40/51 - Avg Training Loss: 10.843093505272499 - Avg Validation Loss: 3.864608883857727\n",
      "Epoch 41/51 - Avg Training Loss: 11.303165252392109 - Avg Validation Loss: 5.745686054229736\n",
      "Epoch 42/51 - Avg Training Loss: 9.862603994516226 - Avg Validation Loss: 14.140636086463928\n",
      "Epoch 43/51 - Avg Training Loss: 10.368636718163124 - Avg Validation Loss: 6.936878144741058\n",
      "Epoch 44/51 - Avg Training Loss: 10.245838091923641 - Avg Validation Loss: 12.510618209838867\n",
      "Epoch 45/51 - Avg Training Loss: 13.614361506242018 - Avg Validation Loss: 6.1465150117874146\n",
      "Epoch 46/51 - Avg Training Loss: 11.192291333125187 - Avg Validation Loss: 4.438021600246429\n",
      "Epoch 47/51 - Avg Training Loss: 10.695959311265211 - Avg Validation Loss: 4.623226344585419\n",
      "Epoch 48/51 - Avg Training Loss: 9.834433665642372 - Avg Validation Loss: 5.0711701810359955\n",
      "Epoch 49/51 - Avg Training Loss: 9.72285116635836 - Avg Validation Loss: 5.340256929397583\n",
      "Epoch 50/51 - Avg Training Loss: 9.490869302016039 - Avg Validation Loss: 6.394553065299988\n",
      "Epoch 51/51 - Avg Training Loss: 9.275442050053524 - Avg Validation Loss: 5.587775528430939\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:38,559] Trial 21 finished with value: 5.587775528430939 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5214911356979214, 'lr': 0.004035845098646725, 'batch_size': 112, 'epochs': 51}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/62 - Avg Training Loss: 80.23353830973308 - Avg Validation Loss: 29.722693529996004\n",
      "Epoch 2/62 - Avg Training Loss: 17.58454836739434 - Avg Validation Loss: 10.23419527574019\n",
      "Epoch 3/62 - Avg Training Loss: 12.68885448243883 - Avg Validation Loss: 12.175943461331455\n",
      "Epoch 4/62 - Avg Training Loss: 12.247080749935574 - Avg Validation Loss: 8.457788857546719\n",
      "Epoch 5/62 - Avg Training Loss: 10.551225768195259 - Avg Validation Loss: 5.258712248368696\n",
      "Epoch 6/62 - Avg Training Loss: 10.370900789896647 - Avg Validation Loss: 8.116713393818248\n",
      "Epoch 7/62 - Avg Training Loss: 9.247599681218466 - Avg Validation Loss: 4.903784881938588\n",
      "Epoch 8/62 - Avg Training Loss: 8.790878507826063 - Avg Validation Loss: 10.68524187261408\n",
      "Epoch 9/62 - Avg Training Loss: 9.050851239098442 - Avg Validation Loss: 7.463035453449596\n",
      "Epoch 10/62 - Avg Training Loss: 8.448039346271091 - Avg Validation Loss: 4.067459626631304\n",
      "Epoch 11/62 - Avg Training Loss: 9.395607868830362 - Avg Validation Loss: 7.053785757585005\n",
      "Epoch 12/62 - Avg Training Loss: 10.196432802412245 - Avg Validation Loss: 4.529891620982777\n",
      "Epoch 13/62 - Avg Training Loss: 9.673010243309868 - Avg Validation Loss: 10.44064660505815\n",
      "Epoch 14/62 - Avg Training Loss: 8.627114958233303 - Avg Validation Loss: 6.224459691481157\n",
      "Epoch 15/62 - Avg Training Loss: 8.294313563240898 - Avg Validation Loss: 4.111441287127408\n",
      "Epoch 16/62 - Avg Training Loss: 8.651072873009575 - Avg Validation Loss: 5.214041883295232\n",
      "Epoch 17/62 - Avg Training Loss: 8.08891577190823 - Avg Validation Loss: 6.276557142084295\n",
      "Epoch 18/62 - Avg Training Loss: 7.8707571559482155 - Avg Validation Loss: 5.7378763285550205\n",
      "Epoch 19/62 - Avg Training Loss: 8.20967960357666 - Avg Validation Loss: 3.879994349046187\n",
      "Epoch 20/62 - Avg Training Loss: 8.216447061962551 - Avg Validation Loss: 8.02601636539806\n",
      "Epoch 21/62 - Avg Training Loss: 8.42527084880405 - Avg Validation Loss: 4.478067853234031\n",
      "Epoch 22/62 - Avg Training Loss: 8.219778590732151 - Avg Validation Loss: 6.6530913006175645\n",
      "Epoch 23/62 - Avg Training Loss: 8.068143182330662 - Avg Validation Loss: 4.33960899439725\n",
      "Epoch 24/62 - Avg Training Loss: 8.70103512869941 - Avg Validation Loss: 4.473668965426358\n",
      "Epoch 25/62 - Avg Training Loss: 8.148000001907349 - Avg Validation Loss: 8.916018052534623\n",
      "Epoch 26/62 - Avg Training Loss: 8.348456568188137 - Avg Validation Loss: 9.298174424604936\n",
      "Epoch 27/62 - Avg Training Loss: 9.159646272659302 - Avg Validation Loss: 4.671386003494263\n",
      "Epoch 28/62 - Avg Training Loss: 7.793196651670668 - Avg Validation Loss: 10.40114012631503\n",
      "Epoch 29/62 - Avg Training Loss: 8.656746546427408 - Avg Validation Loss: 4.028729503804987\n",
      "Epoch 30/62 - Avg Training Loss: 8.748658736546835 - Avg Validation Loss: 4.739068226380781\n",
      "Epoch 31/62 - Avg Training Loss: 7.821445994906956 - Avg Validation Loss: 6.4338202909989795\n",
      "Epoch 32/62 - Avg Training Loss: 7.586499134699504 - Avg Validation Loss: 5.004525618119673\n",
      "Epoch 33/62 - Avg Training Loss: 8.30410631497701 - Avg Validation Loss: 6.469096357172186\n",
      "Epoch 34/62 - Avg Training Loss: 8.14269471168518 - Avg Validation Loss: 11.77434609153054\n",
      "Epoch 35/62 - Avg Training Loss: 9.642075379689535 - Avg Validation Loss: 4.141014207493175\n",
      "Epoch 36/62 - Avg Training Loss: 8.20057119263543 - Avg Validation Loss: 8.431561079892246\n",
      "Epoch 37/62 - Avg Training Loss: 8.471183723873562 - Avg Validation Loss: 11.13397147438743\n",
      "Epoch 38/62 - Avg Training Loss: 8.721510330835978 - Avg Validation Loss: 12.353338501670144\n",
      "Epoch 39/62 - Avg Training Loss: 8.17597132258945 - Avg Validation Loss: 4.7090913165699355\n",
      "Epoch 40/62 - Avg Training Loss: 7.487687190373738 - Avg Validation Loss: 6.840517520904541\n",
      "Epoch 41/62 - Avg Training Loss: 7.324142985873753 - Avg Validation Loss: 8.502690315246582\n",
      "Epoch 42/62 - Avg Training Loss: 7.167208909988403 - Avg Validation Loss: 4.340824322267012\n",
      "Epoch 43/62 - Avg Training Loss: 7.767716195848253 - Avg Validation Loss: 14.244182586669922\n",
      "Epoch 44/62 - Avg Training Loss: 9.42254090309143 - Avg Validation Loss: 6.648178620771929\n",
      "Epoch 45/62 - Avg Training Loss: 7.664383305443658 - Avg Validation Loss: 4.639066479422829\n",
      "Epoch 46/62 - Avg Training Loss: 8.015601608488295 - Avg Validation Loss: 5.0914765271273525\n",
      "Epoch 47/62 - Avg Training Loss: 7.800366852018568 - Avg Validation Loss: 13.851529641584916\n",
      "Epoch 48/62 - Avg Training Loss: 7.462463882234362 - Avg Validation Loss: 10.207448265769266\n",
      "Epoch 49/62 - Avg Training Loss: 7.738483905792236 - Avg Validation Loss: 7.48889311877164\n",
      "Epoch 50/62 - Avg Training Loss: 6.911694765090942 - Avg Validation Loss: 5.37228553945368\n",
      "Epoch 51/62 - Avg Training Loss: 6.8455301655663385 - Avg Validation Loss: 6.033555767752907\n",
      "Epoch 52/62 - Avg Training Loss: 7.139544328053792 - Avg Validation Loss: 4.803274761546742\n",
      "Epoch 53/62 - Avg Training Loss: 7.09980222913954 - Avg Validation Loss: 6.7981696128845215\n",
      "Epoch 54/62 - Avg Training Loss: 7.943678538004558 - Avg Validation Loss: 3.9257125854492188\n",
      "Epoch 55/62 - Avg Training Loss: 10.354070451524523 - Avg Validation Loss: 17.333093209700152\n",
      "Epoch 56/62 - Avg Training Loss: 7.967224412494236 - Avg Validation Loss: 5.375573028217662\n",
      "Epoch 57/62 - Avg Training Loss: 7.096184465620253 - Avg Validation Loss: 5.8287107294256035\n",
      "Epoch 58/62 - Avg Training Loss: 6.753690825568305 - Avg Validation Loss: 9.024152322248979\n",
      "Epoch 59/62 - Avg Training Loss: 7.090532938639323 - Avg Validation Loss: 9.558323946866123\n",
      "Epoch 60/62 - Avg Training Loss: 7.009627315733168 - Avg Validation Loss: 9.4373797936873\n",
      "Epoch 61/62 - Avg Training Loss: 6.402523570590549 - Avg Validation Loss: 8.562358899550004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:44,896] Trial 22 finished with value: 7.677700042724609 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.49436932369201586, 'lr': 0.001841535308297566, 'batch_size': 80, 'epochs': 62}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/62 - Avg Training Loss: 7.173202117284139 - Avg Validation Loss: 7.677700042724609\n",
      "Finished Training\n",
      "Epoch 1/50 - Avg Training Loss: 35.346919706132674 - Avg Validation Loss: 13.379661895610669\n",
      "Epoch 2/50 - Avg Training Loss: 19.8752182536655 - Avg Validation Loss: 9.452969780674687\n",
      "Epoch 3/50 - Avg Training Loss: 22.33807439804077 - Avg Validation Loss: 7.908343447579278\n",
      "Epoch 4/50 - Avg Training Loss: 21.765630298190647 - Avg Validation Loss: 18.71998993555705\n",
      "Epoch 5/50 - Avg Training Loss: 24.626777818467882 - Avg Validation Loss: 19.652851316663956\n",
      "Epoch 6/50 - Avg Training Loss: 20.15246073934767 - Avg Validation Loss: 14.77272891998291\n",
      "Epoch 7/50 - Avg Training Loss: 20.113230228424072 - Avg Validation Loss: 5.211065601419519\n",
      "Epoch 8/50 - Avg Training Loss: 22.568176894717745 - Avg Validation Loss: 18.194858498043484\n",
      "Epoch 9/50 - Avg Training Loss: 20.759234629737005 - Avg Validation Loss: 13.216746021200109\n",
      "Epoch 10/50 - Avg Training Loss: 22.028413751390246 - Avg Validation Loss: 9.17644308231495\n",
      "Epoch 11/50 - Avg Training Loss: 21.000228362613253 - Avg Validation Loss: 15.085142365208378\n",
      "Epoch 12/50 - Avg Training Loss: 24.480512852138943 - Avg Validation Loss: 24.012933766400373\n",
      "Epoch 13/50 - Avg Training Loss: 17.820196522606743 - Avg Validation Loss: 21.537066406673855\n",
      "Epoch 14/50 - Avg Training Loss: 15.56477460861206 - Avg Validation Loss: 12.193363260339808\n",
      "Epoch 15/50 - Avg Training Loss: 13.530932908587985 - Avg Validation Loss: 9.067465543746948\n",
      "Epoch 16/50 - Avg Training Loss: 12.196020687950982 - Avg Validation Loss: 7.054650200737847\n",
      "Epoch 17/50 - Avg Training Loss: 12.074822330474854 - Avg Validation Loss: 4.5585498191692215\n",
      "Epoch 18/50 - Avg Training Loss: 9.865092706680297 - Avg Validation Loss: 5.680316942709464\n",
      "Epoch 19/50 - Avg Training Loss: 10.030209991667006 - Avg Validation Loss: 6.612409053025423\n",
      "Epoch 20/50 - Avg Training Loss: 9.755243492126464 - Avg Validation Loss: 5.753858398508142\n",
      "Epoch 21/50 - Avg Training Loss: 9.827530935075547 - Avg Validation Loss: 6.6826363404591875\n",
      "Epoch 22/50 - Avg Training Loss: 10.155885161293877 - Avg Validation Loss: 5.63122084847203\n",
      "Epoch 23/50 - Avg Training Loss: 11.115724685457018 - Avg Validation Loss: 4.902451568179661\n",
      "Epoch 24/50 - Avg Training Loss: 10.342981889512803 - Avg Validation Loss: 7.356200580243711\n",
      "Epoch 25/50 - Avg Training Loss: 9.29836401409573 - Avg Validation Loss: 5.917795931851423\n",
      "Epoch 26/50 - Avg Training Loss: 9.425293466779921 - Avg Validation Loss: 5.963244601532265\n",
      "Epoch 27/50 - Avg Training Loss: 9.035500020451016 - Avg Validation Loss: 5.949855521873191\n",
      "Epoch 28/50 - Avg Training Loss: 10.05823114183214 - Avg Validation Loss: 5.098358039502744\n",
      "Epoch 29/50 - Avg Training Loss: 11.011521916919284 - Avg Validation Loss: 4.6216452828160035\n",
      "Epoch 30/50 - Avg Training Loss: 9.406166214413114 - Avg Validation Loss: 5.2253592455828635\n",
      "Epoch 31/50 - Avg Training Loss: 9.063235145144992 - Avg Validation Loss: 5.5558048001042115\n",
      "Epoch 32/50 - Avg Training Loss: 8.886510353618197 - Avg Validation Loss: 4.733471693816008\n",
      "Epoch 33/50 - Avg Training Loss: 9.3003744814131 - Avg Validation Loss: 5.401079054231997\n",
      "Epoch 34/50 - Avg Training Loss: 9.18039451705085 - Avg Validation Loss: 5.017743092996103\n",
      "Epoch 35/50 - Avg Training Loss: 9.148713313208686 - Avg Validation Loss: 6.844947391086155\n",
      "Epoch 36/50 - Avg Training Loss: 8.546941184997559 - Avg Validation Loss: 4.723940632961415\n",
      "Epoch 37/50 - Avg Training Loss: 8.863454071680705 - Avg Validation Loss: 5.078890314808598\n",
      "Epoch 38/50 - Avg Training Loss: 8.614538945092095 - Avg Validation Loss: 8.592782241326791\n",
      "Epoch 39/50 - Avg Training Loss: 8.566537131203546 - Avg Validation Loss: 5.594924577960262\n",
      "Epoch 40/50 - Avg Training Loss: 8.262831878662109 - Avg Validation Loss: 8.32323326004876\n",
      "Epoch 41/50 - Avg Training Loss: 8.24331193500095 - Avg Validation Loss: 5.792202274004619\n",
      "Epoch 42/50 - Avg Training Loss: 7.90749888420105 - Avg Validation Loss: 4.196130690751253\n",
      "Epoch 43/50 - Avg Training Loss: 7.484215659565396 - Avg Validation Loss: 8.735992219713\n",
      "Epoch 44/50 - Avg Training Loss: 7.846284998787774 - Avg Validation Loss: 8.378676626417372\n",
      "Epoch 45/50 - Avg Training Loss: 7.111724291907416 - Avg Validation Loss: 6.21015101450461\n",
      "Epoch 46/50 - Avg Training Loss: 7.501040612326728 - Avg Validation Loss: 6.16420778521785\n",
      "Epoch 47/50 - Avg Training Loss: 8.236689768897163 - Avg Validation Loss: 5.795507241178442\n",
      "Epoch 48/50 - Avg Training Loss: 7.829460345374214 - Avg Validation Loss: 6.878725206410444\n",
      "Epoch 49/50 - Avg Training Loss: 7.542333316802979 - Avg Validation Loss: 6.2352706414681895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:46:59,087] Trial 23 finished with value: 6.024723189848441 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5055455528319611, 'lr': 0.0032376231749212565, 'batch_size': 16, 'epochs': 50}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Avg Training Loss: 7.73105460802714 - Avg Validation Loss: 6.024723189848441\n",
      "Finished Training\n",
      "Epoch 1/77 - Avg Training Loss: 66.13769619281476 - Avg Validation Loss: 14.404905438423157\n",
      "Epoch 2/77 - Avg Training Loss: 20.29773858877329 - Avg Validation Loss: 23.556578874588013\n",
      "Epoch 3/77 - Avg Training Loss: 16.19034591087928 - Avg Validation Loss: 5.810279846191406\n",
      "Epoch 4/77 - Avg Training Loss: 16.30988568526048 - Avg Validation Loss: 14.23047137260437\n",
      "Epoch 5/77 - Avg Training Loss: 15.566931284390963 - Avg Validation Loss: 8.812962651252747\n",
      "Epoch 6/77 - Avg Training Loss: 13.124860396751991 - Avg Validation Loss: 4.590393573045731\n",
      "Epoch 7/77 - Avg Training Loss: 12.986891526442308 - Avg Validation Loss: 5.29109525680542\n",
      "Epoch 8/77 - Avg Training Loss: 13.495770307687613 - Avg Validation Loss: 4.602854669094086\n",
      "Epoch 9/77 - Avg Training Loss: 13.336853687579815 - Avg Validation Loss: 5.899704039096832\n",
      "Epoch 10/77 - Avg Training Loss: 11.99879477574275 - Avg Validation Loss: 12.390059471130371\n",
      "Epoch 11/77 - Avg Training Loss: 14.800446290236254 - Avg Validation Loss: 20.55491280555725\n",
      "Epoch 12/77 - Avg Training Loss: 13.925011708186222 - Avg Validation Loss: 8.327999889850616\n",
      "Epoch 13/77 - Avg Training Loss: 13.494116563063402 - Avg Validation Loss: 7.868590414524078\n",
      "Epoch 14/77 - Avg Training Loss: 13.145716373737041 - Avg Validation Loss: 6.611816763877869\n",
      "Epoch 15/77 - Avg Training Loss: 13.100608752324032 - Avg Validation Loss: 5.691743552684784\n",
      "Epoch 16/77 - Avg Training Loss: 12.057164925795336 - Avg Validation Loss: 8.18318223953247\n",
      "Epoch 17/77 - Avg Training Loss: 12.468239784240723 - Avg Validation Loss: 4.534053087234497\n",
      "Epoch 18/77 - Avg Training Loss: 13.05840829702524 - Avg Validation Loss: 4.49920779466629\n",
      "Epoch 19/77 - Avg Training Loss: 12.217825155991774 - Avg Validation Loss: 4.6268473863601685\n",
      "Epoch 20/77 - Avg Training Loss: 12.315674048203688 - Avg Validation Loss: 9.9400315284729\n",
      "Epoch 21/77 - Avg Training Loss: 10.711862564086914 - Avg Validation Loss: 10.981603384017944\n",
      "Epoch 22/77 - Avg Training Loss: 10.602618107428917 - Avg Validation Loss: 11.460567712783813\n",
      "Epoch 23/77 - Avg Training Loss: 11.219990290128267 - Avg Validation Loss: 5.452394962310791\n",
      "Epoch 24/77 - Avg Training Loss: 12.704377761253944 - Avg Validation Loss: 6.194413542747498\n",
      "Epoch 25/77 - Avg Training Loss: 11.36526034428523 - Avg Validation Loss: 3.877192199230194\n",
      "Epoch 26/77 - Avg Training Loss: 15.27646094102126 - Avg Validation Loss: 6.861066937446594\n",
      "Epoch 27/77 - Avg Training Loss: 15.919017645028921 - Avg Validation Loss: 10.643527150154114\n",
      "Epoch 28/77 - Avg Training Loss: 15.586755459125225 - Avg Validation Loss: 4.282406896352768\n",
      "Epoch 29/77 - Avg Training Loss: 16.814744215745193 - Avg Validation Loss: 3.9283875226974487\n",
      "Epoch 30/77 - Avg Training Loss: 12.864898681640625 - Avg Validation Loss: 14.822169303894043\n",
      "Epoch 31/77 - Avg Training Loss: 11.336791111872746 - Avg Validation Loss: 12.475587487220764\n",
      "Epoch 32/77 - Avg Training Loss: 12.344956984886757 - Avg Validation Loss: 21.59504461288452\n",
      "Epoch 33/77 - Avg Training Loss: 12.229633478017954 - Avg Validation Loss: 6.042589068412781\n",
      "Epoch 34/77 - Avg Training Loss: 11.297052823580229 - Avg Validation Loss: 4.390334367752075\n",
      "Epoch 35/77 - Avg Training Loss: 10.901695618262657 - Avg Validation Loss: 7.633015215396881\n",
      "Epoch 36/77 - Avg Training Loss: 10.661205841944767 - Avg Validation Loss: 6.166116297245026\n",
      "Epoch 37/77 - Avg Training Loss: 11.6082181930542 - Avg Validation Loss: 11.91516101360321\n",
      "Epoch 38/77 - Avg Training Loss: 10.362614888411303 - Avg Validation Loss: 11.039304971694946\n",
      "Epoch 39/77 - Avg Training Loss: 9.693264484405518 - Avg Validation Loss: 4.5512195229530334\n",
      "Epoch 40/77 - Avg Training Loss: 9.584206067598783 - Avg Validation Loss: 12.65341591835022\n",
      "Epoch 41/77 - Avg Training Loss: 9.973507807804989 - Avg Validation Loss: 8.208918154239655\n",
      "Epoch 42/77 - Avg Training Loss: 11.391813241518461 - Avg Validation Loss: 6.454145431518555\n",
      "Epoch 43/77 - Avg Training Loss: 11.591775307288536 - Avg Validation Loss: 11.648227572441101\n",
      "Epoch 44/77 - Avg Training Loss: 10.413652566763071 - Avg Validation Loss: 12.683585405349731\n",
      "Epoch 45/77 - Avg Training Loss: 9.760467309218187 - Avg Validation Loss: 8.790810763835907\n",
      "Epoch 46/77 - Avg Training Loss: 9.603551864624023 - Avg Validation Loss: 7.934276700019836\n",
      "Epoch 47/77 - Avg Training Loss: 8.734647714174711 - Avg Validation Loss: 9.397887706756592\n",
      "Epoch 48/77 - Avg Training Loss: 8.936024005596455 - Avg Validation Loss: 4.029448479413986\n",
      "Epoch 49/77 - Avg Training Loss: 9.454741111168495 - Avg Validation Loss: 10.188180446624756\n",
      "Epoch 50/77 - Avg Training Loss: 9.523292578183687 - Avg Validation Loss: 11.058032274246216\n",
      "Epoch 51/77 - Avg Training Loss: 8.72767320046058 - Avg Validation Loss: 4.029510408639908\n",
      "Epoch 52/77 - Avg Training Loss: 9.853248156034029 - Avg Validation Loss: 4.724435120820999\n",
      "Epoch 53/77 - Avg Training Loss: 8.467897488520695 - Avg Validation Loss: 8.704611957073212\n",
      "Epoch 54/77 - Avg Training Loss: 7.945769309997559 - Avg Validation Loss: 4.230887085199356\n",
      "Epoch 55/77 - Avg Training Loss: 8.461521185361422 - Avg Validation Loss: 5.92924565076828\n",
      "Epoch 56/77 - Avg Training Loss: 7.267847391275259 - Avg Validation Loss: 5.354912519454956\n",
      "Epoch 57/77 - Avg Training Loss: 7.026894312638503 - Avg Validation Loss: 5.077838838100433\n",
      "Epoch 58/77 - Avg Training Loss: 6.771975627312293 - Avg Validation Loss: 5.6379199624061584\n",
      "Epoch 59/77 - Avg Training Loss: 6.942455731905424 - Avg Validation Loss: 3.9998835921287537\n",
      "Epoch 60/77 - Avg Training Loss: 7.767449489006629 - Avg Validation Loss: 4.924144804477692\n",
      "Epoch 61/77 - Avg Training Loss: 7.095727407015287 - Avg Validation Loss: 4.65339720249176\n",
      "Epoch 62/77 - Avg Training Loss: 7.237956670614389 - Avg Validation Loss: 5.021331816911697\n",
      "Epoch 63/77 - Avg Training Loss: 6.939585245572603 - Avg Validation Loss: 4.3742353320121765\n",
      "Epoch 64/77 - Avg Training Loss: 7.168688700749324 - Avg Validation Loss: 4.033008247613907\n",
      "Epoch 65/77 - Avg Training Loss: 7.113525390625 - Avg Validation Loss: 6.171780824661255\n",
      "Epoch 66/77 - Avg Training Loss: 6.99685760644766 - Avg Validation Loss: 5.6518189907073975\n",
      "Epoch 67/77 - Avg Training Loss: 6.8539509773254395 - Avg Validation Loss: 4.667367696762085\n",
      "Epoch 68/77 - Avg Training Loss: 6.475380310645471 - Avg Validation Loss: 5.140851318836212\n",
      "Epoch 69/77 - Avg Training Loss: 6.847326352046086 - Avg Validation Loss: 5.080246150493622\n",
      "Epoch 70/77 - Avg Training Loss: 6.703129841731145 - Avg Validation Loss: 4.112108677625656\n",
      "Epoch 71/77 - Avg Training Loss: 6.80649812404926 - Avg Validation Loss: 3.749076634645462\n",
      "Epoch 72/77 - Avg Training Loss: 6.815392347482534 - Avg Validation Loss: 4.599526584148407\n",
      "Epoch 73/77 - Avg Training Loss: 7.331015586853027 - Avg Validation Loss: 3.754015952348709\n",
      "Epoch 74/77 - Avg Training Loss: 7.793548767383282 - Avg Validation Loss: 4.082176983356476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:47:06,101] Trial 24 finished with value: 4.146549701690674 and parameters: {'hidden_layer_size': 512, 'hidden_layer_count': 3, 'dropout_rate': 0.5309210545246215, 'lr': 0.0043503287722308015, 'batch_size': 112, 'epochs': 77}. Best is trial 14 with value: 3.9730366865793862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/77 - Avg Training Loss: 6.97603284395658 - Avg Validation Loss: 5.4356513023376465\n",
      "Epoch 76/77 - Avg Training Loss: 6.912521142225999 - Avg Validation Loss: 4.7036696672439575\n",
      "Epoch 77/77 - Avg Training Loss: 6.758507691896879 - Avg Validation Loss: 4.146549701690674\n",
      "Finished Training\n",
      "====================================\n",
      "Number of finished trials: 25\n",
      "Best trial:\n",
      "     Value:  3.9730366865793862\n",
      "     Params: \n",
      "         hidden_layer_size: 128\n",
      "         hidden_layer_count: 3\n",
      "         dropout_rate: 0.5077424177283034\n",
      "         lr: 0.00213362462464209\n",
      "         batch_size: 16\n",
      "         epochs: 65\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_gridsearch(trial):\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # The line below does not work due to a optuna limitation. It is kept here for reference.\n",
    "    #! hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [ (v,) * i for v in [700, 512, 256, 128] for i in range(2, 5)])\n",
    "    \n",
    "    hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLPfull(hidden_layer_sizes, dropout_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #TODO: Check other optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss = train_MLP_full(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(MLP_full_gridsearch, n_trials=25)\n",
    "\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "else: print('Skipping MLP full grid search')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
