{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "\n",
    "df_train_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_train_x = df_train_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_train_y = df_train_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_train_full:', df_train_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test_full: (1440, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01'] # 01 Corresponds to the same locations as the training set\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_val_full: (864, 623)\n"
     ]
    }
   ],
   "source": [
    "months = ['01']\n",
    "sets = ['02', '03', '04']\n",
    "type = DatasetType.VALIDATION\n",
    "\n",
    "df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-Input Networks\n",
    "These networks take the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_MLP_FULL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPfull(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate):\n",
    "        super(MLPfull, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = 620\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_full(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "            loss.backward() # Perform backpropagation\n",
    "            optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return val_loss/len(val_loader) # Return the average validation loss for final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:22:41,314] A new study created in memory with name: no-name-97fb9791-0ab2-4f23-bd3b-0468d771ec42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP full grid search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700, 700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512, 512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256, 256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128, 128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.4, 0.6)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  lr = trial.suggest_uniform('lr', 0.001, 0.01)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:8: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  batch_size = trial.suggest_int('batch_size', 16, 512, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/79 - Avg Training Loss: 288.79415639241535 - Avg Validation Loss: 310.5250778198242\n",
      "Epoch 2/79 - Avg Training Loss: 245.97005971272787 - Avg Validation Loss: 33.83265781402588\n",
      "Epoch 3/79 - Avg Training Loss: 101.92550913492839 - Avg Validation Loss: 86.13125991821289\n",
      "Epoch 4/79 - Avg Training Loss: 60.142189025878906 - Avg Validation Loss: 63.84167766571045\n",
      "Epoch 5/79 - Avg Training Loss: 42.44377009073893 - Avg Validation Loss: 53.26276683807373\n",
      "Epoch 6/79 - Avg Training Loss: 26.959866523742676 - Avg Validation Loss: 65.34729862213135\n",
      "Epoch 7/79 - Avg Training Loss: 22.73373826344808 - Avg Validation Loss: 92.9996395111084\n",
      "Epoch 8/79 - Avg Training Loss: 20.213278770446777 - Avg Validation Loss: 78.52860641479492\n",
      "Epoch 9/79 - Avg Training Loss: 17.391140302022297 - Avg Validation Loss: 68.05258560180664\n",
      "Epoch 10/79 - Avg Training Loss: 16.101023991902668 - Avg Validation Loss: 68.00931358337402\n",
      "Epoch 11/79 - Avg Training Loss: 15.865698337554932 - Avg Validation Loss: 62.417181968688965\n",
      "Epoch 12/79 - Avg Training Loss: 14.783267339070639 - Avg Validation Loss: 50.132819175720215\n",
      "Epoch 13/79 - Avg Training Loss: 15.004708290100098 - Avg Validation Loss: 50.497304916381836\n",
      "Epoch 14/79 - Avg Training Loss: 13.761113802591959 - Avg Validation Loss: 45.32426738739014\n",
      "Epoch 15/79 - Avg Training Loss: 13.748660882314047 - Avg Validation Loss: 40.39013671875\n",
      "Epoch 16/79 - Avg Training Loss: 14.221889813741049 - Avg Validation Loss: 48.561509132385254\n",
      "Epoch 17/79 - Avg Training Loss: 13.687326272328695 - Avg Validation Loss: 47.44320297241211\n",
      "Epoch 18/79 - Avg Training Loss: 13.01962169011434 - Avg Validation Loss: 44.3240270614624\n",
      "Epoch 19/79 - Avg Training Loss: 12.813101768493652 - Avg Validation Loss: 39.61752128601074\n",
      "Epoch 20/79 - Avg Training Loss: 12.925403277079264 - Avg Validation Loss: 37.677894592285156\n",
      "Epoch 21/79 - Avg Training Loss: 12.687499046325684 - Avg Validation Loss: 36.508222579956055\n",
      "Epoch 22/79 - Avg Training Loss: 12.826073487599691 - Avg Validation Loss: 33.41981840133667\n",
      "Epoch 23/79 - Avg Training Loss: 12.517489433288574 - Avg Validation Loss: 35.74732303619385\n",
      "Epoch 24/79 - Avg Training Loss: 12.171773751576742 - Avg Validation Loss: 36.99830436706543\n",
      "Epoch 25/79 - Avg Training Loss: 12.183775742848715 - Avg Validation Loss: 34.599778175354004\n",
      "Epoch 26/79 - Avg Training Loss: 11.826173782348633 - Avg Validation Loss: 28.199480056762695\n",
      "Epoch 27/79 - Avg Training Loss: 12.520250956217447 - Avg Validation Loss: 29.100738048553467\n",
      "Epoch 28/79 - Avg Training Loss: 11.528741518656412 - Avg Validation Loss: 27.45273780822754\n",
      "Epoch 29/79 - Avg Training Loss: 11.337780316670736 - Avg Validation Loss: 26.252437114715576\n",
      "Epoch 30/79 - Avg Training Loss: 10.862635294596354 - Avg Validation Loss: 24.812199115753174\n",
      "Epoch 31/79 - Avg Training Loss: 11.208669503529867 - Avg Validation Loss: 25.92502737045288\n",
      "Epoch 32/79 - Avg Training Loss: 11.204615434010824 - Avg Validation Loss: 31.446568489074707\n",
      "Epoch 33/79 - Avg Training Loss: 10.766967932383219 - Avg Validation Loss: 27.342530250549316\n",
      "Epoch 34/79 - Avg Training Loss: 11.483999252319336 - Avg Validation Loss: 27.007521629333496\n",
      "Epoch 35/79 - Avg Training Loss: 11.364536126454672 - Avg Validation Loss: 27.103633403778076\n",
      "Epoch 36/79 - Avg Training Loss: 11.395581404368082 - Avg Validation Loss: 27.136489391326904\n",
      "Epoch 37/79 - Avg Training Loss: 10.947275161743164 - Avg Validation Loss: 24.588046073913574\n",
      "Epoch 38/79 - Avg Training Loss: 10.655720869700113 - Avg Validation Loss: 24.29289960861206\n",
      "Epoch 39/79 - Avg Training Loss: 10.513738950093588 - Avg Validation Loss: 21.648170471191406\n",
      "Epoch 40/79 - Avg Training Loss: 10.81600014368693 - Avg Validation Loss: 22.955049991607666\n",
      "Epoch 41/79 - Avg Training Loss: 10.594341119130453 - Avg Validation Loss: 18.098564624786377\n",
      "Epoch 42/79 - Avg Training Loss: 10.749613285064697 - Avg Validation Loss: 17.938697338104248\n",
      "Epoch 43/79 - Avg Training Loss: 11.071499029795328 - Avg Validation Loss: 19.270874977111816\n",
      "Epoch 44/79 - Avg Training Loss: 11.352231661478678 - Avg Validation Loss: 17.97021198272705\n",
      "Epoch 45/79 - Avg Training Loss: 11.164395809173584 - Avg Validation Loss: 18.661428928375244\n",
      "Epoch 46/79 - Avg Training Loss: 10.677740414937338 - Avg Validation Loss: 18.73569679260254\n",
      "Epoch 47/79 - Avg Training Loss: 10.96839459737142 - Avg Validation Loss: 18.522164344787598\n",
      "Epoch 48/79 - Avg Training Loss: 10.688684781392416 - Avg Validation Loss: 19.365429878234863\n",
      "Epoch 49/79 - Avg Training Loss: 10.447513739267984 - Avg Validation Loss: 25.07063579559326\n",
      "Epoch 50/79 - Avg Training Loss: 11.076345443725586 - Avg Validation Loss: 20.059570789337158\n",
      "Epoch 51/79 - Avg Training Loss: 10.929726600646973 - Avg Validation Loss: 18.620666980743408\n",
      "Epoch 52/79 - Avg Training Loss: 10.645095984141031 - Avg Validation Loss: 21.118698120117188\n",
      "Epoch 53/79 - Avg Training Loss: 10.611249128977457 - Avg Validation Loss: 22.66023349761963\n",
      "Epoch 54/79 - Avg Training Loss: 10.594101587931315 - Avg Validation Loss: 24.47143268585205\n",
      "Epoch 55/79 - Avg Training Loss: 10.263391494750977 - Avg Validation Loss: 19.44343137741089\n",
      "Epoch 56/79 - Avg Training Loss: 10.115792910257975 - Avg Validation Loss: 21.531350135803223\n",
      "Epoch 57/79 - Avg Training Loss: 10.416083494822184 - Avg Validation Loss: 20.275916576385498\n",
      "Epoch 58/79 - Avg Training Loss: 10.211807568868002 - Avg Validation Loss: 16.930200576782227\n",
      "Epoch 59/79 - Avg Training Loss: 10.601766268412272 - Avg Validation Loss: 18.857551097869873\n",
      "Epoch 60/79 - Avg Training Loss: 9.972112973531088 - Avg Validation Loss: 18.78215217590332\n",
      "Epoch 61/79 - Avg Training Loss: 9.909451007843018 - Avg Validation Loss: 18.643526554107666\n",
      "Epoch 62/79 - Avg Training Loss: 9.945046583811441 - Avg Validation Loss: 17.18270969390869\n",
      "Epoch 63/79 - Avg Training Loss: 10.048551400502523 - Avg Validation Loss: 18.59369945526123\n",
      "Epoch 64/79 - Avg Training Loss: 9.523905754089355 - Avg Validation Loss: 18.978451251983643\n",
      "Epoch 65/79 - Avg Training Loss: 9.44151226679484 - Avg Validation Loss: 19.889161586761475\n",
      "Epoch 66/79 - Avg Training Loss: 9.78428570429484 - Avg Validation Loss: 20.043890476226807\n",
      "Epoch 67/79 - Avg Training Loss: 10.2395068804423 - Avg Validation Loss: 15.189811944961548\n",
      "Epoch 68/79 - Avg Training Loss: 9.997589906056723 - Avg Validation Loss: 18.322978019714355\n",
      "Epoch 69/79 - Avg Training Loss: 10.363025983174643 - Avg Validation Loss: 18.379160404205322\n",
      "Epoch 70/79 - Avg Training Loss: 9.47682555516561 - Avg Validation Loss: 16.335299015045166\n",
      "Epoch 71/79 - Avg Training Loss: 9.707435766855875 - Avg Validation Loss: 16.693231105804443\n",
      "Epoch 72/79 - Avg Training Loss: 10.130423704783121 - Avg Validation Loss: 18.174959659576416\n",
      "Epoch 73/79 - Avg Training Loss: 9.979657967885336 - Avg Validation Loss: 20.77956485748291\n",
      "Epoch 74/79 - Avg Training Loss: 10.12370506922404 - Avg Validation Loss: 20.437922477722168\n",
      "Epoch 75/79 - Avg Training Loss: 9.534943262736002 - Avg Validation Loss: 17.9312424659729\n",
      "Epoch 76/79 - Avg Training Loss: 10.275075594584147 - Avg Validation Loss: 22.12300157546997\n",
      "Epoch 77/79 - Avg Training Loss: 9.898503939310709 - Avg Validation Loss: 24.012083530426025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:22:47,480] Trial 0 finished with value: 21.865248680114746 and parameters: {'hidden_layer_sizes': (512, 512, 512), 'dropout_rate': 0.4931179333392294, 'lr': 0.007853299328225179, 'batch_size': 256, 'epochs': 79}. Best is trial 0 with value: 21.865248680114746.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/79 - Avg Training Loss: 10.450519720713297 - Avg Validation Loss: 22.355361461639404\n",
      "Epoch 79/79 - Avg Training Loss: 9.480810324350992 - Avg Validation Loss: 21.865248680114746\n",
      "Finished Training\n",
      "Epoch 1/149 - Avg Training Loss: 367.77060953776044 - Avg Validation Loss: 313.95501708984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (700, 700, 700, 700) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (512, 512, 512, 512) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (256, 256, 256, 256) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (128, 128, 128, 128) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.4, 0.6)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  lr = trial.suggest_uniform('lr', 0.001, 0.01)\n",
      "C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py:8: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  batch_size = trial.suggest_int('batch_size', 16, 512, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/149 - Avg Training Loss: 313.99546305338544 - Avg Validation Loss: 314.8337860107422\n",
      "Epoch 3/149 - Avg Training Loss: 313.3801778157552 - Avg Validation Loss: 311.9761047363281\n",
      "Epoch 4/149 - Avg Training Loss: 310.82044474283856 - Avg Validation Loss: 306.0960693359375\n",
      "Epoch 5/149 - Avg Training Loss: 292.35589599609375 - Avg Validation Loss: 248.9697723388672\n",
      "Epoch 6/149 - Avg Training Loss: 196.77216593424478 - Avg Validation Loss: 51.76559257507324\n",
      "Epoch 7/149 - Avg Training Loss: 172.83719889322916 - Avg Validation Loss: 153.4046630859375\n",
      "Epoch 8/149 - Avg Training Loss: 138.87708028157553 - Avg Validation Loss: 116.52304077148438\n",
      "Epoch 9/149 - Avg Training Loss: 106.14808400472005 - Avg Validation Loss: 91.98417282104492\n",
      "Epoch 10/149 - Avg Training Loss: 79.96490224202473 - Avg Validation Loss: 115.64155197143555\n",
      "Epoch 11/149 - Avg Training Loss: 68.11126963297527 - Avg Validation Loss: 113.4745101928711\n",
      "Epoch 12/149 - Avg Training Loss: 58.07279968261719 - Avg Validation Loss: 123.97480010986328\n",
      "Epoch 13/149 - Avg Training Loss: 49.381248474121094 - Avg Validation Loss: 159.3232421875\n",
      "Epoch 14/149 - Avg Training Loss: 40.004889170328774 - Avg Validation Loss: 173.163330078125\n",
      "Epoch 15/149 - Avg Training Loss: 38.915042877197266 - Avg Validation Loss: 175.0892562866211\n",
      "Epoch 16/149 - Avg Training Loss: 36.90708923339844 - Avg Validation Loss: 168.69325256347656\n",
      "Epoch 17/149 - Avg Training Loss: 33.94474665323893 - Avg Validation Loss: 168.52747344970703\n",
      "Epoch 18/149 - Avg Training Loss: 30.211315155029297 - Avg Validation Loss: 168.26264190673828\n",
      "Epoch 19/149 - Avg Training Loss: 28.485677083333332 - Avg Validation Loss: 176.79885864257812\n",
      "Epoch 20/149 - Avg Training Loss: 26.151198705037434 - Avg Validation Loss: 172.97061157226562\n",
      "Epoch 21/149 - Avg Training Loss: 25.395015080769856 - Avg Validation Loss: 178.02010345458984\n",
      "Epoch 22/149 - Avg Training Loss: 23.871245702107746 - Avg Validation Loss: 164.77816009521484\n",
      "Epoch 23/149 - Avg Training Loss: 22.51564343770345 - Avg Validation Loss: 167.60107421875\n",
      "Epoch 24/149 - Avg Training Loss: 23.5558344523112 - Avg Validation Loss: 164.03287506103516\n",
      "Epoch 25/149 - Avg Training Loss: 22.898035685221355 - Avg Validation Loss: 168.96099853515625\n",
      "Epoch 26/149 - Avg Training Loss: 20.21092478434245 - Avg Validation Loss: 158.22564697265625\n",
      "Epoch 27/149 - Avg Training Loss: 19.158917744954426 - Avg Validation Loss: 155.34114837646484\n",
      "Epoch 28/149 - Avg Training Loss: 19.433116912841797 - Avg Validation Loss: 152.11321258544922\n",
      "Epoch 29/149 - Avg Training Loss: 19.415247599283855 - Avg Validation Loss: 149.7021255493164\n",
      "Epoch 30/149 - Avg Training Loss: 19.400753021240234 - Avg Validation Loss: 149.9776382446289\n",
      "Epoch 31/149 - Avg Training Loss: 18.456783930460613 - Avg Validation Loss: 148.7414093017578\n",
      "Epoch 32/149 - Avg Training Loss: 18.012324651082356 - Avg Validation Loss: 147.29150390625\n",
      "Epoch 33/149 - Avg Training Loss: 17.51031239827474 - Avg Validation Loss: 147.56937408447266\n",
      "Epoch 34/149 - Avg Training Loss: 17.236726760864258 - Avg Validation Loss: 142.02690887451172\n",
      "Epoch 35/149 - Avg Training Loss: 17.037188212076824 - Avg Validation Loss: 144.07635498046875\n",
      "Epoch 36/149 - Avg Training Loss: 15.915579477945963 - Avg Validation Loss: 139.3398666381836\n",
      "Epoch 37/149 - Avg Training Loss: 17.698684692382812 - Avg Validation Loss: 142.3157196044922\n",
      "Epoch 38/149 - Avg Training Loss: 16.389561971028645 - Avg Validation Loss: 138.08360290527344\n",
      "Epoch 39/149 - Avg Training Loss: 15.42297649383545 - Avg Validation Loss: 137.79724884033203\n",
      "Epoch 40/149 - Avg Training Loss: 15.966228802998861 - Avg Validation Loss: 134.6852569580078\n",
      "Epoch 41/149 - Avg Training Loss: 16.25976022084554 - Avg Validation Loss: 136.69147491455078\n",
      "Epoch 42/149 - Avg Training Loss: 15.392736117045084 - Avg Validation Loss: 140.04124450683594\n",
      "Epoch 43/149 - Avg Training Loss: 15.352895100911459 - Avg Validation Loss: 135.96311950683594\n",
      "Epoch 44/149 - Avg Training Loss: 14.701178550720215 - Avg Validation Loss: 137.80966186523438\n",
      "Epoch 45/149 - Avg Training Loss: 14.873207410176596 - Avg Validation Loss: 128.9333267211914\n",
      "Epoch 46/149 - Avg Training Loss: 14.707688013712565 - Avg Validation Loss: 140.09406280517578\n",
      "Epoch 47/149 - Avg Training Loss: 14.800694783528646 - Avg Validation Loss: 130.9634017944336\n",
      "Epoch 48/149 - Avg Training Loss: 15.219969749450684 - Avg Validation Loss: 136.1446075439453\n",
      "Epoch 49/149 - Avg Training Loss: 13.866763432820639 - Avg Validation Loss: 129.1534423828125\n",
      "Epoch 50/149 - Avg Training Loss: 14.984708468119303 - Avg Validation Loss: 135.82833862304688\n",
      "Epoch 51/149 - Avg Training Loss: 14.989845911661783 - Avg Validation Loss: 132.69507598876953\n",
      "Epoch 52/149 - Avg Training Loss: 14.38229783376058 - Avg Validation Loss: 133.50421142578125\n",
      "Epoch 53/149 - Avg Training Loss: 14.43159039815267 - Avg Validation Loss: 133.09392547607422\n",
      "Epoch 54/149 - Avg Training Loss: 14.248534520467123 - Avg Validation Loss: 131.02916717529297\n",
      "Epoch 55/149 - Avg Training Loss: 13.733742078145346 - Avg Validation Loss: 129.31493759155273\n",
      "Epoch 56/149 - Avg Training Loss: 13.654844284057617 - Avg Validation Loss: 128.1242561340332\n",
      "Epoch 57/149 - Avg Training Loss: 14.086176554361979 - Avg Validation Loss: 124.70790481567383\n",
      "Epoch 58/149 - Avg Training Loss: 13.081730206807455 - Avg Validation Loss: 125.2897720336914\n",
      "Epoch 59/149 - Avg Training Loss: 14.324379603068033 - Avg Validation Loss: 121.08071899414062\n",
      "Epoch 60/149 - Avg Training Loss: 13.808576265970865 - Avg Validation Loss: 125.55901336669922\n",
      "Epoch 61/149 - Avg Training Loss: 13.913731257120768 - Avg Validation Loss: 115.4281997680664\n",
      "Epoch 62/149 - Avg Training Loss: 13.780067761739096 - Avg Validation Loss: 122.98094177246094\n",
      "Epoch 63/149 - Avg Training Loss: 13.88592497507731 - Avg Validation Loss: 115.0540542602539\n",
      "Epoch 64/149 - Avg Training Loss: 13.41329542795817 - Avg Validation Loss: 116.19855117797852\n",
      "Epoch 65/149 - Avg Training Loss: 13.4514373143514 - Avg Validation Loss: 111.8692626953125\n",
      "Epoch 66/149 - Avg Training Loss: 13.587881406148275 - Avg Validation Loss: 110.88534927368164\n",
      "Epoch 67/149 - Avg Training Loss: 13.355200131734213 - Avg Validation Loss: 112.08066940307617\n",
      "Epoch 68/149 - Avg Training Loss: 13.783675829569498 - Avg Validation Loss: 107.72562026977539\n",
      "Epoch 69/149 - Avg Training Loss: 13.037919998168945 - Avg Validation Loss: 107.0237808227539\n",
      "Epoch 70/149 - Avg Training Loss: 13.546683629353842 - Avg Validation Loss: 104.9390983581543\n",
      "Epoch 71/149 - Avg Training Loss: 13.263192494710287 - Avg Validation Loss: 102.49239349365234\n",
      "Epoch 72/149 - Avg Training Loss: 13.095316569010416 - Avg Validation Loss: 99.6406135559082\n",
      "Epoch 73/149 - Avg Training Loss: 12.97251001993815 - Avg Validation Loss: 99.06090545654297\n",
      "Epoch 74/149 - Avg Training Loss: 12.592354774475098 - Avg Validation Loss: 93.87444686889648\n",
      "Epoch 75/149 - Avg Training Loss: 12.543574333190918 - Avg Validation Loss: 94.22811126708984\n",
      "Epoch 76/149 - Avg Training Loss: 12.792561848958334 - Avg Validation Loss: 90.37837600708008\n",
      "Epoch 77/149 - Avg Training Loss: 12.159072558085123 - Avg Validation Loss: 89.40346145629883\n",
      "Epoch 78/149 - Avg Training Loss: 12.914535522460938 - Avg Validation Loss: 87.99428939819336\n",
      "Epoch 79/149 - Avg Training Loss: 12.517640431722006 - Avg Validation Loss: 83.23175811767578\n",
      "Epoch 80/149 - Avg Training Loss: 12.634539922078451 - Avg Validation Loss: 81.51558303833008\n",
      "Epoch 81/149 - Avg Training Loss: 12.615549723307291 - Avg Validation Loss: 76.96738052368164\n",
      "Epoch 82/149 - Avg Training Loss: 12.290359179178873 - Avg Validation Loss: 75.03216171264648\n",
      "Epoch 83/149 - Avg Training Loss: 12.09115473429362 - Avg Validation Loss: 72.9361343383789\n",
      "Epoch 84/149 - Avg Training Loss: 11.708456675211588 - Avg Validation Loss: 67.97647476196289\n",
      "Epoch 85/149 - Avg Training Loss: 12.186132748921713 - Avg Validation Loss: 69.67407608032227\n",
      "Epoch 86/149 - Avg Training Loss: 11.8987824122111 - Avg Validation Loss: 65.12456130981445\n",
      "Epoch 87/149 - Avg Training Loss: 12.52621873219808 - Avg Validation Loss: 67.1099739074707\n",
      "Epoch 88/149 - Avg Training Loss: 12.467081705729166 - Avg Validation Loss: 66.4716682434082\n",
      "Epoch 89/149 - Avg Training Loss: 12.995421727498373 - Avg Validation Loss: 61.13345718383789\n",
      "Epoch 90/149 - Avg Training Loss: 11.999945004781088 - Avg Validation Loss: 60.169315338134766\n",
      "Epoch 91/149 - Avg Training Loss: 11.960938135782877 - Avg Validation Loss: 57.69558906555176\n",
      "Epoch 92/149 - Avg Training Loss: 12.264053344726562 - Avg Validation Loss: 55.29455757141113\n",
      "Epoch 93/149 - Avg Training Loss: 11.55060068766276 - Avg Validation Loss: 53.828651428222656\n",
      "Epoch 94/149 - Avg Training Loss: 12.035989761352539 - Avg Validation Loss: 54.45247268676758\n",
      "Epoch 95/149 - Avg Training Loss: 11.89722983042399 - Avg Validation Loss: 54.11253356933594\n",
      "Epoch 96/149 - Avg Training Loss: 11.99329916636149 - Avg Validation Loss: 52.16644287109375\n",
      "Epoch 97/149 - Avg Training Loss: 11.722207069396973 - Avg Validation Loss: 52.02263641357422\n",
      "Epoch 98/149 - Avg Training Loss: 11.847311973571777 - Avg Validation Loss: 48.72572898864746\n",
      "Epoch 99/149 - Avg Training Loss: 11.473695437113443 - Avg Validation Loss: 49.74341583251953\n",
      "Epoch 100/149 - Avg Training Loss: 11.533136049906412 - Avg Validation Loss: 46.205814361572266\n",
      "Epoch 101/149 - Avg Training Loss: 11.296541213989258 - Avg Validation Loss: 44.553632736206055\n",
      "Epoch 102/149 - Avg Training Loss: 11.572637557983398 - Avg Validation Loss: 42.44076728820801\n",
      "Epoch 103/149 - Avg Training Loss: 12.07688013712565 - Avg Validation Loss: 43.690744400024414\n",
      "Epoch 104/149 - Avg Training Loss: 11.764310201009115 - Avg Validation Loss: 42.64756965637207\n",
      "Epoch 105/149 - Avg Training Loss: 11.97153091430664 - Avg Validation Loss: 44.767404556274414\n",
      "Epoch 106/149 - Avg Training Loss: 11.536455790201822 - Avg Validation Loss: 42.34440231323242\n",
      "Epoch 107/149 - Avg Training Loss: 11.52513313293457 - Avg Validation Loss: 43.04705810546875\n",
      "Epoch 108/149 - Avg Training Loss: 11.059080123901367 - Avg Validation Loss: 42.15542793273926\n",
      "Epoch 109/149 - Avg Training Loss: 11.556040128072103 - Avg Validation Loss: 43.26181221008301\n",
      "Epoch 110/149 - Avg Training Loss: 11.614658991495768 - Avg Validation Loss: 41.980234146118164\n",
      "Epoch 111/149 - Avg Training Loss: 11.61117935180664 - Avg Validation Loss: 40.95768928527832\n",
      "Epoch 112/149 - Avg Training Loss: 11.308822949727377 - Avg Validation Loss: 38.70005798339844\n",
      "Epoch 113/149 - Avg Training Loss: 11.238048235575357 - Avg Validation Loss: 39.77589797973633\n",
      "Epoch 114/149 - Avg Training Loss: 11.156165758768717 - Avg Validation Loss: 40.690799713134766\n",
      "Epoch 115/149 - Avg Training Loss: 11.285319010416666 - Avg Validation Loss: 42.05995750427246\n",
      "Epoch 116/149 - Avg Training Loss: 11.094725608825684 - Avg Validation Loss: 38.8466854095459\n",
      "Epoch 117/149 - Avg Training Loss: 11.060815493265787 - Avg Validation Loss: 41.28239631652832\n",
      "Epoch 118/149 - Avg Training Loss: 11.752878189086914 - Avg Validation Loss: 37.69438934326172\n",
      "Epoch 119/149 - Avg Training Loss: 11.506595929463705 - Avg Validation Loss: 40.28084754943848\n",
      "Epoch 120/149 - Avg Training Loss: 11.860157012939453 - Avg Validation Loss: 37.6087703704834\n",
      "Epoch 121/149 - Avg Training Loss: 11.480173428853353 - Avg Validation Loss: 38.19211387634277\n",
      "Epoch 122/149 - Avg Training Loss: 11.33646043141683 - Avg Validation Loss: 38.27773094177246\n",
      "Epoch 123/149 - Avg Training Loss: 11.726259231567383 - Avg Validation Loss: 38.22578239440918\n",
      "Epoch 124/149 - Avg Training Loss: 11.206178665161133 - Avg Validation Loss: 39.07036018371582\n",
      "Epoch 125/149 - Avg Training Loss: 11.006218274434408 - Avg Validation Loss: 38.41811561584473\n",
      "Epoch 126/149 - Avg Training Loss: 11.192452112833658 - Avg Validation Loss: 38.5229606628418\n",
      "Epoch 127/149 - Avg Training Loss: 11.081415812174479 - Avg Validation Loss: 36.29729652404785\n",
      "Epoch 128/149 - Avg Training Loss: 11.292231241861979 - Avg Validation Loss: 37.14311599731445\n",
      "Epoch 129/149 - Avg Training Loss: 11.327312151590982 - Avg Validation Loss: 36.82855033874512\n",
      "Epoch 130/149 - Avg Training Loss: 11.393966356913248 - Avg Validation Loss: 36.97428512573242\n",
      "Epoch 131/149 - Avg Training Loss: 11.340644836425781 - Avg Validation Loss: 37.47486877441406\n",
      "Epoch 132/149 - Avg Training Loss: 10.90768559773763 - Avg Validation Loss: 38.60751533508301\n",
      "Epoch 133/149 - Avg Training Loss: 11.341965675354004 - Avg Validation Loss: 38.85097694396973\n",
      "Epoch 134/149 - Avg Training Loss: 11.080426851908365 - Avg Validation Loss: 38.95416259765625\n",
      "Epoch 135/149 - Avg Training Loss: 10.878118832906088 - Avg Validation Loss: 37.40741729736328\n",
      "Epoch 136/149 - Avg Training Loss: 11.198744138081869 - Avg Validation Loss: 38.665550231933594\n",
      "Epoch 137/149 - Avg Training Loss: 11.084847450256348 - Avg Validation Loss: 37.59670639038086\n",
      "Epoch 138/149 - Avg Training Loss: 10.91636880238851 - Avg Validation Loss: 39.168832778930664\n",
      "Epoch 139/149 - Avg Training Loss: 11.257762908935547 - Avg Validation Loss: 38.22136688232422\n",
      "Epoch 140/149 - Avg Training Loss: 11.499995867411295 - Avg Validation Loss: 38.320505142211914\n",
      "Epoch 141/149 - Avg Training Loss: 11.489885965983072 - Avg Validation Loss: 34.91229057312012\n",
      "Epoch 142/149 - Avg Training Loss: 11.18826421101888 - Avg Validation Loss: 36.973453521728516\n",
      "Epoch 143/149 - Avg Training Loss: 10.8376677831014 - Avg Validation Loss: 35.36998748779297\n",
      "Epoch 144/149 - Avg Training Loss: 11.1364164352417 - Avg Validation Loss: 37.768733978271484\n",
      "Epoch 145/149 - Avg Training Loss: 11.101075490315756 - Avg Validation Loss: 36.220502853393555\n",
      "Epoch 146/149 - Avg Training Loss: 11.101909637451172 - Avg Validation Loss: 39.37139701843262\n",
      "Epoch 147/149 - Avg Training Loss: 11.12377897898356 - Avg Validation Loss: 35.48309326171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-05 23:22:59,243] Trial 1 finished with value: 35.44999313354492 and parameters: {'hidden_layer_sizes': (512, 512, 512, 512), 'dropout_rate': 0.522003652604353, 'lr': 0.0070277537248807295, 'batch_size': 512, 'epochs': 149}. Best is trial 0 with value: 21.865248680114746.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/149 - Avg Training Loss: 11.199386278788248 - Avg Validation Loss: 38.231679916381836\n",
      "Epoch 149/149 - Avg Training Loss: 10.836978912353516 - Avg Validation Loss: 35.44999313354492\n",
      "Finished Training\n",
      "Epoch 1/97 - Avg Training Loss: 204.91171864100866 - Avg Validation Loss: 137.16927185058594\n",
      "Epoch 2/97 - Avg Training Loss: 103.42510604858398 - Avg Validation Loss: 77.70313415527343\n",
      "Epoch 3/97 - Avg Training Loss: 44.59282466343471 - Avg Validation Loss: 53.267076110839845\n",
      "Epoch 4/97 - Avg Training Loss: 35.313100814819336 - Avg Validation Loss: 42.65153274536133\n",
      "Epoch 5/97 - Avg Training Loss: 27.087751933506556 - Avg Validation Loss: 25.803448104858397\n",
      "Epoch 6/97 - Avg Training Loss: 22.66701235089983 - Avg Validation Loss: 38.72228240966797\n",
      "Epoch 7/97 - Avg Training Loss: 20.900374548775808 - Avg Validation Loss: 35.45978469848633\n",
      "Epoch 8/97 - Avg Training Loss: 20.12446539742606 - Avg Validation Loss: 51.058936309814456\n",
      "Epoch 9/97 - Avg Training Loss: 17.90816020965576 - Avg Validation Loss: 57.536749267578124\n",
      "Epoch 10/97 - Avg Training Loss: 17.632730756487167 - Avg Validation Loss: 45.93903579711914\n",
      "Epoch 11/97 - Avg Training Loss: 16.2162081854684 - Avg Validation Loss: 50.81305694580078\n",
      "Epoch 12/97 - Avg Training Loss: 16.32657895769392 - Avg Validation Loss: 55.19831848144531\n",
      "Epoch 13/97 - Avg Training Loss: 15.085725375584193 - Avg Validation Loss: 42.70559310913086\n",
      "Epoch 14/97 - Avg Training Loss: 14.187059674944196 - Avg Validation Loss: 48.35256576538086\n",
      "Epoch 15/97 - Avg Training Loss: 15.03891999380929 - Avg Validation Loss: 48.64552230834961\n",
      "Epoch 16/97 - Avg Training Loss: 13.70142092023577 - Avg Validation Loss: 38.14369506835938\n",
      "Epoch 17/97 - Avg Training Loss: 13.02828598022461 - Avg Validation Loss: 34.33160552978516\n",
      "Epoch 18/97 - Avg Training Loss: 13.123978887285505 - Avg Validation Loss: 31.179440689086913\n",
      "Epoch 19/97 - Avg Training Loss: 12.079387528555733 - Avg Validation Loss: 28.315418243408203\n",
      "Epoch 20/97 - Avg Training Loss: 11.448787825448173 - Avg Validation Loss: 26.144692611694335\n",
      "Epoch 21/97 - Avg Training Loss: 11.825950622558594 - Avg Validation Loss: 23.21656379699707\n",
      "Epoch 22/97 - Avg Training Loss: 12.616948672703334 - Avg Validation Loss: 23.430365753173827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-05 23:23:01,088] Trial 2 failed with parameters: {'hidden_layer_sizes': (512, 512, 512), 'dropout_rate': 0.5978841944588567, 'lr': 0.007737579285801058, 'batch_size': 208, 'epochs': 97} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\3470110313.py\", line 21, in MLP_full_gridsearch\n",
      "    val_loss = train_MLP_full(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Robbie\\AppData\\Local\\Temp\\ipykernel_16892\\1111882985.py\", line 12, in train_MLP_full\n",
      "    optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py\", line 166, in step\n",
      "    adam(\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py\", line 316, in adam\n",
      "    func(params,\n",
      "  File \"c:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py\", line 441, in _single_tensor_adam\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-05 23:23:01,090] Trial 2 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/97 - Avg Training Loss: 11.378931181771415 - Avg Validation Loss: 20.993088912963866\n",
      "Epoch 24/97 - Avg Training Loss: 11.36909784589495 - Avg Validation Loss: 20.132217788696288\n",
      "Epoch 25/97 - Avg Training Loss: 11.121209825788226 - Avg Validation Loss: 21.476254653930663\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Optuna study object and direction (minimize validation loss)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_full_gridsearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====================================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[65], line 21\u001b[0m, in \u001b[0;36mMLP_full_gridsearch\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(t_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model, return validation loss\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_MLP_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m, in \u001b[0;36mtrain_MLP_full\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# Compute the loss (MSE) between the predictions and the ground-truth labels\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Update model parameters (weights) based on the gradients computed during backpropagation\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH <-- TODO: (Make list for final model to plot)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Validation time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Robbie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_gridsearch(trial):\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # The line below does not work due to a optuna limitation. It is kept here for reference.\n",
    "    #! hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [ (v,) * i for v in [700, 512, 256, 128] for i in range(2, 5)])\n",
    "    \n",
    "    hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, 16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLPfull(hidden_layer_sizes, dropout_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss = train_MLP_full(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(MLP_full_gridsearch, n_trials=25)\n",
    "\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "else: print('Skipping MLP full grid search')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
