{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Scaling(Enum):\n",
    "    INDEPENDENT = 1\n",
    "    JOINT = 2\n",
    "\n",
    "class DatasetType(StrEnum):\n",
    "    TRAIN = 'trn'\n",
    "    TEST = 'tst'\n",
    "    VALIDATION = 'trn'\n",
    "\n",
    "# Global variable to enable debug mode\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# *Helper function to preprocess the RSSI data\n",
    "def preprocess_rssi_data(df_rssi: pd.DataFrame, scaling_strategy: Scaling) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function preprocesses the training data by:\n",
    "    1. Replacing all 100 values with -110 (ensures continuity of data)\n",
    "    2. Separating the RSS values from the labels\n",
    "    3. Scaling the data to have zero mean and unit variance\n",
    "\n",
    "    Parameters:\n",
    "    - train: The training data to be preprocessed\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. replace all 100 values with -110 (ensures continuity of data)\n",
    "    df = df_rssi.replace(100, -110)\n",
    "    \n",
    "    # 2. Separate the RSS values from the labels\n",
    "    rssiValues = df.iloc[:, :-3]\n",
    "    labels = df.iloc[:, -3:]\n",
    "    \n",
    "    # 3. Scale the data to have zero mean and unit variance\n",
    "    # This is done either independently for each AP or jointly for all APs\n",
    "    if scaling_strategy == Scaling.INDEPENDENT:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        scaled_rss = scaler.fit_transform(rssiValues)\n",
    "        df_scaled_rss = pd.DataFrame(scaled_rss, columns=rssiValues.columns)\n",
    "        df = pd.concat([df_scaled_rss, labels], axis=1)\n",
    "    \n",
    "    elif scaling_strategy == Scaling.JOINT:\n",
    "        flattened = rssiValues.values.flatten()\n",
    "        global_mean = np.mean(flattened)\n",
    "        global_std = np.std(flattened)\n",
    "        \n",
    "        scaled_rss = (rssiValues - global_mean) / global_std\n",
    "        df = pd.concat([scaled_rss, labels], axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    else: \n",
    "        raise NotImplementedError(\"Specified scaling strategy is not implemented, use either Scaling.INDEPENDENT or Scaling.JOINT.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # *Load and pre-process the training data\n",
    "# def get_preprocessed_training_data(data_path: str, training_months: list[str], num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data_path: The path to the data\n",
    "#     - training_months: The list of training months to be used\n",
    "#     - num_APs: The number of access points\n",
    "#     - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "#     - floor: The floor to be used\n",
    "#     \"\"\"\n",
    "#     # Since the csv files do not have column names, we define these first.\n",
    "#     list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "#     # Load the training data from all specified training sets.  \n",
    "#     df_rss = pd.concat([pd.read_csv(data_path + training_set + 'trn01rss.csv', names=list_of_APs) for training_set in training_months])\n",
    "#     df_rss = df_rss.reset_index(drop=True)\n",
    "    \n",
    "#     # Get all x,y,floor labels (gotten from data_path + training_month + 'trn01crd.csv')\n",
    "#     df_labels = pd.concat([pd.read_csv(data_path + training_set + 'trn01crd.csv', names=['x', 'y', 'floor']) for training_set in training_months])\n",
    "#     df_labels = df_labels.reset_index(drop=True)\n",
    "\n",
    "#     # Add the labels to the pre-processed data\n",
    "#     df_labeled = pd.concat([df_rss, df_labels], axis=1)\n",
    "    \n",
    "#     # Filter the data to only include the specified floor\n",
    "#     df_labeled = df_labeled[df_labeled['floor'] == floor]\n",
    "\n",
    "#     # Pre-processing of the training data\n",
    "#     df_train = preprocess_rssi_data(df_labeled, scaling_strategy)\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# *Load and pre-process the data\n",
    "def get_preprocessed_dataset(data_path: str, months: list[str], sets: list[str], type: DatasetType, num_APs: int, scaling_strategy: Scaling, floor: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function loads and preprocesses the training data from the specified training months and floor.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: The path to the data\n",
    "    - months: The list of months to be used\n",
    "    - sets: The list of set numbers to be used\n",
    "    - type: The type of dataset to be made (TRAIN, TEST or VALIDATION)\n",
    "    - num_APs: The number of access points\n",
    "    - scaling_strategy: The scaling strategy to be used (INDEPENDENT or JOINT)\n",
    "    - floor: The floor to be used\n",
    "    \"\"\"\n",
    "    # Since the csv files do not have column names, we define these first.\n",
    "    list_of_APs = [\"AP\" + str(i) for i in range(0, num_APs)]\n",
    "\n",
    "    # Load the test data from all specified test sets.  \n",
    "    df_test_rss = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'rss.csv', names=list_of_APs) for month in months for set in sets])\n",
    "    df_test_rss = df_test_rss.reset_index(drop=True)\n",
    "    \n",
    "    # Get all x,y,floor labels\n",
    "    df_test_labels = pd.concat([pd.read_csv(data_path + month + '/' + type + set + 'crd.csv', names=['x', 'y', 'floor']) for month in months for set in sets])\n",
    "    df_test_labels = df_test_labels.reset_index(drop=True)\n",
    "\n",
    "    # Add the labels to the pre-processed data\n",
    "    df_test_labeled = pd.concat([df_test_rss, df_test_labels], axis=1)\n",
    "    \n",
    "    # Filter the data to only include the specified floor\n",
    "    df_test_labeled = df_test_labeled[df_test_labeled['floor'] == floor]\n",
    "\n",
    "    # Pre-processing of the training data\n",
    "    df_test = preprocess_rssi_data(df_test_labeled, scaling_strategy)\n",
    "    \n",
    "    return df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "training_months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01']\n",
    "type = DatasetType.TRAIN\n",
    "num_APs = 620\n",
    "scaling_strategy = Scaling.JOINT\n",
    "floor = 3\n",
    "\n",
    "\n",
    "df_train_full = get_preprocessed_dataset(data_path, training_months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_train_x = df_train_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_train_y = df_train_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_train_full:', df_train_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['01', '02', '03', '04', '05']\n",
    "sets = ['01'] # 01 Corresponds to the same locations as the training set\n",
    "type = DatasetType.TEST\n",
    "\n",
    "df_test_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_test_x = df_test_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_test_y = df_test_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_test_full:', df_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['01']\n",
    "sets = ['02', '03', '04']\n",
    "type = DatasetType.VALIDATION\n",
    "\n",
    "df_val_full = get_preprocessed_dataset(data_path, months, sets, type, num_APs, scaling_strategy, floor)\n",
    "df_val_x = df_val_full.iloc[:, :-3] # Just the RSSI values\n",
    "df_val_y = df_val_full.iloc[:, -3:-1] # Just the x and y coordinates (no floor)\n",
    "\n",
    "if DEBUG: print('df_val_full:', df_val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Since the implementations will be made in PyTorch, we convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(df_train_x.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(df_train_y.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(df_test_x.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(df_test_y.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(df_val_x.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(df_val_y.values, dtype=torch.float32)\n",
    "\n",
    "# Get the data via DataLoaders\n",
    "t_training = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "t_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "t_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# train_loader = DataLoader(t_training, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(t_test, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(t_val, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-Input MLP\n",
    "This network takes the full input of 620 features to perform x,y predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rate, input_dim=620):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Make it easier to grid-search different sizes of hidden layers\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim # Ensure all hidden layers are constant size\n",
    "        \n",
    "        # At this point we know input_dim equals the output size of the last hidden layer, so we can re-use it here.\n",
    "        layers.append(nn.Linear(input_dim, 2)) # x,y output\n",
    "        \n",
    "        # Construct the actual model based on the layers defined above.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "def train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Sets the model to training mode\n",
    "        running_loss = 0.0 # Keep track of the (MSE) loss\n",
    "        \n",
    "        # Actual training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) #! Move data to GPU if available (seems to require different torch install...)\n",
    "            \n",
    "            # Extra case for LGFBS\n",
    "            def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "            \n",
    "            if isinstance(optimizer, torch.optim.LBFGS):\n",
    "                optimizer.step(closure)\n",
    "                loss = closure()\n",
    "            \n",
    "            else:\n",
    "                optimizer.zero_grad() # Reset gradients from last iteration\n",
    "                outputs = model(inputs) # Forward pass\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                loss.backward() # Perform backpropagation\n",
    "                optimizer.step() # Update model parameters (weights) based on the gradients computed during backpropagation\n",
    "            \n",
    "            running_loss += loss.item() # Running loss is the sum of the losses for all batches FOR THE CURRENT EPOCH <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Validation time\n",
    "        model.eval()\n",
    "        val_loss = 0.0 # Accumulated validation loss\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad(): # No need to compute gradients during validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "                \n",
    "                outputs = model(inputs) # Forward pass to get predictions\n",
    "                loss = criterion(outputs, labels) # Compute the loss (MSE) between the predictions and the ground-truth labels\n",
    "                val_loss += loss.item() # Accumulate the validation loss for this epoch <-- TODO: (Make list for final model to plot)\n",
    "        \n",
    "        # Print the loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(train_loader)} - Avg Validation Loss: {val_loss/len(val_loader)}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return val_loss/len(val_loader) # Return the average validation loss for final epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced-Input MLP\n",
    "Use either stacked or deep autoencoder to reduce the input space before training a MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Simple auto-encoder class with a single hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder - Compress input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder - Reconstruct input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a *single* autoencoder\n",
    "def train_autoencoder(autoencoder, data_loader, criterion, optimizer, epochs):\n",
    "    autoencoder.train() # Enable training mode\n",
    "    \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # Running loss for this epoch\n",
    "        \n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad() # Reset gradients from last iteration\n",
    "            \n",
    "            _, outputs = autoencoder(inputs) # Forward pass - only care about the reconstructed data to compute the loss with.\n",
    "            loss = criterion(outputs, inputs) # Compute the loss between the reconstructed data and the original input\n",
    "            \n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update model params based on gradients\n",
    "            \n",
    "            running_loss += loss.item() # Accumulate loss, item() is used to extract the actual loss value from the tensor\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Avg Training Loss: {running_loss/len(data_loader)}')\n",
    "\n",
    "# Function to, sequentially, train a stack of autoencoders\n",
    "def train_stacked_autoencoders(train_data, input_dim, num_encoders, epochs=20):\n",
    "    train_dataset = TensorDataset(train_data, train_data) # Autoencoders are unsupervised, so the input data is also the target data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    encoders = [] # List to store the trained autoencoders\n",
    "    current_dim = input_dim # The current input dimension\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for enc_out in num_encoders:\n",
    "        autoencoder = Autoencoder(current_dim, enc_out).to(device) # Create a new autoencoder\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        train_autoencoder(autoencoder, train_loader, criterion, optimizer, epochs)\n",
    "        \n",
    "        encoders.append(autoencoder) # Add the trained autoencoder to the list\n",
    "        \n",
    "        # Update input data to the encoded data from the current autoencoder\n",
    "        train_data = get_encoded_data(autoencoder, train_loader)\n",
    "        train_dataset = TensorDataset(train_data, train_data)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        current_dim = enc_out # Update the current input dimension\n",
    "\n",
    "    return encoders\n",
    "\n",
    "# Utility function to get the encoded data from the autoencoder\n",
    "def get_encoded_data(autoencoder, data_loader):\n",
    "    encoded_data = []\n",
    "\n",
    "    autoencoder.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # No need to compute gradients during inference\n",
    "        for inputs, _ in data_loader: # Unsupervised learning, so we don't need the labels\n",
    "            inputs = inputs.to(device) # Move data to GPU if available\n",
    "            \n",
    "            encoded, _ = autoencoder(inputs) # Forward pass - only care about the encoded data\n",
    "            encoded_data.append(encoded)\n",
    "    \n",
    "    return torch.cat(encoded_data, dim=0) # Concatenate all encoded data into a single tensor\n",
    "\n",
    "def stacked_encode_data(data, encoders):\n",
    "    \"\"\"\n",
    "    Function to encode data using a stack of autoencoders.\n",
    "    Assumes that the autoencoders have already been trained.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to be encoded\n",
    "    - encoders: The stack of trained autoencoders to be used (provided as ordered list)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for encoder in encoders:\n",
    "            data, _ = encoder(data.to(device))\n",
    "    \n",
    "    return data.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def MLP_full_gridsearch(trial, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    # The line below does not work due to a optuna limitation. It is kept here for reference.\n",
    "    #! hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [ (v,) * i for v in [700, 512, 256, 128] for i in range(2, 5)])\n",
    "    \n",
    "    hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [700, 512, 256, 128])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate)\n",
    "    \n",
    "    if optim == 'Adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim == 'LBFGS': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    train_loader = DataLoader(t_training, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(t_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP_SAE_optimize(trial, SAE, input_size, optim : str = 'Adam') -> float:\n",
    "    # Hyper-parameters to be optimized\n",
    "    \n",
    "    \n",
    "    \n",
    "    hidden_layer_size = trial.suggest_categorical('hidden_layer_size', [256, 128, 64, 32, 16])\n",
    "    hidden_layer_count = trial.suggest_int('hidden_layer_count', 2, 4) # inclusive\n",
    "    hidden_layer_sizes = (hidden_layer_size,) * hidden_layer_count\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.6)\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 512, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 50, 150)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MLP(hidden_layer_sizes, dropout_rate, input_size)\n",
    "    \n",
    "    \n",
    "    if optim == 'Adam': optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim == 'LBFGS': optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else : raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use chosen batch size instead of pre-defined one\n",
    "    \n",
    "    # Encode training and validation data using the stacked autoencoders in SAE\n",
    "    train_data_encoded = stacked_encode_data(X_train_tensor, SAE)\n",
    "    val_data_encoded = stacked_encode_data(X_val_tensor, SAE)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_data_encoded, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data_encoded, y_val_tensor), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Train the model, return validation loss\n",
    "    val_loss = train_MLP(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following booleans to enable or disable the grid-search for the different models.\n",
    "# After running the grid-search, train the final models with the best hyperparameters.\n",
    "\n",
    "SEARCH_MLP_FULL = False\n",
    "SEARCH_MLP_REDUCED_256 = True\n",
    "SEARCH_MLP_REDUCED_128 = False\n",
    "\n",
    "#! Not sure if grid search will work with KANs, as the recommended approach differs.\n",
    "SEARCH_KAN_FULL = False #TODO\n",
    "SEARCH_KAN_REDUCED = False #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SEARCH_MLP_FULL\n",
      "\n",
      "====================================\n",
      "\n",
      "V results for SEARCH_MLP_REDUCED_256 V\n",
      "\n",
      "====================================\n",
      "\n",
      "Starting MLP reduced grid search for 512-256 SAE\n",
      "Epoch 1/20 - Avg Training Loss: 0.5597692119038623\n",
      "Epoch 2/20 - Avg Training Loss: 0.2800369061853575\n",
      "Epoch 3/20 - Avg Training Loss: 0.2187383991220723\n",
      "Epoch 4/20 - Avg Training Loss: 0.1854914569336435\n",
      "Epoch 5/20 - Avg Training Loss: 0.1603270322084427\n",
      "Epoch 6/20 - Avg Training Loss: 0.14226026962632718\n",
      "Epoch 7/20 - Avg Training Loss: 0.13016211565421976\n",
      "Epoch 8/20 - Avg Training Loss: 0.12168703293022902\n",
      "Epoch 9/20 - Avg Training Loss: 0.11597446384637253\n",
      "Epoch 10/20 - Avg Training Loss: 0.11235506994568784\n",
      "Epoch 11/20 - Avg Training Loss: 0.10975772563530051\n",
      "Epoch 12/20 - Avg Training Loss: 0.10651862880458003\n",
      "Epoch 13/20 - Avg Training Loss: 0.10541031736394633\n",
      "Epoch 14/20 - Avg Training Loss: 0.10356261419213336\n",
      "Epoch 15/20 - Avg Training Loss: 0.10345107122607854\n",
      "Epoch 16/20 - Avg Training Loss: 0.1023368903476259\n",
      "Epoch 17/20 - Avg Training Loss: 0.10155539214611053\n",
      "Epoch 18/20 - Avg Training Loss: 0.10112637896900592\n",
      "Epoch 19/20 - Avg Training Loss: 0.10085488629081975\n",
      "Epoch 20/20 - Avg Training Loss: 0.10069399067889089\n",
      "Epoch 1/20 - Avg Training Loss: 1.2941488312638325\n",
      "Epoch 2/20 - Avg Training Loss: 0.7312812053638956\n",
      "Epoch 3/20 - Avg Training Loss: 0.5101285745268282\n",
      "Epoch 4/20 - Avg Training Loss: 0.4454605372055717\n",
      "Epoch 5/20 - Avg Training Loss: 0.40476395384125086\n",
      "Epoch 6/20 - Avg Training Loss: 0.3628893818544305\n",
      "Epoch 7/20 - Avg Training Loss: 0.333862418713777\n",
      "Epoch 8/20 - Avg Training Loss: 0.29937611455502716\n",
      "Epoch 9/20 - Avg Training Loss: 0.2711673281762911\n",
      "Epoch 10/20 - Avg Training Loss: 0.2558466753234034\n",
      "Epoch 11/20 - Avg Training Loss: 0.2432542682989784\n",
      "Epoch 12/20 - Avg Training Loss: 0.2320750971203265\n",
      "Epoch 13/20 - Avg Training Loss: 0.22113730272521143\n",
      "Epoch 14/20 - Avg Training Loss: 0.21089449784030084\n",
      "Epoch 15/20 - Avg Training Loss: 0.2034505676964055\n",
      "Epoch 16/20 - Avg Training Loss: 0.1921419559613518\n",
      "Epoch 17/20 - Avg Training Loss: 0.18351498699706534\n",
      "Epoch 18/20 - Avg Training Loss: 0.17821752441966016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-07 00:20:29,704] A new study created in memory with name: no-name-f426e26f-afdb-4d97-b86e-817677f21512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Avg Training Loss: 0.17383758335009866\n",
      "Epoch 20/20 - Avg Training Loss: 0.16982170939445496\n",
      "Epoch 1/80 - Avg Training Loss: 304.9111836751302 - Avg Validation Loss: 296.6076965332031\n",
      "Epoch 2/80 - Avg Training Loss: 274.05018107096356 - Avg Validation Loss: 250.98193359375\n",
      "Epoch 3/80 - Avg Training Loss: 239.64032999674478 - Avg Validation Loss: 196.93150329589844\n",
      "Epoch 4/80 - Avg Training Loss: 215.02565002441406 - Avg Validation Loss: 190.51630401611328\n",
      "Epoch 5/80 - Avg Training Loss: 190.57705688476562 - Avg Validation Loss: 192.10848999023438\n",
      "Epoch 6/80 - Avg Training Loss: 177.76195271809897 - Avg Validation Loss: 171.52418518066406\n",
      "Epoch 7/80 - Avg Training Loss: 168.87046813964844 - Avg Validation Loss: 141.07421112060547\n",
      "Epoch 8/80 - Avg Training Loss: 167.75939432779947 - Avg Validation Loss: 147.10289764404297\n",
      "Epoch 9/80 - Avg Training Loss: 155.90685526529947 - Avg Validation Loss: 146.8261260986328\n",
      "Epoch 10/80 - Avg Training Loss: 144.18819173177084 - Avg Validation Loss: 134.395263671875\n",
      "Epoch 11/80 - Avg Training Loss: 141.35113016764322 - Avg Validation Loss: 123.35024642944336\n",
      "Epoch 12/80 - Avg Training Loss: 141.81511942545572 - Avg Validation Loss: 119.31320571899414\n",
      "Epoch 13/80 - Avg Training Loss: 139.1162567138672 - Avg Validation Loss: 128.19641876220703\n",
      "Epoch 14/80 - Avg Training Loss: 127.5020268758138 - Avg Validation Loss: 122.51991271972656\n",
      "Epoch 15/80 - Avg Training Loss: 130.48600260416666 - Avg Validation Loss: 144.72150421142578\n",
      "Epoch 16/80 - Avg Training Loss: 124.44986979166667 - Avg Validation Loss: 112.89996337890625\n",
      "Epoch 17/80 - Avg Training Loss: 120.28584543863933 - Avg Validation Loss: 117.42898941040039\n",
      "Epoch 18/80 - Avg Training Loss: 114.37651824951172 - Avg Validation Loss: 140.29678344726562\n",
      "Epoch 19/80 - Avg Training Loss: 107.53389231363933 - Avg Validation Loss: 129.34184646606445\n",
      "Epoch 20/80 - Avg Training Loss: 115.60766855875652 - Avg Validation Loss: 150.17153930664062\n",
      "Epoch 21/80 - Avg Training Loss: 108.99945831298828 - Avg Validation Loss: 139.32632446289062\n",
      "Epoch 22/80 - Avg Training Loss: 108.76580556233723 - Avg Validation Loss: 125.3916130065918\n",
      "Epoch 23/80 - Avg Training Loss: 98.10914103190105 - Avg Validation Loss: 114.67002487182617\n",
      "Epoch 24/80 - Avg Training Loss: 104.6628926595052 - Avg Validation Loss: 139.95542907714844\n",
      "Epoch 25/80 - Avg Training Loss: 110.02546691894531 - Avg Validation Loss: 128.40148162841797\n",
      "Epoch 26/80 - Avg Training Loss: 99.91015370686848 - Avg Validation Loss: 106.14878845214844\n",
      "Epoch 27/80 - Avg Training Loss: 101.15540822347005 - Avg Validation Loss: 128.3267936706543\n",
      "Epoch 28/80 - Avg Training Loss: 99.38398996988933 - Avg Validation Loss: 110.1758041381836\n",
      "Epoch 29/80 - Avg Training Loss: 96.04593658447266 - Avg Validation Loss: 123.58355331420898\n",
      "Epoch 30/80 - Avg Training Loss: 97.40950520833333 - Avg Validation Loss: 120.36031341552734\n",
      "Epoch 31/80 - Avg Training Loss: 91.22374216715495 - Avg Validation Loss: 89.10973739624023\n",
      "Epoch 32/80 - Avg Training Loss: 90.26096852620442 - Avg Validation Loss: 102.64251708984375\n",
      "Epoch 33/80 - Avg Training Loss: 90.14166005452473 - Avg Validation Loss: 98.15869140625\n",
      "Epoch 34/80 - Avg Training Loss: 90.7738749186198 - Avg Validation Loss: 90.0687255859375\n",
      "Epoch 35/80 - Avg Training Loss: 87.27825164794922 - Avg Validation Loss: 99.2743148803711\n",
      "Epoch 36/80 - Avg Training Loss: 85.24105326334636 - Avg Validation Loss: 73.25416946411133\n",
      "Epoch 37/80 - Avg Training Loss: 85.81773122151692 - Avg Validation Loss: 83.37356948852539\n",
      "Epoch 38/80 - Avg Training Loss: 83.16809336344402 - Avg Validation Loss: 69.2074203491211\n",
      "Epoch 39/80 - Avg Training Loss: 80.12708282470703 - Avg Validation Loss: 52.35671615600586\n",
      "Epoch 40/80 - Avg Training Loss: 73.96167246500652 - Avg Validation Loss: 51.503122329711914\n",
      "Epoch 41/80 - Avg Training Loss: 75.53302510579427 - Avg Validation Loss: 40.02022933959961\n",
      "Epoch 42/80 - Avg Training Loss: 69.73741658528645 - Avg Validation Loss: 33.68459701538086\n",
      "Epoch 43/80 - Avg Training Loss: 63.88157653808594 - Avg Validation Loss: 21.52120304107666\n",
      "Epoch 44/80 - Avg Training Loss: 64.10891342163086 - Avg Validation Loss: 26.586200714111328\n",
      "Epoch 45/80 - Avg Training Loss: 63.78531010945638 - Avg Validation Loss: 24.219714164733887\n",
      "Epoch 46/80 - Avg Training Loss: 59.25483067830404 - Avg Validation Loss: 21.64246940612793\n",
      "Epoch 47/80 - Avg Training Loss: 61.06568272908529 - Avg Validation Loss: 21.078890800476074\n",
      "Epoch 48/80 - Avg Training Loss: 62.36168416341146 - Avg Validation Loss: 21.879937171936035\n",
      "Epoch 49/80 - Avg Training Loss: 56.92722702026367 - Avg Validation Loss: 23.071945190429688\n",
      "Epoch 50/80 - Avg Training Loss: 59.35173543294271 - Avg Validation Loss: 21.447322845458984\n",
      "Epoch 51/80 - Avg Training Loss: 56.45155715942383 - Avg Validation Loss: 22.604357719421387\n",
      "Epoch 52/80 - Avg Training Loss: 52.800679524739586 - Avg Validation Loss: 23.725589752197266\n",
      "Epoch 53/80 - Avg Training Loss: 57.85690561930338 - Avg Validation Loss: 22.112847328186035\n",
      "Epoch 54/80 - Avg Training Loss: 56.62777837117513 - Avg Validation Loss: 20.875415802001953\n",
      "Epoch 55/80 - Avg Training Loss: 57.19696807861328 - Avg Validation Loss: 20.84002685546875\n",
      "Epoch 56/80 - Avg Training Loss: 52.281368255615234 - Avg Validation Loss: 21.252946853637695\n",
      "Epoch 57/80 - Avg Training Loss: 58.235129038492836 - Avg Validation Loss: 22.141003608703613\n",
      "Epoch 58/80 - Avg Training Loss: 51.52868906656901 - Avg Validation Loss: 22.758549690246582\n",
      "Epoch 59/80 - Avg Training Loss: 55.356194814046226 - Avg Validation Loss: 22.019041061401367\n",
      "Epoch 60/80 - Avg Training Loss: 55.18218104044596 - Avg Validation Loss: 21.364500999450684\n",
      "Epoch 61/80 - Avg Training Loss: 49.72575124104818 - Avg Validation Loss: 20.143771171569824\n",
      "Epoch 62/80 - Avg Training Loss: 51.061257680257164 - Avg Validation Loss: 20.350902557373047\n",
      "Epoch 63/80 - Avg Training Loss: 53.50614547729492 - Avg Validation Loss: 21.33252716064453\n",
      "Epoch 64/80 - Avg Training Loss: 52.49067687988281 - Avg Validation Loss: 21.728928565979004\n",
      "Epoch 65/80 - Avg Training Loss: 52.401611328125 - Avg Validation Loss: 22.298648834228516\n",
      "Epoch 66/80 - Avg Training Loss: 53.09801483154297 - Avg Validation Loss: 21.62968635559082\n",
      "Epoch 67/80 - Avg Training Loss: 51.06762949625651 - Avg Validation Loss: 20.422860145568848\n",
      "Epoch 68/80 - Avg Training Loss: 52.72736612955729 - Avg Validation Loss: 19.29886817932129\n",
      "Epoch 69/80 - Avg Training Loss: 53.535970052083336 - Avg Validation Loss: 19.7445707321167\n",
      "Epoch 70/80 - Avg Training Loss: 53.02705510457357 - Avg Validation Loss: 20.691826820373535\n",
      "Epoch 71/80 - Avg Training Loss: 51.24449793497721 - Avg Validation Loss: 21.25131320953369\n",
      "Epoch 72/80 - Avg Training Loss: 49.365832010904946 - Avg Validation Loss: 21.329798698425293\n",
      "Epoch 73/80 - Avg Training Loss: 51.37637837727865 - Avg Validation Loss: 20.5245418548584\n",
      "Epoch 74/80 - Avg Training Loss: 50.424407958984375 - Avg Validation Loss: 19.4646053314209\n",
      "Epoch 75/80 - Avg Training Loss: 50.534010569254555 - Avg Validation Loss: 18.572622299194336\n",
      "Epoch 76/80 - Avg Training Loss: 49.706939697265625 - Avg Validation Loss: 18.977124214172363\n",
      "Epoch 77/80 - Avg Training Loss: 51.11462529500326 - Avg Validation Loss: 20.068711280822754\n",
      "Epoch 78/80 - Avg Training Loss: 49.281289418538414 - Avg Validation Loss: 21.491182327270508\n",
      "Epoch 79/80 - Avg Training Loss: 48.678611755371094 - Avg Validation Loss: 21.92002582550049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-07 00:20:30,723] Trial 0 finished with value: 20.830942153930664 and parameters: {'hidden_layer_size': 16, 'hidden_layer_count': 4, 'dropout_rate': 0.5783353912288658, 'lr': 0.005824796562274099, 'batch_size': 512, 'epochs': 80}. Best is trial 0 with value: 20.830942153930664.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/80 - Avg Training Loss: 49.907997131347656 - Avg Validation Loss: 20.830942153930664\n",
      "Finished Training\n",
      "Epoch 1/56 - Avg Training Loss: 218.7154983520508 - Avg Validation Loss: 21.509294509887695\n",
      "Epoch 2/56 - Avg Training Loss: 82.16027908325195 - Avg Validation Loss: 96.44327036539714\n",
      "Epoch 3/56 - Avg Training Loss: 59.69555511474609 - Avg Validation Loss: 17.683320999145508\n",
      "Epoch 4/56 - Avg Training Loss: 46.65856399536133 - Avg Validation Loss: 61.62969207763672\n",
      "Epoch 5/56 - Avg Training Loss: 47.06623458862305 - Avg Validation Loss: 19.55637804667155\n",
      "Epoch 6/56 - Avg Training Loss: 37.75793342590332 - Avg Validation Loss: 45.328590393066406\n",
      "Epoch 7/56 - Avg Training Loss: 36.67086639404297 - Avg Validation Loss: 18.090246200561523\n",
      "Epoch 8/56 - Avg Training Loss: 36.15837707519531 - Avg Validation Loss: 47.64255396525065\n",
      "Epoch 9/56 - Avg Training Loss: 32.22306785583496 - Avg Validation Loss: 43.31865310668945\n",
      "Epoch 10/56 - Avg Training Loss: 31.7847541809082 - Avg Validation Loss: 29.208344141642254\n",
      "Epoch 11/56 - Avg Training Loss: 28.956501007080078 - Avg Validation Loss: 36.88552729288737\n",
      "Epoch 12/56 - Avg Training Loss: 28.974833679199218 - Avg Validation Loss: 30.713180541992188\n",
      "Epoch 13/56 - Avg Training Loss: 27.438788223266602 - Avg Validation Loss: 44.35022481282552\n",
      "Epoch 14/56 - Avg Training Loss: 26.863190841674804 - Avg Validation Loss: 25.37768809000651\n",
      "Epoch 15/56 - Avg Training Loss: 27.02006378173828 - Avg Validation Loss: 32.54372533162435\n",
      "Epoch 16/56 - Avg Training Loss: 27.429286575317384 - Avg Validation Loss: 32.546515146891274\n",
      "Epoch 17/56 - Avg Training Loss: 27.79369125366211 - Avg Validation Loss: 47.42569478352865\n",
      "Epoch 18/56 - Avg Training Loss: 25.88265953063965 - Avg Validation Loss: 47.0228157043457\n",
      "Epoch 19/56 - Avg Training Loss: 25.78635902404785 - Avg Validation Loss: 38.490684509277344\n",
      "Epoch 20/56 - Avg Training Loss: 23.375257873535155 - Avg Validation Loss: 40.248610178629555\n",
      "Epoch 21/56 - Avg Training Loss: 23.315565872192384 - Avg Validation Loss: 26.610629399617512\n",
      "Epoch 22/56 - Avg Training Loss: 25.114034271240236 - Avg Validation Loss: 29.076149622599285\n",
      "Epoch 23/56 - Avg Training Loss: 24.41652717590332 - Avg Validation Loss: 33.68113454182943\n",
      "Epoch 24/56 - Avg Training Loss: 23.007576751708985 - Avg Validation Loss: 59.425558725992836\n",
      "Epoch 25/56 - Avg Training Loss: 25.335365295410156 - Avg Validation Loss: 56.4159787495931\n",
      "Epoch 26/56 - Avg Training Loss: 23.32031669616699 - Avg Validation Loss: 45.87993621826172\n",
      "Epoch 27/56 - Avg Training Loss: 25.345581817626954 - Avg Validation Loss: 57.319444020589195\n",
      "Epoch 28/56 - Avg Training Loss: 24.62220993041992 - Avg Validation Loss: 54.696634928385414\n",
      "Epoch 29/56 - Avg Training Loss: 22.754296493530273 - Avg Validation Loss: 38.59312438964844\n",
      "Epoch 30/56 - Avg Training Loss: 22.873359298706056 - Avg Validation Loss: 42.168094635009766\n",
      "Epoch 31/56 - Avg Training Loss: 22.291697311401368 - Avg Validation Loss: 41.590895334879555\n",
      "Epoch 32/56 - Avg Training Loss: 22.825499725341796 - Avg Validation Loss: 44.93147659301758\n",
      "Epoch 33/56 - Avg Training Loss: 23.455373764038086 - Avg Validation Loss: 68.12560780843098\n",
      "Epoch 34/56 - Avg Training Loss: 25.864621734619142 - Avg Validation Loss: 65.09861882527669\n",
      "Epoch 35/56 - Avg Training Loss: 26.437135696411133 - Avg Validation Loss: 31.096925099690754\n",
      "Epoch 36/56 - Avg Training Loss: 23.45970230102539 - Avg Validation Loss: 28.91800308227539\n",
      "Epoch 37/56 - Avg Training Loss: 22.44904136657715 - Avg Validation Loss: 34.57914352416992\n",
      "Epoch 38/56 - Avg Training Loss: 20.88456001281738 - Avg Validation Loss: 25.588795344034832\n",
      "Epoch 39/56 - Avg Training Loss: 22.81006202697754 - Avg Validation Loss: 36.63509623209635\n",
      "Epoch 40/56 - Avg Training Loss: 22.208159255981446 - Avg Validation Loss: 45.61051813761393\n",
      "Epoch 41/56 - Avg Training Loss: 21.483542251586915 - Avg Validation Loss: 40.092291514078774\n",
      "Epoch 42/56 - Avg Training Loss: 19.40846366882324 - Avg Validation Loss: 38.472188313802086\n",
      "Epoch 43/56 - Avg Training Loss: 19.052592468261718 - Avg Validation Loss: 40.17267100016276\n",
      "Epoch 44/56 - Avg Training Loss: 19.524351501464842 - Avg Validation Loss: 48.2792116800944\n",
      "Epoch 45/56 - Avg Training Loss: 19.7955753326416 - Avg Validation Loss: 26.680901209513348\n",
      "Epoch 46/56 - Avg Training Loss: 19.977606201171874 - Avg Validation Loss: 27.44938023885091\n",
      "Epoch 47/56 - Avg Training Loss: 20.125282287597656 - Avg Validation Loss: 38.67263539632162\n",
      "Epoch 48/56 - Avg Training Loss: 18.751866149902344 - Avg Validation Loss: 41.25809987386068\n",
      "Epoch 49/56 - Avg Training Loss: 19.227006912231445 - Avg Validation Loss: 39.528279622395836\n",
      "Epoch 50/56 - Avg Training Loss: 17.573732376098633 - Avg Validation Loss: 28.23093096415202\n",
      "Epoch 51/56 - Avg Training Loss: 18.337641143798827 - Avg Validation Loss: 26.70423698425293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-07 00:20:32,126] Trial 1 finished with value: 22.811290105183918 and parameters: {'hidden_layer_size': 128, 'hidden_layer_count': 4, 'dropout_rate': 0.4741465288898049, 'lr': 0.004293661914079832, 'batch_size': 352, 'epochs': 56}. Best is trial 0 with value: 20.830942153930664.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/56 - Avg Training Loss: 16.308664321899414 - Avg Validation Loss: 24.201282501220703\n",
      "Epoch 53/56 - Avg Training Loss: 16.071591758728026 - Avg Validation Loss: 20.708600362141926\n",
      "Epoch 54/56 - Avg Training Loss: 15.133833122253417 - Avg Validation Loss: 19.969640096028645\n",
      "Epoch 55/56 - Avg Training Loss: 15.758245277404786 - Avg Validation Loss: 18.200864791870117\n",
      "Epoch 56/56 - Avg Training Loss: 16.082697105407714 - Avg Validation Loss: 22.811290105183918\n",
      "Finished Training\n",
      "====================================\n",
      "Number of finished trials: 2\n",
      "Best trial:\n",
      "     Duration:  1.016373\n",
      "     Value:  20.830942153930664\n",
      "     Params: \n",
      "         hidden_layer_size: 16\n",
      "         hidden_layer_count: 4\n",
      "         dropout_rate: 0.5783353912288658\n",
      "         lr: 0.005824796562274099\n",
      "         batch_size: 512\n",
      "         epochs: 80\n",
      "\n",
      "====================================\n",
      "\n",
      "V results for SEARCH_MLP_REDUCED_128 V\n",
      "\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_study(study):\n",
    "    print('====================================')\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('     Duration: ', trial.duration.total_seconds())\n",
    "    print('     Value: ', trial.value)\n",
    "    print('     Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'         {key}: {value}')\n",
    "\n",
    "if SEARCH_MLP_FULL:\n",
    "    print('Starting MLP full grid search')\n",
    "\n",
    "    # Optuna study object and direction (minimize validation loss)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(MLP_full_gridsearch, n_trials=2)\n",
    "    study.optimize(lambda trial: MLP_full_gridsearch(trial, 'Adam'), n_trials=2)\n",
    "\n",
    "    pretty_print_study(study)\n",
    "\n",
    "    \n",
    "else: print('Skipping SEARCH_MLP_FULL')\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "print('V results for SEARCH_MLP_REDUCED_256 V')\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "\n",
    "if SEARCH_MLP_REDUCED_256:\n",
    "    print('Starting MLP reduced grid search for 512-256 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256], 20)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 256, 'Adam'), n_trials=2)\n",
    "    \n",
    "    pretty_print_study(study)\n",
    "\n",
    "else: print('Skipping SEARCH_MLP_REDUCED_256') \n",
    "\n",
    "print('\\n====================================\\n')\n",
    "print('V results for SEARCH_MLP_REDUCED_128 V')\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "if SEARCH_MLP_REDUCED_128:\n",
    "    print('Starting MLP reduced grid search for 512-256-128 SAE')\n",
    "\n",
    "    encoders = train_stacked_autoencoders(X_train_tensor, 620, [512, 256, 128], 20)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: MLP_SAE_optimize(trial, encoders, 128, 'Adam'), n_trials=2)\n",
    "    \n",
    "    pretty_print_study(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
